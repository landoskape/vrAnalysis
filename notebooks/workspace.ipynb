{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62912f35-2f40-4d93-872e-439919754adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# pd.options.display.width = 1000\n",
    "\n",
    "from _old_vrAnalysis import analysis\n",
    "from _old_vrAnalysis import helpers\n",
    "from _old_vrAnalysis import database\n",
    "from _old_vrAnalysis import tracking\n",
    "from _old_vrAnalysis import session\n",
    "from _old_vrAnalysis import registration\n",
    "from _old_vrAnalysis import fileManagement as fm\n",
    "from _old_vrAnalysis import faststats as fs\n",
    "\n",
    "# from _old_vrAnalysis.uiDatabase import addEntryGUI\n",
    "# from _old_vrAnalysis.redgui import redCellGUI as rgui\n",
    "\n",
    "from dimilibi import CrossCompare\n",
    "from dimilibi import SVCANet, HurdleNet, BetaVAE\n",
    "from dimilibi import Population\n",
    "from dimilibi import SVCA\n",
    "from dimilibi import PCA\n",
    "from dimilibi import RidgeRegression, ReducedRankRegression\n",
    "from dimilibi import LocalSimilarity, FlexibleFilter, EmptyRegularizer, BetaVAE_KLDiv\n",
    "\n",
    "sessiondb = database.vrDatabase('vrSessions')\n",
    "mousedb = database.vrDatabase('vrMice')\n",
    "\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e303fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17250, 128) (17250,) (17250,) (17250,)\n",
      "Control Cells:\n",
      "Fraction Cells: 0.30, Fraction Dendrites: 0.20, Fraction Other: 0.50\n",
      "\n",
      "Red Cells:\n",
      "Fraction Cells: 0.31, Fraction Dendrites: 0.34, Fraction Other: 0.34\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "mouse_name = \"ATL022\"\n",
    "datestr = \"2023-04-19\"\n",
    "session_id = \"701\"\n",
    "\n",
    "ses = session.vrExperiment(mouse_name, datestr, session_id)\n",
    "\n",
    "cell_label = \"roicat_test_label_cells.npy\"\n",
    "dendrites_label = \"roicat_test_label_dendrites.npy\"\n",
    "\n",
    "cell_label = np.concatenate([np.load(ses.suite2pPath() / pname / cell_label) for pname in ses.planeNames]).astype(bool)\n",
    "dendrite_label = np.concatenate([np.load(ses.suite2pPath() / pname / dendrites_label) for pname in ses.planeNames]).astype(bool)\n",
    "\n",
    "red_idx = ses.getRedIdx()\n",
    "\n",
    "latents = np.load(ses.sessionPath() / \"roicat\" / \"roinet_latents.npy\")\n",
    "print(latents.shape, cell_label.shape, dendrite_label.shape, red_idx.shape)\n",
    "\n",
    "model_umap = UMAP(\n",
    "    n_neighbors=25,\n",
    "    n_components=2,\n",
    "    n_epochs=400,\n",
    "    verbose=False,\n",
    "    densmap=False,\n",
    ").fit(latents)\n",
    "emb = model_umap.transform(latents)\n",
    "\n",
    "\n",
    "# Show fraction of cells / dendrites / other for control and red\n",
    "def get_fractions(master_idx):\n",
    "    fraction_cell = np.sum((cell_label == 1) & master_idx) / np.sum(master_idx)\n",
    "    fraction_dendrite = np.sum((dendrite_label == 1) & master_idx) / np.sum(master_idx)\n",
    "    fraction_other = np.sum((cell_label == 0) & (dendrite_label == 0) & master_idx) / np.sum(master_idx)\n",
    "    return fraction_cell, fraction_dendrite, fraction_other\n",
    "\n",
    "fraction_cell, fraction_dendrite, fraction_other = get_fractions(~red_idx)\n",
    "print(\"Control Cells:\")\n",
    "print(f\"Fraction Cells: {fraction_cell:.2f}, Fraction Dendrites: {fraction_dendrite:.2f}, Fraction Other: {fraction_other:.2f}\")\n",
    "\n",
    "fraction_cell, fraction_dendrite, fraction_other = get_fractions(red_idx)\n",
    "print(\"\\nRed Cells:\")\n",
    "print(f\"Fraction Cells: {fraction_cell:.2f}, Fraction Dendrites: {fraction_dendrite:.2f}, Fraction Other: {fraction_other:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d281a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def get_spread_out_points(\n",
    "    data: np.ndarray, \n",
    "    n_ims: int = 1000, \n",
    "    dist_im_to_point: float = 0.3, \n",
    "    border_frac: float = 0.05, \n",
    "    device: str = 'cpu',\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a set of points, returns the indices of a subset of points that are\n",
    "    spread out. Intended to be used to overlay images on a scatter plot of\n",
    "    points.\n",
    "    RH 2023\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): \n",
    "            Array containing the points to be spread out. Shape: *(N, 2)*\n",
    "        n_ims (int): \n",
    "            Number of indices to return corresponding to the number of images to\n",
    "            be displayed. (Default is *1000*)\n",
    "        dist_im_to_point (float): \n",
    "            Minimum distance between an image and its nearest point. Images with\n",
    "            a minimum distance to a point greater than this value will be\n",
    "            discarded. (Default is *0.3*)\n",
    "        border_frac (float): \n",
    "            Fraction of the range of the data to add as a border around the\n",
    "            points. (Default is *0.05*)\n",
    "        device (str): \n",
    "            Device to use for torch operations. (Default is 'cpu')\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): \n",
    "            idx_images_overlay (np.ndarray):\n",
    "                Array containing the indices of the points to overlay images on.\n",
    "                Shape: *(n_ims,)*\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    DEVICE = device\n",
    "\n",
    "    min_data = np.nanmin(data, axis=0)  ## shape (2,)\n",
    "    max_data = np.nanmax(data, axis=0)  ## shape (2,)\n",
    "    range_data = max_data - min_data  ## shape (2,)\n",
    "    lims_canvas = ((min_data - range_data*border_frac), (max_data + range_data*border_frac))  ## ([\n",
    "    \n",
    "    sz_im = (range_data / (n_ims**0.5))\n",
    "    \n",
    "    grid_canvas = np.meshgrid(\n",
    "        np.linspace(lims_canvas[0][0], lims_canvas[1][0], int(n_ims**0.5)),\n",
    "        np.linspace(lims_canvas[0][1], lims_canvas[1][1], int(n_ims**0.5)),\n",
    "        indexing='xy',\n",
    "    )\n",
    "    grid_canvas_flat = np.vstack([g.reshape(-1) for g in grid_canvas]).T\n",
    "\n",
    "    dist_grid_to_imIdx = torch.as_tensor(data, device=DEVICE, dtype=torch.float32)[:,None,:] - \\\n",
    "        torch.as_tensor(grid_canvas_flat, device=DEVICE, dtype=torch.float32)[None,:,:]\n",
    "    distNorm_grid_to_imIdx = torch.linalg.norm(dist_grid_to_imIdx, dim=2)\n",
    "    distMin_grid_to_imIdx = torch.min(distNorm_grid_to_imIdx, dim=0)\n",
    "    max_dist = (np.min(sz_im))*dist_im_to_point\n",
    "    idx_good = distMin_grid_to_imIdx.values < max_dist\n",
    "    idx_images_overlay = distMin_grid_to_imIdx.indices[idx_good]\n",
    "\n",
    "    return idx_images_overlay\n",
    "\n",
    "def create_composite_overlay(embeddings, images, idx_images_overlay, image_overlay_raster_size=(1000, 1000), size_images_overlay=0.25, crop_images_overlay=0.35, frac_overlap_allowed=0.5):\n",
    "    \"\"\"Create a single composite image with all overlays\"\"\"\n",
    "    if size_images_overlay is None:\n",
    "        # Calculate optimal size based on nearest neighbors\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nn = NearestNeighbors(n_neighbors=2).fit(embeddings[idx_images_overlay])\n",
    "        distances = nn.kneighbors(embeddings[idx_images_overlay])[0]\n",
    "        min_dist = np.min(distances[:, 1])\n",
    "        size_images_overlay = min_dist * (1 + frac_overlap_allowed)\n",
    "\n",
    "    min_emb = np.nanmin(embeddings, axis=0)  ## shape (2,)\n",
    "    max_emb = np.nanmax(embeddings, axis=0)  ## shape (2,)\n",
    "    range_emb = max_emb - min_emb  ## shape (2,)\n",
    "    aspect_ratio_ims = (range_emb[1] / range_emb[0])  ## shape (1,)\n",
    "\n",
    "    assert isinstance(size_images_overlay, (int, float, np.ndarray)), 'size_images_overlay must be an int, float, or shape (2,) numpy array'\n",
    "    if isinstance(size_images_overlay, (int, float)):\n",
    "        size_images_overlay = np.array([size_images_overlay / aspect_ratio_ims, size_images_overlay])\n",
    "    assert size_images_overlay.shape == (2,), 'size_images_overlay must be an int, float, or shape (2,) numpy array'\n",
    "\n",
    "    # Create empty canvas\n",
    "    iors = image_overlay_raster_size\n",
    "    canvas = np.zeros((*iors, 4))  # RGBA\n",
    "    \n",
    "    pad = 0.07\n",
    "    min_vals = np.min(embeddings, axis=0)\n",
    "    max_vals = np.max(embeddings, axis=0)\n",
    "    range_vals = max_vals - min_vals\n",
    "    data_limits = (\n",
    "        min_vals - range_vals * pad,\n",
    "        max_vals + range_vals * pad\n",
    "    )\n",
    "\n",
    "    # Create interpolators for mapping data coordinates to pixel coordinates\n",
    "    interp_x = scipy.interpolate.interp1d(\n",
    "        [data_limits[0][0], data_limits[1][0]],\n",
    "        [0, iors[0]]\n",
    "    )\n",
    "    interp_y = scipy.interpolate.interp1d(\n",
    "        [data_limits[0][1], data_limits[1][1]],\n",
    "        [0, iors[1]]\n",
    "    )\n",
    "    \n",
    "    # Calculate size of each image in pixels\n",
    "    range_x = data_limits[1][0] - data_limits[0][0]\n",
    "    range_y = data_limits[1][1] - data_limits[0][1]\n",
    "    size_x = int((size_images_overlay[0] / range_x) * iors[0])\n",
    "    size_y = int((size_images_overlay[1] / range_y) * iors[1])\n",
    "    \n",
    "    xwidth = images.shape[2]\n",
    "    ywidth = images.shape[1]\n",
    "    crop_value = min(1.0, crop_images_overlay)\n",
    "    crop_value = max(0.1, crop_value)\n",
    "    x_crop_points = int((xwidth - crop_value * xwidth)/2)\n",
    "    y_crop_points = int((ywidth - crop_value * ywidth)/2)\n",
    "    for idx in idx_images_overlay:\n",
    "        # Normalize and convert to RGB if grayscale\n",
    "        img = images[idx][x_crop_points:-x_crop_points, y_crop_points:-y_crop_points]\n",
    "        if img.ndim == 2:\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            img = np.stack([img] * 3, axis=-1)\n",
    "        elif img.ndim == 3:\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        # Resize image\n",
    "        coords = np.stack(np.meshgrid(\n",
    "            np.linspace(0, img.shape[0], size_x),\n",
    "            np.linspace(0, img.shape[1], size_y)\n",
    "        ), axis=-1)\n",
    "        \n",
    "        img_resized = scipy.interpolate.interpn(\n",
    "            (np.arange(img.shape[0]), np.arange(img.shape[1])),\n",
    "            img,\n",
    "            coords,\n",
    "            method='linear',\n",
    "            bounds_error=False,\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Calculate position\n",
    "        x = int(interp_x(embeddings[idx, 0]))\n",
    "        y = int(interp_y(embeddings[idx, 1]))\n",
    "        \n",
    "        # Calculate bounds\n",
    "        x1 = max(0, x - size_x // 2)\n",
    "        x2 = min(iors[0], x + size_x // 2)\n",
    "        y1 = max(0, y - size_y // 2)\n",
    "        y2 = min(iors[1], y + size_y // 2)\n",
    "        \n",
    "        # Add to canvas\n",
    "        canvas[y1:y2, x1:x2, :3] = img_resized[:y2-y1, :x2-x1]\n",
    "        canvas[y1:y2, x1:x2, 3] = 1.0  # Alpha channel\n",
    "    \n",
    "    composite_overlay = np.flipud(canvas)  # Flip because imshow origin is bottom left\n",
    "    return composite_overlay, data_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e145411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17250, 51, 51) torch.Size([26]) (26, 51, 51) (1000, 1000, 4)\n"
     ]
    }
   ],
   "source": [
    "from cellector.io import create_from_suite2p\n",
    "roi_processor = create_from_suite2p(ses.suite2pPath(), use_redcell=\"False\", autocompute=False, save_features=False)\n",
    "roi_processor.parameters[\"centered_width\"] = 25\n",
    "centered_mask = roi_processor.centered_masks\n",
    "\n",
    "idx_images_overlay = get_spread_out_points(\n",
    "    emb,\n",
    "    n_ims=min(emb.shape[0], 50),  ## Select number of overlayed images here\n",
    "    dist_im_to_point=0.8,\n",
    ")\n",
    "\n",
    "images_overlay = centered_mask[idx_images_overlay]\n",
    "composite_overlay, data_limits = create_composite_overlay(emb, centered_mask, idx_images_overlay, size_images_overlay=0.4)\n",
    "\n",
    "print(centered_mask.shape, idx_images_overlay.shape, images_overlay.shape, composite_overlay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16dda88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "alpha = 0.3\n",
    "\n",
    "red_and_cell = red_idx & cell_label\n",
    "red_and_dendrite = red_idx & dendrite_label\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(emb[:, 0], emb[:, 1], c='k', s=s, alpha=alpha)\n",
    "# ax.scatter(emb[cell_label, 0], emb[cell_label, 1], c='g', s=3*s, alpha=0.2)\n",
    "# ax.scatter(emb[dendrite_label, 0], emb[dendrite_label, 1], c='b', s=3*s, alpha=0.2)\n",
    "ax.scatter(emb[red_idx, 0], emb[red_idx, 1], c='r', s=3*s, alpha=1.0)\n",
    "ax.scatter(emb[red_and_cell, 0], emb[red_and_cell, 1], c='g', s=3*s, alpha=1.0)\n",
    "ax.scatter(emb[red_and_dendrite, 0], emb[red_and_dendrite, 1], c='b', s=3*s, alpha=1.0)\n",
    "# ax.imshow(composite_overlay, extent=[data_limits[0][0], data_limits[1][0], data_limits[0][1], data_limits[1][1]], aspect=\"auto\", zorder=1000)\n",
    "ax.set_xlim(data_limits[0][0], data_limits[1][0])\n",
    "ax.set_ylim(data_limits[0][1], data_limits[1][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f802b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0, 1, 2, 3, 4, 5, 6, 7, 8], 3: [1, 2, 3, 4, 5, 6, 7, 8], 4: [6, 7, 8]}\n",
      "3 [1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "mouse_name = \"ATL058\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False, keep_planes=[1, 2, 3, 4], speedThreshold=1)\n",
    "env_stats = pcm.env_stats()\n",
    "print(env_stats)\n",
    "\n",
    "envs = list(env_stats.keys())\n",
    "first_session = [env_stats[env][0] for env in envs]\n",
    "idx_first_session = np.argsort(first_session)\n",
    "\n",
    "# use environment that was introduced second\n",
    "use_environment = envs[idx_first_session[1]]\n",
    "idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    # Attempt to use first environment if not enough sessions in second\n",
    "    use_environment = envs[idx_first_session[0]]\n",
    "    idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    print(f\"Skipping {mouse_name} due to not enough sessions!\")\n",
    "\n",
    "print(use_environment, idx_ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b98a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:52<00:00,  6.51s/it]\n",
      "Measuring reliability...: 100%|██████████| 8/8 [00:00<00:00, 83.33session/s]\n"
     ]
    }
   ],
   "source": [
    "envnum = use_environment\n",
    "max_diff = 4\n",
    "relcor_cutoff = 0.5\n",
    "smooth = 10\n",
    "\n",
    "bins_cor = np.linspace(-1, 1, 21)\n",
    "bins_mse = np.linspace(-4, 1, 21)\n",
    "centers_cor = helpers.edge2center(bins_cor)\n",
    "centers_mse = helpers.edge2center(bins_mse)\n",
    "\n",
    "def make_histograms(pcm, idx_ses):\n",
    "    pcm.load_pcss_data(idx_ses=idx_ses)\n",
    "    ctl_relcor = []\n",
    "    red_relcor = []\n",
    "    ctl_relmse = []\n",
    "    red_relmse = []\n",
    "    for idx in tqdm(idx_ses, desc=\"Measuring reliability...\", unit=\"session\"):\n",
    "        relmse, relcor = map(lambda x: x[0], pcm.pcss[idx].get_reliability_values(envnum=envnum))\n",
    "        idx_red = pcm.pcss[idx].vrexp.getRedIdx(keep_planes=pcm.keep_planes)\n",
    "        ctl_relcor.append(helpers.fractional_histogram(relcor[~idx_red], bins_cor)[0])\n",
    "        red_relcor.append(helpers.fractional_histogram(relcor[idx_red], bins_cor)[0])\n",
    "        ctl_relmse.append(helpers.fractional_histogram(relmse[~idx_red], bins_mse)[0])\n",
    "        red_relmse.append(helpers.fractional_histogram(relmse[idx_red], bins_mse)[0])\n",
    "\n",
    "    return ctl_relcor, red_relcor, ctl_relmse, red_relmse\n",
    "\n",
    "ctl_relcor, red_relcor, ctl_relmse, red_relmse = make_histograms(pcm, idx_ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95042f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, len(idx_ses), figsize=(15, 7), layout=\"constrained\")\n",
    "for idx, (cc, rc, cm, rm) in enumerate(zip(ctl_relcor, red_relcor, ctl_relmse, red_relmse)):\n",
    "    ax[0, idx].plot(centers_cor, cc, linewidth=1.2, color=\"k\", label=\"Ctl\")\n",
    "    ax[0, idx].plot(centers_cor, rc, linewidth=1.2, color=\"r\", label=\"Red\")\n",
    "    ax[1, idx].plot(centers_mse, cm, linewidth=1.2, color=\"k\", label=\"Ctl\")\n",
    "    ax[1, idx].plot(centers_mse, rm, linewidth=1.2, color=\"r\", label=\"Red\")\n",
    "    ax[0, idx].set_title(f\"Session {idx_ses[idx]}\")\n",
    "    ax[0, idx].set_xlabel(\"Correlation\")\n",
    "    ax[1, idx].set_xlabel(\"MSE\")\n",
    "    ax[0, idx].set_ylabel(\"Fraction\")\n",
    "    ax[1, idx].set_ylabel(\"Fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b776cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [4, 5, 6], 3: [1, 2, 3, 4, 5, 6], 4: [0, 1, 2, 3, 4, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "mouse_name = \"ATL060\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False, keep_planes=[1, 2, 3, 4], speedThreshold=1)\n",
    "env_stats = pcm.env_stats()\n",
    "print(env_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bfb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4] [4, 1, 0] 3\n"
     ]
    }
   ],
   "source": [
    "envs = list(env_stats.keys())\n",
    "first_session = [env_stats[env][0] for env in envs]\n",
    "idx_first_session = np.argsort(first_session)\n",
    "\n",
    "# use environment that was introduced second\n",
    "use_environment = envs[idx_first_session[1]]\n",
    "idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    # Attempt to use first environment if not enough sessions in second\n",
    "    use_environment = envs[idx_first_session[0]]\n",
    "    idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    print(f\"Skipping {mouse_name} due to not enough sessions!\")\n",
    "\n",
    "print(envs, first_session, use_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4570971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL027/2023-08-08/701\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "mouse_name = \"ATL027\"\n",
    "ses = random.choice(sessiondb.iterSessions(mouseName=mouse_name, experimentID=3, imaging=True))\n",
    "print(ses.sessionPrint())\n",
    "\n",
    "# Load the session\n",
    "pcss = analysis.placeCellSingleSession(ses, keep_planes=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d0531b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pcss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[273], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m average \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m smooth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m spkmaps \u001b[38;5;241m=\u001b[39m \u001b[43mpcss\u001b[49m\u001b[38;5;241m.\u001b[39mget_spkmap(average\u001b[38;5;241m=\u001b[39maverage, smooth\u001b[38;5;241m=\u001b[39msmooth, trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m num_pos \u001b[38;5;241m=\u001b[39m spkmaps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m average:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pcss' is not defined"
     ]
    }
   ],
   "source": [
    "average = True\n",
    "smooth = None\n",
    "spkmaps = pcss.get_spkmap(average=average, smooth=smooth, trials=\"full\")\n",
    "num_pos = spkmaps[0].shape[-1]\n",
    "if not average:\n",
    "    env_trials = [s.shape[1] for s in spkmaps]\n",
    "    min_trials = min(env_trials)\n",
    "    idx_use_trials = [np.random.permutation(etr)[:min_trials] for etr in env_trials]\n",
    "    spkmaps = [s[:, idx_use_trials[i]] for i, s in enumerate(spkmaps)]\n",
    "    # each row is a neuron with each trial concatenated along columns\n",
    "    spkmaps = [s.reshape(s.shape[0], -1) for s in spkmaps]\n",
    "\n",
    "reliable_only = True\n",
    "if reliable_only:\n",
    "    idx_reliable = pcss.get_reliable(cutoffs=(0.3, 0.6))\n",
    "    any_reliable = np.any(np.stack(idx_reliable, axis=0), axis=0)\n",
    "    spkmaps = [s[any_reliable] for s in spkmaps]\n",
    "\n",
    "pos_colormaps = [\"coolwarm\", \"coolwarm\", \"coolwarm\"] #, \"spring\", \"cool\"]\n",
    "env_colors = [\"k\", \"r\", \"b\"]\n",
    "\n",
    "pos_colors = [mpl.colormaps[cm](np.linspace(0, 1, num_pos)) for cm in pos_colormaps]\n",
    "env_colors = [np.tile(np.array(mpl.colors.to_rgba(c)).reshape(1, -1), (num_pos, 1)) for c in env_colors]\n",
    "if not average:\n",
    "    pos_colors = [np.tile(pc, (min_trials, 1)) for pc in pos_colors]\n",
    "    env_colors = [np.tile(ec, (min_trials, 1)) for ec in env_colors]\n",
    "print([s.shape for s in spkmaps])\n",
    "print([s.shape for s in env_colors], [s.shape for s in pos_colors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a7727978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 2) (195, 2) (585, 2)\n"
     ]
    }
   ],
   "source": [
    "test_env = 2\n",
    "\n",
    "train_data = np.concatenate([spkmaps[envidx] for envidx in range(len(spkmaps)) if envidx != test_env], axis=1)\n",
    "test_data = spkmaps[test_env]\n",
    "\n",
    "train_colors_pos = np.concatenate([pos_colors[envidx] for envidx in range(len(spkmaps)) if envidx != test_env], axis=0)\n",
    "test_colors_pos = pos_colors[test_env]\n",
    "train_colors_env = np.concatenate([env_colors[envidx] for envidx in range(len(spkmaps)) if envidx != test_env], axis=0)\n",
    "test_colors_env = env_colors[test_env]\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=5, n_components=2).fit(train_data.T)\n",
    "train_embedding = reducer.transform(train_data.T)\n",
    "test_embedding = reducer.transform(test_data.T)\n",
    "\n",
    "reducer_full = umap.UMAP(n_neighbors=5, n_components=2).fit(np.concatenate(spkmaps, axis=1).T)\n",
    "full_embedding = reducer_full.transform(np.concatenate(spkmaps, axis=1).T)\n",
    "\n",
    "print(train_embedding.shape, test_embedding.shape, full_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c4d328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "ax[0, 0].scatter(train_embedding[:, 0], train_embedding[:, 1], c=train_colors_pos, s=10)\n",
    "ax[0, 0].scatter(test_embedding[:, 0], test_embedding[:, 1], c=test_colors_pos, s=10)\n",
    "ax[0, 1].scatter(train_embedding[:, 0], train_embedding[:, 1], c=train_colors_env, s=10)\n",
    "ax[0, 1].scatter(test_embedding[:, 0], test_embedding[:, 1], c=test_colors_env, s=10)\n",
    "ax[1, 0].scatter(full_embedding[:, 0], full_embedding[:, 1], c=np.concatenate(pos_colors, axis=0), s=10)\n",
    "ax[1, 1].scatter(full_embedding[:, 0], full_embedding[:, 1], c=np.concatenate(env_colors, axis=0), s=10)\n",
    "\n",
    "for a in ax.flatten():\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "for a in ax[1, :]:\n",
    "    a.set_xlabel(\"UMAP 1\")\n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel(\"UMAP 2\")\n",
    "ax[0, 0].set_title(\"Color by Position\")\n",
    "ax[0, 1].set_title(\"Color by Environment\")\n",
    "ax[0, 0].set_ylabel(\"Train(BlackRed) vs Test(BLUE)\\n\\n\\nUMAP 2\")\n",
    "ax[1, 0].set_ylabel(\"Full Embedding\\n\\n\\nUMAP 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ee67980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "spkmaps = pcss.get_spkmap(average=True, trials=\"full\")\n",
    "idx_reliable = pcss.get_reliable(cutoffs=(0.3, 0.6))\n",
    "any_reliable = np.any(np.stack(idx_reliable, axis=0), axis=0)\n",
    "\n",
    "def get_kernels(data):\n",
    "    dot_product = np.dot(data.T, data)\n",
    "    norms = np.linalg.norm(data, axis=0)\n",
    "    cosine_angle = dot_product / np.outer(norms, norms)\n",
    "    kernel = np.corrcoef(data.T)\n",
    "    return cosine_angle, kernel\n",
    "\n",
    "angle, kernel = get_kernels(np.concatenate(spkmaps, axis=1))\n",
    "rel_angle, rel_kernel = get_kernels(np.concatenate(spkmaps, axis=1)[any_reliable])\n",
    "\n",
    "edges = [num_pos * i for i in range(1, len(spkmaps))]\n",
    "extent = [0, angle.shape[0], 0, angle.shape[1]]\n",
    "ticks = [num_pos/2 + num_pos * i for i in range(len(spkmaps))]\n",
    "labels = [f\"Env {i}\" for i in range(len(spkmaps))]\n",
    "\n",
    "cmap = \"bwr\"\n",
    "\n",
    "# Create figure and gridspec\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.2])\n",
    "\n",
    "# Create the main plots\n",
    "ax00 = fig.add_subplot(gs[0, 0])\n",
    "ax01 = fig.add_subplot(gs[0, 1])\n",
    "ax10 = fig.add_subplot(gs[1, 0])\n",
    "ax11 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Create colorbar axis that spans both rows\n",
    "cax = fig.add_subplot(gs[:, 2])  # This spans both rows\n",
    "\n",
    "# Create the plots\n",
    "im = ax00.imshow(angle, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "ax01.imshow(kernel, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "ax10.imshow(rel_angle, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "ax11.imshow(rel_kernel, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "# Add grid lines\n",
    "for edge in edges:\n",
    "    for ax in [ax00, ax01, ax10, ax11]:\n",
    "        ax.axhline(edge, color=\"k\", lw=0.5)\n",
    "        ax.axvline(edge, color=\"k\", lw=0.5)\n",
    "for ax in [ax00, ax01, ax10, ax11]:\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticks([])\n",
    "for ax in [ax00, ax10]:\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(reversed(labels))\n",
    "ax00.set_title(\"Cosine Angle\")\n",
    "ax01.set_title(\"Correlation\")\n",
    "ax00.set_ylabel(\"All cells\")\n",
    "ax10.set_ylabel(\"Reliable cells\")\n",
    "\n",
    "# Create colorbar\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb1608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9aaa3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vrAnalysis.external.pettit2022 import find_pettit_harvey_sessions, data_path\n",
    "sessions = find_pettit_harvey_sessions(data_path / \"dataFolder\")\n",
    "\n",
    "behavior = sessions[0].behavior\n",
    "spks = sessions[0].spks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vrAnalysis import files\n",
    "from _old_vrAnalysis import database\n",
    "sessiondb = database.vrDatabase('vrSessions')\n",
    "from typing import Union\n",
    "\n",
    "def find_experiment_options(root_dir: Union[str, Path]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Find all vrExperimentOptions.json files in the given directory and its subdirectories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : str or Path\n",
    "        The root directory to start the search from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[Path]\n",
    "        List of paths to all matching files\n",
    "    \"\"\"\n",
    "    def make_identifier(pth: Path) -> list[str]:\n",
    "        return \"_\".join(list(reversed([p.stem for p in list(pth.parents)[:3]])))\n",
    "    \n",
    "    root_path = Path(root_dir)\n",
    "    all_paths = list(root_path.rglob(\"vrExperimentOptions.json\"))\n",
    "    session_identifier = [make_identifier(pth) for pth in all_paths]\n",
    "    return all_paths, session_identifier\n",
    "\n",
    "pths, sids = find_experiment_options(files.local_data_path())\n",
    "csesids = [sessiondb.sessionPrint(joinby=\"_\") for sessiondb in sessiondb.iterSessions(useDefault=True)]\n",
    "\n",
    "for sid in sids:\n",
    "    if sid not in csesids:\n",
    "        print(\"oops\", sid)\n",
    "\n",
    "for ses in sessiondb.iterSessions():\n",
    "    csesid = ses.sessionPrint(joinby=\"_\")\n",
    "    print(csesid, csesid in sids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70933a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessiondb.printSessions(mouseName=\"ATL057\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fb78f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(d, length_scale=1.0):\n",
    "    \"\"\"Covariance function r(x - x') for scalar distance d\"\"\"\n",
    "    return np.exp(-0.5 * (d**2) / length_scale**2)\n",
    "\n",
    "def generate_cov(x, length_scale=1.0):\n",
    "    \"\"\"Generate covariance matrix K(x, x') for a given x\"\"\"\n",
    "    distances = np.abs(x - x[0])\n",
    "    first_row = r(distances, length_scale)\n",
    "    return sp.linalg.toeplitz(first_row)\n",
    "\n",
    "L = 0.2\n",
    "NP = 801\n",
    "N = 1e6\n",
    "sigma = 0.015\n",
    "theta = 1.3\n",
    "x = np.linspace(0, L, NP)  # 1D space from 0 to 10\n",
    "K = generate_cov(x, length_scale=sigma)  # Generate covariance matrix\n",
    "h = np.random.multivariate_normal(mean=np.zeros(len(x)), cov=K, size=int(N))  # Generate h(x) as a sample from GP[0, r(x - x')]\n",
    "f = np.maximum(0, h - theta)\n",
    "Ch = np.cov(h.T)\n",
    "Cf = np.cov(f.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting spectra data for CR_Hippocannula6\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for CR_Hippocannula7\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL022\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from _old_vrAnalysis.helpers import AttributeDict, cutoff_type, positive_float\n",
    "from _old_vrAnalysis.analysis.variance_structure import load_spectra_data\n",
    "\n",
    "MOUSE_NAMES = [\n",
    "    \"CR_Hippocannula6\",\n",
    "    \"CR_Hippocannula7\",\n",
    "    \"ATL022\",\n",
    "    \"ATL027\",\n",
    "    \"ATL028\",\n",
    "    \"ATL020\",\n",
    "    \"ATL012\",\n",
    "    \"ATL045\",\n",
    "]\n",
    "CUTOFFS = (0.4, 0.7)\n",
    "MAXCUTOFFS = None\n",
    "\n",
    "def get_spectra(mouse_name, args):\n",
    "    \"\"\"method for analyzing and plotting spectra with cvPCA and cvFOURIER analyses\"\"\"\n",
    "    # load spectra data (use temp if it matches)\n",
    "    track = tracking.tracker(mouse_name)  # get tracker object for mouse\n",
    "    pcm = analysis.placeCellMultiSession(track, autoload=False)  # open up place cell multi session analysis object (don't autoload!!!)\n",
    "\n",
    "    single_args = AttributeDict(vars(args))\n",
    "    single_args[\"mouse_name\"] = mouse_name\n",
    "\n",
    "    spectra_dictionary = load_spectra_data(pcm, single_args, save_as_temp=False, reload=False)\n",
    "\n",
    "    # return the dictionary\n",
    "    return spectra_dictionary\n",
    "\n",
    "\n",
    "def handle_inputs(inputs=[\"--do-spectra\"]):\n",
    "    \"\"\"method for creating and parsing input arguments\"\"\"\n",
    "    parser = ArgumentParser(description=\"do summary plots for a mouse\")\n",
    "    parser.add_argument(\n",
    "        \"--mouse-names\",\n",
    "        type=str,\n",
    "        nargs=\"*\",\n",
    "        default=\"processed\",\n",
    "        help=\"which mice to compare (list of mouse names, or like default), (default='all')\",\n",
    "    )\n",
    "    parser.add_argument(\"--cutoffs\", nargs=\"*\", type=cutoff_type, default=CUTOFFS, help=f\"cutoffs for reliability (default={CUTOFFS})\")\n",
    "    parser.add_argument(\"--maxcutoffs\", nargs=\"*\", type=cutoff_type, default=MAXCUTOFFS, help=\"maxcutoffs for reliability cells (default=None)\")\n",
    "    parser.add_argument(\"--do-spectra\", default=False, action=\"store_true\", help=\"create spectrum plots for mouse (default=False)\")\n",
    "    parser.add_argument(\"--dist-step\", default=1, type=float, help=\"dist-step for creating spkmaps (default=1cm)\")\n",
    "    parser.add_argument(\"--smooth\", default=0.1, type=positive_float, help=\"smoothing width for spkmaps (default=0.1cm)\")\n",
    "    parser.add_argument(\"--reload-spectra-data\", default=False, action=\"store_true\", help=\"reload spectra data (default=False)\")\n",
    "    args = parser.parse_args(inputs)\n",
    "\n",
    "    # if mouse_names is \"all\", get all mouse names from the database\n",
    "    if args.mouse_names == \"all\":\n",
    "        # mousedb = database.vrDatabase(\"vrSessions\")\n",
    "        mousedb = database.vrDatabase(\"vrMice\")\n",
    "        df = mousedb.getTable(trackerExists=True)\n",
    "        mouse_names = df[\"mouseName\"].unique()\n",
    "        args.mouse_names = mouse_names\n",
    "    elif args.mouse_names == \"processed\":\n",
    "        args.mouse_names = MOUSE_NAMES\n",
    "\n",
    "    # return the parsed arguments\n",
    "    return args\n",
    "\n",
    "# analyze spectra and make plots\n",
    "args = handle_inputs()\n",
    "pcms = []\n",
    "spectra_data = []\n",
    "for mouse in MOUSE_NAMES:\n",
    "    print(f\"Getting spectra data for {mouse}\")\n",
    "    spectra_data.append(get_spectra(mouse, args))  # Each is a dictionary of all the spectral output data\n",
    "    c_track = tracking.tracker(mouse)\n",
    "    c_pcm = analysis.placeCellMultiSession(c_track, autoload=False)\n",
    "    pcms.append(c_pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uSessionID\n",
      "mouseName\n",
      "sessionDate\n",
      "sessionID\n",
      "experimentType\n",
      "experimentID\n",
      "variableGain\n",
      "behavior\n",
      "imaging\n",
      "faceCamera\n",
      "vrEnvironments\n",
      "headPlateRotation\n",
      "numPlanes\n",
      "planeSeparation\n",
      "pockelsPercentage\n",
      "objectiveRotation\n",
      "vrRegistration\n",
      "suite2p\n",
      "suite2pQC\n",
      "redCellQC\n",
      "sessionQC\n",
      "scratchJustification\n",
      "logtime\n",
      "sessionNotes\n",
      "suite2pDate\n",
      "vrRegistrationDate\n",
      "vrRegistrationError\n",
      "vrRegistrationException\n",
      "redCellQCDate\n",
      "vrBehaviorVersion\n",
      "dontTrack\n"
     ]
    }
   ],
   "source": [
    "for fieldName in sessiondb.tableData()[0]:\n",
    "    print(fieldName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a1ae4079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL020/2023-04-05/701\n",
      "ATL022/2023-04-06/701\n",
      "ATL022/2023-03-27/701\n",
      "ATL023/2023-04-28/702\n",
      "ATL045/2024-01-26/701\n",
      "ATL020/2023-03-31/701\n",
      "ATL022/2023-03-27/701\n",
      "ATL020/2023-04-04/701\n",
      "ATL045/2024-01-24/701\n",
      "ATL027/2023-07-21/701\n"
     ]
    }
   ],
   "source": [
    "ises = np.random.choice(sessiondb.iterSessions(imaging=True, vrRegistration=True, experimentID=1), 10)\n",
    "for ses in ises: print(ses.sessionPrint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4c3f6fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10996, 200), (14021, 200), (13940, 200), (12310, 200), (11086, 200), (11902, 200), (13940, 200), (10254, 200), (14780, 200), (12839, 200)]\n"
     ]
    }
   ],
   "source": [
    "pcss = [analysis.placeCellSingleSession(ses) for ses in ises]\n",
    "spkmaps = [p.get_spkmap(average=True, trials=\"full\")[0] for p in pcss]\n",
    "print([s.shape for s in spkmaps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e26dd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_nan = np.any(np.stack([np.any(np.isnan(s), axis=0) for s in spkmaps], axis=0), axis=0)\n",
    "spkmaps = [s[:, ~idx_nan] for s in spkmaps]\n",
    "kernels = [np.cov(s.T) for s in spkmaps]\n",
    "def get_kfunc(kernel, rows):\n",
    "    kfunc = []\n",
    "    for r in range(rows):\n",
    "        kfunc.append(kernel[r][r:])\n",
    "    max_length = max([len(k) for k in kfunc])\n",
    "    for r in range(rows):\n",
    "        kfunc[r] = np.concatenate([kfunc[r], np.zeros(max_length - len(kfunc[r]))])\n",
    "    return np.stack(kfunc)\n",
    "kfuns = [get_kfunc(k, 100) for k in kernels]\n",
    "avg_kfuns = np.stack([np.mean(k, axis=0) for k in kfuns])\n",
    "avg_kfuns = avg_kfuns / np.max(avg_kfuns, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0ad45799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01009633 0.01075494]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fpath = Path(r\"C:\\Users\\Andrew\\Documents\\GitHub\\vrAnalysis\\figures\\plots_for_dataclub_241111\")\n",
    "orange = np.array([241, 80, 15]) / 255\n",
    "\n",
    "def lorentz(x, alpha, magnitude):\n",
    "    return magnitude * alpha / (x**2 + alpha**2)\n",
    "\n",
    "# Fit the data\n",
    "popt, pcov = curve_fit(lorentz, x, Cf[0] / max(Cf[0]), p0=[1.0, 1.0], bounds=(0, np.inf))\n",
    "print(popt)\n",
    "\n",
    "rlorentz = lorentz(x, popt[0], popt[1])\n",
    "\n",
    "xcm = x * 1000\n",
    "Lcm = L * 1000\n",
    "xcm_kf = np.linspace(0, max(xcm), avg_kfuns.shape[1])\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, 4, figsize=(18, 4), layout=\"constrained\")\n",
    "ax[0].imshow(Cf / np.max(Cf), extent=[0, Lcm, 0, Lcm], cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[1].imshow(sp.linalg.toeplitz(rlorentz), extent=[0, Lcm, 0, Lcm], cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[2].plot(xcm, Cf[0] / max(Cf[0]), c=orange, label=\"f-Covariance\")\n",
    "ax[2].plot(xcm, rlorentz, c='k', label=\"Lorentz\")\n",
    "ax[2].plot(xcm_kf, np.mean(avg_kfuns, axis=0), c='b', label=\"data\")\n",
    "ax[3].imshow(kernels[0] / np.max(kernels[0]), extent=[0, 100, 0, 100], cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax[2].set_xlim(0, 120)\n",
    "\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[0].set_title(\"f(x) Kernel\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[1].set_title(\"Lorentz Kernel\")\n",
    "ax[2].set_xlabel(\"Displacement\")\n",
    "ax[2].set_ylabel(\"Correlation\")\n",
    "ax[2].legend(loc=\"upper right\", fontsize=14)\n",
    "\n",
    "ax[3].set_xlabel(\"Position\")\n",
    "ax[3].set_ylabel(\"Position\")\n",
    "ax[3].set_title(\"Data Kernel\")\n",
    "plt.show()\n",
    "helpers.save_figure(fig, fpath / \"lorentz_comparison_withdata.png\")\n",
    "\n",
    "# # w, v = helpers.smart_pca(Ch)\n",
    "# wf, vf = helpers.smart_pca(Cf)\n",
    "# w = w / np.sum(w)\n",
    "# wf = wf / np.sum(wf)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(7, 5), layout=\"constrained\")\n",
    "# ax.plot(range(1, len(w)+1), w, label=\"Gaussian Kernel\", linewidth=2, color=\"k\")\n",
    "# ax.plot(range(1, len(wf)+1), wf, label=\"Thresholded Kernel\", linewidth=2, color=orange)\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_xlim(0, 81)\n",
    "# ax.set_ylim(1e-10, 1)\n",
    "# ax.legend(loc=\"upper right\")\n",
    "# ax.text(17, 3e-4, \"<---leading values are linear\", color=orange)\n",
    "# plt.show()\n",
    "# helpers.save_figure(fig, fpath / \"GP_Model_1.png\")\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(7, 5), layout=\"constrained\")\n",
    "# ax.plot(range(1, len(w)+1), w, label=\"Gaussian Kernel\", linewidth=2, color=\"k\")\n",
    "# # ax.plot(range(1, len(wf)+1), wf, label=\"Thresholded Kernel\", linewidth=2, color=\"b\")\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_xlim(0, 81)\n",
    "# ax.set_ylim(1e-10, 1)\n",
    "# ax.legend(loc=\"upper right\")\n",
    "# # ax.text(17, 3e-4, \"<---leading values are linear\", color=\"b\")\n",
    "# helpers.save_figure(fig, fpath / \"GP_Model_0.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c81b788b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((801,), (801,), (801, 801))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape, wf.shape, Ch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c0110c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = helpers.smart_pca(Cf) #sp.linalg.toeplitz(rlorentz))\n",
    "\n",
    "plt.plot(w)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also get real place field data\n",
    "mouse_name = \"ATL027\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False)\n",
    "ises = 8\n",
    "pcss = analysis.placeCellSingleSession(pcm.pcss[ises].vrexp, keep_planes=[1, 2, 3, 4], autoload=False)\n",
    "split_params = dict(total_folds=2, train_folds=1)\n",
    "pcss.define_train_test_split(**split_params)\n",
    "pcss.load_data(new_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b796225",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, P, T = 3000, 200, 100\n",
    "\n",
    "xpos = np.linspace(0, P, P)\n",
    "\n",
    "method = \"relugp\"\n",
    "\n",
    "if method == \"rbf\":\n",
    "    pf_loc = np.linspace(0, P, N) # place field location\n",
    "    pf_width = 1.0 * np.random.rand(N) + 2.5 # place field width\n",
    "    pf_basis = np.exp(-(pf_loc[:, None] - xpos[None, :]) ** 2 / 2 / pf_width[:, None] ** 2) # shape of place field\n",
    "    \n",
    "elif method == \"relugp\":\n",
    "    def r(d, length_scale=1.0):\n",
    "        \"\"\"Covariance function r(x - x') for scalar distance d\"\"\"\n",
    "        return np.exp(-0.5 * (d**2) / length_scale**2)\n",
    "\n",
    "    def generate_cov(x, length_scale=1.0):\n",
    "        \"\"\"Generate covariance matrix K(x, x') for a given x\"\"\"\n",
    "        distances = np.abs(x - x[0])\n",
    "        first_row = r(distances, length_scale)\n",
    "        return sp.linalg.toeplitz(first_row)\n",
    "    \n",
    "    L = 200 / 1000\n",
    "    fs = 0.001\n",
    "    NP = int(L / fs)\n",
    "    sigma = 0.015\n",
    "    theta = 1.3\n",
    "    x = np.linspace(0, L, NP)  # 1D space from 0 to 10\n",
    "    K = generate_cov(x, length_scale=sigma)  # Generate covariance matrix\n",
    "    h = np.random.multivariate_normal(mean=np.zeros(len(x)), cov=K, size=N*10)  # Generate h(x) as a sample from GP[0, r(x - x')]\n",
    "    pf_basis = np.maximum(0, h - theta)\n",
    "    idx_with_pf = np.where(np.any(pf_basis > 0, axis=1))[0]\n",
    "    pf_basis = pf_basis[idx_with_pf]\n",
    "    pf_basis = pf_basis[np.random.permutation(pf_basis.shape[0])[:N]]\n",
    "    if pf_basis.shape[0] < N:\n",
    "        raise ValueError(\"Not enough place fields\")\n",
    "    \n",
    "    idx_sort = np.argsort(np.argmax(pf_basis, axis=1))\n",
    "    pf_basis = pf_basis[idx_sort]\n",
    "    pf_loc = np.argmax(pf_basis, axis=1)\n",
    "    pf_basis = pf_basis / np.max(pf_basis, axis=1)[:, None]\n",
    "    \n",
    "    print(np.sum(np.any(pf_basis > 0, axis=1)) / N)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Method not recognized\")\n",
    "\n",
    "# Generate some place field properties\n",
    "beta_val = 0.1\n",
    "prob_pf = np.random.beta(beta_val, beta_val, N)\n",
    "# prob_pf = np.random.rand(N) ** 5.0 # probability of expressing place field\n",
    "noise_value = 0.5\n",
    "\n",
    "# Generate place field data\n",
    "pf_trial = np.random.rand(N, T) < prob_pf[:, None] # place field expression per trial\n",
    "pf_activity = pf_basis[:, :, None] * pf_trial[:, None, :] # place field activity\n",
    "noise_activity = np.random.randn(N, P, T) * noise_value # noise activity\n",
    "data = pf_activity + noise_activity\n",
    "\n",
    "train_data = np.mean(data[:, :, :T//2], axis=2)\n",
    "test_data = np.mean(data[:, :, T//2:], axis=2)\n",
    "\n",
    "# Get real place field data\n",
    "envidx = 1\n",
    "train_spkmaps = pcss.get_spkmap(trials=\"train\", average=True)\n",
    "test_spkmaps = pcss.get_spkmap(trials=\"test\", average=True)\n",
    "idx_nan = np.any(np.stack([np.any(np.isnan(spkmap), axis=0) for spkmap in train_spkmaps+test_spkmaps]), axis=0)\n",
    "train_spkmaps = [spkmap[:, ~idx_nan] for spkmap in train_spkmaps]\n",
    "test_spkmaps = [spkmap[:, ~idx_nan] for spkmap in test_spkmaps]\n",
    "train_spkmap = train_spkmaps[envidx]\n",
    "test_spkmap = test_spkmaps[envidx]\n",
    "\n",
    "# Run cvPCA Analyses\n",
    "nc = 80\n",
    "cvpca = helpers.cvPCA(train_data.T, test_data.T, nc=nc)\n",
    "truev = helpers.cvPCA(pf_basis.T, pf_basis.T, nc=nc)\n",
    "\n",
    "# Run on real mouse data\n",
    "cvpca_mouse = helpers.cvPCA(train_spkmap.T, test_spkmap.T, nc=nc)\n",
    "\n",
    "cvpca_v = helpers.smart_pca(train_data, centered=True)[1][:, :nc]\n",
    "train_proj = cvpca_v.T @ (train_data - train_data.mean(axis=1, keepdims=True))\n",
    "test_proj = cvpca_v.T @ (test_data - test_data.mean(axis=1, keepdims=True))\n",
    "\n",
    "ineg = np.where(cvpca < 0)[0]\n",
    "if len(ineg) == 0:\n",
    "    ineg = [30]\n",
    "\n",
    "norm = lambda x: x / np.sum(x)\n",
    "\n",
    "xv = range(1, nc + 1)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6, 6), layout=\"constrained\")\n",
    "ax[0, 0].imshow(train_data, aspect=\"auto\", cmap=\"inferno\", interpolation=\"none\")\n",
    "ax[0, 0].set_title(\"Train Data\")\n",
    "ax[0, 1].imshow(test_data, aspect=\"auto\", cmap=\"inferno\", interpolation=\"none\")\n",
    "ax[0, 1].set_title(\"Test Data\")\n",
    "ax[1, 0].plot(xv, norm(cvpca), c=\"k\")\n",
    "ax[1, 0].plot(xv, norm(truev), c=\"r\")\n",
    "ax[1, 0].plot(xv, norm(cvpca_mouse), c=\"b\")\n",
    "ax[1, 0].set_xlabel(\"Component\")\n",
    "ax[1, 0].set_ylabel(\"C-V Variance\")\n",
    "# ax[1, 0].set_xscale(\"log\")\n",
    "ax[1, 0].set_yscale(\"log\")\n",
    "ax[1, 1].plot(train_proj[ineg[0]], \"k\", label=\"Train\")\n",
    "ax[1, 1].plot(test_proj[ineg[0]], \"b\", label=\"Test\")\n",
    "ax[1, 1].set_xlabel(\"Train Projection onto Component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642a0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_name = \"ATL022\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False)\n",
    "ises = 7\n",
    "pcss = analysis.placeCellSingleSession(pcm.pcss[ises].vrexp, keep_planes=[1, 2, 3, 4], autoload=False)\n",
    "split_params = dict(total_folds=2, train_folds=1)\n",
    "pcss.define_train_test_split(**split_params)\n",
    "pcss.load_data(new_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spkmaps = pcss.get_spkmap(average=True, smooth=0.1, trials=\"train\")\n",
    "idx_trials = [np.argsort(ti) for ti in pcss.idxFullTrialEachEnv]\n",
    "spkmaps = [spkmap[:, itt] for spkmap, itt in zip(pcss.get_spkmap(average=False, smooth=0.1, trials=\"full\"), idx_trials)]\n",
    "spks = pcss.prepare_spks()\n",
    "\n",
    "idx_nan = np.any(\n",
    "    np.stack([np.any(np.isnan(t), axis=0) for t in train_spkmaps] + [np.any(np.isnan(t), axis=(0, 1)) for t in spkmaps]), axis=0\n",
    ")\n",
    "train_spkmaps = [t[:, ~idx_nan] for t in train_spkmaps]\n",
    "spkmaps = [t[:, :, ~idx_nan] for t in spkmaps]\n",
    "\n",
    "# Measure noise on test trials\n",
    "noise = [te - tr[:, None, :] for tr, te in zip(train_spkmaps, spkmaps)]\n",
    "print([t.shape for t in noise], [t.shape for t in spkmaps])\n",
    "\n",
    "# Flattened (bin by bin across the session)\n",
    "noise = [t.transpose((0, 1, 2)).reshape(t.shape[0], -1) for t in noise]\n",
    "print([t.shape for t in noise])\n",
    "\n",
    "noisecorr = [np.corrcoef(t.T) for t in noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29827a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = len(train_spkmaps)\n",
    "fig, ax = plt.subplots(1, num_envs, figsize=(5 * num_envs, 5), layout=\"constrained\")\n",
    "for i, tnc in enumerate(noisecorr):\n",
    "    ax[i].imshow(tnc, aspect=\"auto\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "611ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_name = \"ATL027\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False)\n",
    "ises = 12\n",
    "pcss = analysis.placeCellSingleSession(pcm.pcss[ises].vrexp, keep_planes=[1, 2, 3, 4], autoload=False)\n",
    "split_params = dict(total_folds=2, train_folds=1)\n",
    "pcss.define_train_test_split(**split_params)\n",
    "pcss.load_data(new_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eafcd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spkmaps = pcss.get_spkmap(average=True, smooth=0.1, trials=\"train\")\n",
    "test_spkmaps = pcss.get_spkmap(average=True, smooth=0.1, trials=\"test\")\n",
    "\n",
    "idx_nan = np.any(np.stack([np.any(np.isnan(t), axis=0) for t in train_spkmaps] + [np.any(np.isnan(t), axis=0) for t in test_spkmaps]), axis=0)\n",
    "train_spkmaps = [t[:, ~idx_nan] for t in train_spkmaps]\n",
    "test_spkmaps = [t[:, ~idx_nan] for t in test_spkmaps]\n",
    "\n",
    "train_cov = [np.cov(t.T) for t in train_spkmaps]\n",
    "test_cov = [np.cov(t.T) for t in test_spkmaps]\n",
    "cv_cov = [helpers.abcov(tr.T, te.T) for tr, te in zip(train_spkmaps, test_spkmaps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "023009d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_cvf, basis = helpers.get_fourier_basis(train_spkmaps[0].shape[1], Fs=pcss.distStep)\n",
    "num_components = basis.shape[0]\n",
    "\n",
    "s = [helpers.cvPCA(tr.T, te.T, nc=num_components) for tr, te in zip(train_spkmaps, test_spkmaps)]\n",
    "corr, cos_train, sin_train, cos_test, sin_test = helpers.named_transpose([helpers.cvFOURIER(tr, te, basis, covariance=True) for tr, te in zip(train_spkmaps, test_spkmaps)])\n",
    "corrsum = [np.mean(c, axis=0) for c in corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c4bf5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "envidx = 1\n",
    "\n",
    "num_neurons, num_bins = train_spkmaps[0].shape\n",
    "\n",
    "train_show = train_cov[envidx]\n",
    "test_show = test_cov[envidx]\n",
    "cv_show = cv_cov[envidx]\n",
    "\n",
    "# pad with numbins//2 zeros on each side\n",
    "shift_center = np.arange(num_bins)\n",
    "roll_center = num_bins // 2\n",
    "\n",
    "pad_matrix = np.full((num_bins, roll_center), np.nan)\n",
    "train_show_pad = np.hstack([pad_matrix, train_show, pad_matrix])\n",
    "test_show_pad = np.hstack([pad_matrix, test_show, pad_matrix])\n",
    "cv_show_pad = np.hstack([pad_matrix, cv_show, pad_matrix])\n",
    "\n",
    "# Roll each row to be centered on the peak\n",
    "train_show_roll = np.array([np.roll(row, roll_center - p) for row, p in zip(train_show_pad, shift_center)])\n",
    "test_show_roll = np.array([np.roll(row, roll_center-p) for row, p in zip(test_show_pad, shift_center)])\n",
    "cv_show_roll = np.array([np.roll(row, roll_center-p) for row, p in zip(cv_show_pad, shift_center)])\n",
    "\n",
    "freq, train_show_power = sp.signal.welch(train_show, axis=1, fs=1, scaling=\"density\")\n",
    "_, test_show_power = sp.signal.welch(test_show, axis=1, fs=1, scaling=\"density\")\n",
    "_, cv_show_power = sp.signal.welch(cv_show, axis=1, fs=1, scaling=\"density\")\n",
    "\n",
    "freq_ac, train_ac_power = sp.signal.welch(np.nanmean(train_show_roll, axis=0), fs=1, scaling=\"density\")\n",
    "_, test_ac_power = sp.signal.welch(np.nanmean(test_show_roll, axis=0), fs=1, scaling=\"density\")\n",
    "_, cv_ac_power = sp.signal.welch(np.nanmean(cv_show_roll, axis=0), fs=1, scaling=\"density\")\n",
    "\n",
    "middle_band = slice(100, num_bins*3-100)\n",
    "freq_ac_mid, train_ac_power_mid = sp.signal.welch(np.nanmean(train_show_roll, axis=0)[middle_band], fs=1, scaling=\"density\")\n",
    "_, test_ac_power_mid = sp.signal.welch(np.nanmean(test_show_roll, axis=0)[middle_band], fs=1, scaling=\"density\")\n",
    "_, cv_ac_power_mid = sp.signal.welch(np.nanmean(cv_show_roll, axis=0)[middle_band], fs=1, scaling=\"density\")\n",
    "\n",
    "\n",
    "f_xvals = np.arange(len(freq)) + 1\n",
    "\n",
    "cmap = mpl.colormaps[\"inferno\"]\n",
    "cmap.set_bad(color=[0.2, 0.2, 0.2])\n",
    "\n",
    "vmin = 0\n",
    "vmax = 0.1 #np.nanmax(train_show_roll)\n",
    "\n",
    "ymax = np.nanmax(np.nanmean(train_show_roll, axis=0)) * 1.1\n",
    "pmax = np.nanmax(np.nanmean(train_show_power, axis=0)) * 1.1\n",
    "\n",
    "norm = lambda x: x / np.sum(x)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot stuff\n",
    "xvals = np.arange(3*num_bins) - (3*num_bins)//2\n",
    "fig, ax = plt.subplots(4, 3, figsize=(14, 12), sharex=\"row\", sharey=\"row\", layout=\"constrained\")\n",
    "# autocorrelation maps\n",
    "ax[0, 0].imshow(train_show_pad, aspect='auto', cmap=cmap, vmin=0, vmax=vmax)\n",
    "ax[0, 1].imshow(test_show_pad, aspect='auto', cmap=cmap, vmin=0, vmax=vmax)\n",
    "ax[0, 2].imshow(cv_show_pad, aspect='auto', cmap=cmap, vmin=0, vmax=vmax)\n",
    "\n",
    "# average autocorrelation\n",
    "ax[1, 0].plot(np.nanmean(train_show_roll, axis=0), label=\"Train\")\n",
    "ax[1, 1].plot(np.nanmean(test_show_roll, axis=0), label=\"Test\")\n",
    "ax[1, 2].plot(np.nanmean(cv_show_roll, axis=0), label=\"CV\")\n",
    "ax[1, 0].set_ylim([0, ymax])\n",
    "\n",
    "# power - train/test/cv - \n",
    "# black: full, red: over average autocorrelation, blue: over middle band\n",
    "ax[2, 0].plot(freq, np.nanmean(train_show_power, axis=0), c='k', label=\"Train\")\n",
    "ax[2, 1].plot(freq, np.nanmean(test_show_power, axis=0), c='k', label=\"Test\")\n",
    "ax[2, 2].plot(freq, np.nanmean(cv_show_power, axis=0), c='k', label=\"CV\")\n",
    "ax[2, 0].plot(freq_ac, train_ac_power.T, c='r', label=\"Train AC\")\n",
    "ax[2, 1].plot(freq_ac, test_ac_power.T, c='r', label=\"Test AC\")\n",
    "ax[2, 2].plot(freq_ac, cv_ac_power.T, c='r', label=\"CV AC\")\n",
    "ax[2, 0].plot(freq_ac_mid, train_ac_power_mid.T, c='b', label=\"Train AC Mid\")\n",
    "ax[2, 1].plot(freq_ac_mid, test_ac_power_mid.T, c='b', label=\"Test AC Mid\")\n",
    "ax[2, 2].plot(freq_ac_mid, cv_ac_power_mid.T, c='b', label=\"CV AC Mid\")\n",
    "ax[2, 0].set_yscale('log')\n",
    "ax[2, 1].set_yscale('log')\n",
    "ax[2, 2].set_yscale('log')\n",
    "\n",
    "# cross-validated variance\n",
    "# left, green: cvpca\n",
    "# left, magenta: cv-fourier average\n",
    "# left, black: fourier power autocorr\n",
    "# middle, green: cv-fourier cosine\n",
    "# right, green: cv-fourier sine\n",
    "\n",
    "ax[3, 0].plot(freqs_cvf, norm(s[envidx]), c='g', label=\"Train\")\n",
    "ax[3, 1].plot(freqs_cvf, norm(corr[envidx][0]), c='g', label=\"Correlation - Cosine\")\n",
    "ax[3, 2].plot(freqs_cvf, norm(corr[envidx][1]), c='g', label=\"Correlation - Sine\")\n",
    "\n",
    "ax[3, 0].plot(freqs_cvf, norm(corrsum[envidx]), c='m', label=\"Correlation - SumFourier\")\n",
    "\n",
    "ax[3, 0].plot(freq, norm(np.nanmean(cv_show_power, axis=0)), c='k', label=\"CV\")\n",
    "ax[3, 1].plot(freq, norm(np.nanmean(cv_show_power, axis=0)), c='k', label=\"CV\")\n",
    "ax[3, 2].plot(freq, norm(np.nanmean(cv_show_power, axis=0)), c='k', label=\"CV\")\n",
    "ax[3, 0].set_xlabel(\"Frequency\")\n",
    "ax[3, 1].set_xlabel(\"Frequency\")\n",
    "ax[3, 2].set_xlabel(\"Frequency\")\n",
    "ax[3, 0].set_yscale('log')\n",
    "ax[3, 1].set_yscale('log')\n",
    "ax[3, 2].set_yscale('log')\n",
    "\n",
    "pmin = min([np.nanmin(np.nanmean(train_show_power, axis=0)), np.nanmin(np.nanmean(test_show_power, axis=0)), np.nanmin(np.nanmean(cv_show_power, axis=0))])\n",
    "\n",
    "ax[0, 0].set_title(\"Train\")\n",
    "ax[0, 1].set_title(\"Test\")\n",
    "ax[0, 2].set_title(\"CV\")\n",
    "ax[0, 0].set_ylabel(\"Position\")\n",
    "ax[1, 0].set_ylabel(\"AutoCorr\\n(average row of cov)\")\n",
    "ax[2, 0].set_ylabel(\"Power Spectrum\")\n",
    "\n",
    "ax[0, 0].set_xlabel(\"Position\")\n",
    "ax[0, 1].set_xlabel(\"Position\")\n",
    "ax[0, 2].set_xlabel(\"Position\")\n",
    "\n",
    "ax[1, 0].set_xlabel(\"Position\")\n",
    "ax[1, 1].set_xlabel(\"Position\")\n",
    "ax[1, 2].set_xlabel(\"Position\")\n",
    "\n",
    "ax[2, 0].set_xlabel(\"Frequency\")\n",
    "ax[2, 1].set_xlabel(\"Frequency\")\n",
    "ax[2, 2].set_xlabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "baf1353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = helpers.smart_pca(cv_show)\n",
    "_, wf = sp.signal.welch(cv_show[cv_show.shape[0]//2], fs=1, scaling=\"density\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), layout=\"constrained\")\n",
    "ax[0].plot(range(len(freqs_cvf)), norm(s[envidx]), c='k', label=\"cvPCA\")\n",
    "ax[0].plot(range(len(freqs_cvf)), norm(corrsum[envidx]), c='r', label=\"cv-Fourier\")\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_ylim([1e-5, 1])\n",
    "ax[0].set_xlabel(\"Dimensions\")\n",
    "ax[0].set_ylabel(\"Relative Variance\")\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "\n",
    "ax[1].plot(range(len(freqs_cvf)), norm(w[:len(freqs_cvf)]), c='k', label=\"cvPCA\")\n",
    "ax[1].plot(range(len(freqs_cvf)), norm(wf[:len(freqs_cvf)]), c='b', label=\"True\")\n",
    "ax[1].set_yscale('log')\n",
    "# ax[1].set_ylim([1e-5, 1])\n",
    "ax[1].set_xlabel(\"Dimensions\")\n",
    "ax[1].set_ylabel(\"Relative Variance\")\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247fd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyses and work to do:\n",
    "\n",
    "# ROICaT Figure:\n",
    "# - add a \"print pair data\" button to the interactive viewer (and maybe even a \"save figure\" button?)\n",
    "# - build an example figure with the ROICaT data (can be simple, just make it soon)\n",
    "\n",
    "# LBM-s3d:\n",
    "# - get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bbdd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Management:\n",
    "# I need a way to report how many sessions the mouse has experienced each environment, independent of \n",
    "# which environments are represented in imaging sessions (which is how I'm doing it now...)\n",
    "\n",
    "# Required Updates: \n",
    "# need to update the placeCellMultiSession object to reflect changes to spkmap code\n",
    "# anything that uses pcss.get_place_field (pcmm make_snake_data and make_paired_snake)\n",
    "\n",
    "# Compare cvPCA analyses with eigenspectrum of spontaneous data unrelated to SVCA\n",
    "# And I want to start with the rastermap on projected place field data\n",
    "\n",
    "# Compare cvPCA to SVCA (do a hybrid: use cvPCA to get the spatial PCs, then apply those to the SVCA split)\n",
    "\n",
    "# Buzsaki Data:\n",
    "# https://crcns.org/data-sets/hc/hc-3 -- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4097350/\n",
    "# https://app.globus.org/file-manager?origin_id=188a6110-96db-11eb-b7a9-f57b2d55370d&origin_path=%2FVargaV%2F&two_pane=false - https://buzsakilab.com/wp/animals/?frm_search&project=67125&frm-page-14333=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471f554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Dataclub 240429: \n",
    "# -- need to consolidate all my figures (especially for the last few slides in a script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b225b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Meeting with Kenneth Plan:\n",
    "# - Relate to Kernel Matrices:\n",
    "#   - https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c09_p291-326.pdf\n",
    "#   - First, look at the kernel matrices (the position x position covariance matrices for each environment/session)\n",
    "#   - Study the structure, and how it changes over time. \n",
    "#   - Compare the cross-validated kernel and the non-cv kernel matrix and compare their changes over time\n",
    "#            - notes about ^^, this will tell us how much changes in eigenspectrum relate to reliability across trials vs the shape of the kernel matrix etc...\n",
    "# - Studies of non-place cells:\n",
    "#   - Look at the eigenspectrum from non-place cells, suppose as a function of the reliability...\n",
    "#   - Do cross-validated decoding from non-place cells\n",
    "# - Discussion of SVCA results\n",
    "#   - SVCA dimensionality could have issues with noise estimation... the same way the trial expanded cvPCA plots did...\n",
    "#   - Predict cell2 group from cell1 group, and predict cell2 group from their cross-validated place field, compare variance explained and overlap in variance explained\n",
    "# - Rastermap: \n",
    "#   - need to find a way to remove expected spatial activity from full spike trace data (then maybe do rastermap again?)\n",
    "# - Signal to Noise\n",
    "#   - For each ROI, measure activity in center of place field, outside of place field on a linear track, and outside the track (or in other environments)\n",
    "# - Measure spontaneous periods of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9078fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMILIBI GOALS:\n",
    "# Compare best networks to ridge regression for a bunch of sessions.\n",
    "# Ridge Regression:\n",
    "#   - need to optimize ridge parameter: I can use a simple grid search on a log-space for this in two stages\n",
    "#   - Note: I tested (for one session) if the best ridge parameter is the same for full-rank and low-rank, and it was. \n",
    "#   - setup a train/val/test split program and then fit the model to each session, and record the results for several ranks\n",
    "# Networks: \n",
    "#   - for a subset of ranks, train a standard network and a beta-VAE network, record results for each session\n",
    "#   - to validate, just train a network on lots of epochs, store the evaluation test score throughout training, and \n",
    "#     save the full trajectory across training along with the best test score and the associated epoch number.\n",
    "#   - Note: for BetaVAELoss, will need to separate the loss into reconstruction and KL divergence, for proper saving.\n",
    "# Analysis / summary:\n",
    "#   - plot summary curves across mice for each rank, color-coded by RRR, BetaVAE, and SVCANet\n",
    "#   - probably also compare for each mouse somehow? Maybe categorized dot plots separated by rank? \n",
    "#   - I also want to compare how the validation scores improve over time for the network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('CR_Hippocannula6', '2022-08-26', '702') # test this because performance improved for 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a session randomly that has registered imaging data and a single environment\n",
    "vrexp = random.choice(sessiondb.iterSessions(imaging=True, vrRegistration=True, experimentID=1))\n",
    "print(vrexp.sessionPrint()) # show which session you chose\n",
    "\n",
    "keep_planes = [1, 2]\n",
    "onefile = \"mpci.roiActivityDeconvolvedOasis\"\n",
    "ospks = vrexp.loadone(onefile)\n",
    "keep_idx = vrexp.idxToPlanes(keep_planes=keep_planes)\n",
    "ospks = ospks[:, keep_idx]\n",
    "time_split_prms = dict(\n",
    "    num_groups=3,\n",
    "    relative_size=[5, 5, 1], #[5, 5, 1],\n",
    "    chunks_per_group=-3, # 25\n",
    "    num_buffer=3, # usually use default (which is 10)\n",
    ")\n",
    "npop = Population(ospks.T, generate_splits=True, time_split_prms=time_split_prms)\n",
    "print(npop.size())\n",
    "\n",
    "pcss = analysis.placeCellSingleSession(vrexp, keep_planes=keep_planes, onefile=onefile, autoload=True)\n",
    "assert len(pcss.environments) == 1, \"Only one environment is supported for this analysis\"\n",
    "\n",
    "train_source, train_target = npop.get_split_data(0, center=False, scale=True, pre_split=False, scale_type=\"preserve\")\n",
    "test_source, test_target = npop.get_split_data(1, center=False, scale=True, pre_split=False, scale_type=\"preserve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf5382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a comparison of cvPCA and SVCA\n",
    "svca = SVCA(centered=True).fit(train_source, train_target)\n",
    "\n",
    "# Get place fields\n",
    "envnum = pcss.environments[0]\n",
    "train_spkmap = pcss.get_spkmap(envnum=envnum, average=True, trials=\"train\")[0]\n",
    "source_spkmap = train_spkmap[npop.cell_split_indices[0]]\n",
    "target_spkmap = train_spkmap[npop.cell_split_indices[1]]\n",
    "\n",
    "test_spkmap = pcss.get_spkmap(envnum=envnum, average=True, trials=\"test\")[0]\n",
    "source_spkmap_test = test_spkmap[npop.cell_split_indices[0]]\n",
    "target_spkmap_test = test_spkmap[npop.cell_split_indices[1]]\n",
    "\n",
    "idx_nan = np.any(np.isnan(source_spkmap), axis=0) | np.any(np.isnan(target_spkmap), axis=0)\n",
    "source_pca = PCA().fit(source_spkmap[:, ~idx_nan])\n",
    "target_pca = PCA().fit(target_spkmap[:, ~idx_nan])\n",
    "source_components = source_pca.get_components()\n",
    "target_components = target_pca.get_components()\n",
    "\n",
    "# Compare the PCA map of train to test trials on the source data\n",
    "idx_nan = np.any(np.isnan(source_spkmap_test), axis=0)\n",
    "source_pca_test = PCA().fit(source_spkmap_test[:, ~idx_nan])\n",
    "traintest_map = np.dot(source_components.T, source_pca_test.get_components())\n",
    "\n",
    "# For U, V, and components, each column is a component (so each row is a neuron)\n",
    "source_map = np.dot(source_components.T, svca.U)\n",
    "target_map = np.dot(target_components.T, svca.V)\n",
    "\n",
    "vmin = min(source_map.min(), target_map.min(), traintest_map.min())\n",
    "vmax = max(source_map.max(), target_map.max(), traintest_map.max())\n",
    "\n",
    "# Take weighted average across axis 0 (for each SV, which PF PCs are it composed of?)\n",
    "idx = np.arange(source_map.shape[0]).reshape(-1, 1)\n",
    "source_map_avg = np.sum(np.abs(source_map) * idx, axis=0) / np.sum(np.abs(source_map), axis=0)\n",
    "target_map_avg = np.sum(np.abs(target_map) * idx, axis=0) / np.sum(np.abs(target_map), axis=0)\n",
    "pc_map_avg = np.sum(np.abs(traintest_map) * idx, axis=0) / np.sum(np.abs(traintest_map), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbae25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get SV activity projections, \n",
    "u_activity = fs.zscore(np.array(svca.U.T @ npop.data[npop.cell_split_indices[0]]), axis=1)\n",
    "v_activity = fs.zscore(np.array(svca.V.T @ npop.data[npop.cell_split_indices[1]]), axis=1)\n",
    "u_rawspkmap = helpers.getBehaviorAndSpikeMaps(vrexp, onefile=u_activity.T)[3]\n",
    "v_rawspkmap = helpers.getBehaviorAndSpikeMaps(vrexp, onefile=v_activity.T)[3]\n",
    "uspkmap_train = pcss.get_spkmap(envnum=envnum, average=True, trials=\"train\", rawspkmap=u_rawspkmap)[0]\n",
    "vspkmap_train = pcss.get_spkmap(envnum=envnum, average=True, trials=\"train\", rawspkmap=v_rawspkmap)[0]\n",
    "uspkmap_test = pcss.get_spkmap(envnum=envnum, average=True, trials=\"test\", rawspkmap=u_rawspkmap)[0]\n",
    "vspkmap_test = pcss.get_spkmap(envnum=envnum, average=True, trials=\"test\", rawspkmap=v_rawspkmap)[0]\n",
    "\n",
    "def select_env(tup, idx):\n",
    "    return list(map(lambda x: x[idx], tup))\n",
    "\n",
    "urelmse, urelcor = select_env(pcss.get_reliability_values(envnum=envnum, rawspkmap=u_rawspkmap), 0)\n",
    "vrelmse, vrelcor = select_env(pcss.get_reliability_values(envnum=envnum, rawspkmap=v_rawspkmap), 0)\n",
    "relmse, relcor = select_env(pcss.get_reliability_values(envnum=envnum), 0)\n",
    "\n",
    "u_rel_idx = urelcor > 0.6\n",
    "v_rel_idx = vrelcor > 0.6\n",
    "rel_idx = relcor > 0.6\n",
    "\n",
    "uidx = pcss.get_place_field(uspkmap_train[u_rel_idx], method=\"max\")[1]\n",
    "vidx = pcss.get_place_field(vspkmap_train[v_rel_idx], method=\"max\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5103f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dimilibi.helpers import make_position_basis, filter_timepoints\n",
    "frame_position, frame_environment, environments = vrexp.get_frame_behavior(speedThreshold=1)\n",
    "valid_u_activity, valid_position, valid_environment = filter_timepoints(u_activity.T, frame_position, frame_environment)\n",
    "position_basis = make_position_basis(valid_position, valid_environment, num_basis=10)\n",
    "\n",
    "upospop = Population(valid_u_activity.T, time_split_prms={\"num_groups\": 2, \"relative_size\": [5, 1], \"chunks_per_group\": -3, \"num_buffer\": 3}, dtype=torch.float32)\n",
    "train_valid_u = upospop.apply_split(valid_u_activity.T, 0)\n",
    "test_valid_u = upospop.apply_split(valid_u_activity.T, 1)\n",
    "train_pos_basis = upospop.apply_split(position_basis.T, 0)\n",
    "test_pos_basis = upospop.apply_split(position_basis.T, 1)\n",
    "\n",
    "print(train_valid_u.shape, train_pos_basis.shape, test_valid_u.shape, test_pos_basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = RidgeRegression(alpha=1e3, fit_intercept=True).fit(train_valid_u.T, train_pos_basis.T)\n",
    "print(rmodel.score(test_valid_u.T, test_pos_basis.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded97af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "tspkmapidx = pcss.get_place_field(train_spkmap[rel_idx], method=\"max\")[1]\n",
    "print(vmin, vmax)\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 6), layout=\"constrained\")\n",
    "ax[0, 0].imshow(uspkmap_train[u_rel_idx][uidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[0, 1].imshow(vspkmap_train[v_rel_idx][vidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[0, 2].imshow(train_spkmap[rel_idx][tspkmapidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[1, 0].imshow(uspkmap_test[u_rel_idx][uidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[1, 1].imshow(vspkmap_test[v_rel_idx][vidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[1, 2].imshow(test_spkmap[rel_idx][tspkmapidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[0, 3].ecdf(urelmse[~np.isnan(urelmse)], label=\"U\")\n",
    "ax[0, 3].ecdf(vrelmse[~np.isnan(vrelmse)], label=\"V\")\n",
    "ax[0, 3].ecdf(relmse[~np.isnan(relmse)], label=\"PCA\")\n",
    "ax[1, 3].ecdf(urelcor[~np.isnan(urelcor)], label=\"U\")\n",
    "ax[1, 3].ecdf(vrelcor[~np.isnan(vrelcor)], label=\"V\")\n",
    "ax[1, 3].ecdf(relcor[~np.isnan(relcor)], label=\"PCA\")\n",
    "ax[0, 3].set_xlabel(\"Reliability (method 1)\")\n",
    "ax[1, 3].set_xlabel(\"Reliability (method 2)\")\n",
    "ax[0, 3].set_ylabel(\"Cumulative probability\")\n",
    "ax[1, 3].set_ylabel(\"Cumulative probability\")\n",
    "ax[0, 3].legend()\n",
    "ax[1, 3].legend()\n",
    "ax[0, 3].set_xlim(-2, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f97ee395",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = source_components.shape[1]\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), layout=\"constrained\")\n",
    "ax[0].imshow(np.abs(source_map[:, :max_components]), aspect=\"auto\", interpolation=\"none\", cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[1].imshow(np.abs(target_map[:, :max_components]), aspect=\"auto\", interpolation=\"none\", cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[2].imshow(np.abs(traintest_map[:, :max_components]), aspect=\"auto\", interpolation=\"none\", cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[3].scatter(np.abs(source_map[:, :max_components].flatten()), np.abs(target_map[:, :max_components].flatten()), s=1)\n",
    "\n",
    "# Link axes 0 and 1\n",
    "ax[1].sharex(ax[0])\n",
    "ax[1].sharey(ax[0])\n",
    "ax[2].sharex(ax[0])\n",
    "ax[2].sharey(ax[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b4d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64077205",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = False\n",
    "scale = True\n",
    "pre_split = False\n",
    "scale_type = \"preserve\"\n",
    "\n",
    "train_source, train_target = npop.get_split_data(0, center=center, scale=scale, pre_split=pre_split, scale_type=scale_type)\n",
    "val_source, val_target = npop.get_split_data(1, center=center, scale=scale, pre_split=pre_split, scale_type=scale_type)\n",
    "test_source, test_target = npop.get_split_data(2, center=center, scale=scale, pre_split=pre_split, scale_type=scale_type)\n",
    "\n",
    "get_whitening = False\n",
    "\n",
    "if get_whitening:\n",
    "    zca_source = PCA().fit(train_source).get_zca().to(device)\n",
    "    zca_val = PCA().fit(val_source).get_zca().to(device)\n",
    "    zca_target = PCA().fit(train_target).get_zca().to(device)\n",
    "\n",
    "print(train_source.shape, train_target.shape, val_source.shape, val_target.shape, test_source.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc3aa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get eigenvalues of the full population to compare with simulated data appropriately\n",
    "# npop_evals = PCA().fit(ospks.T).get_eigenvalues()\n",
    "\n",
    "N = npop.size(0) // 2\n",
    "T = 8000\n",
    "Ttest = 1000\n",
    "Q = torch.linalg.qr(torch.normal(0, 1, (2*N, 2*N)))[0]\n",
    "D = npop_evals[:2*N]\n",
    "\n",
    "train_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, T))\n",
    "val_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, Ttest))\n",
    "train_data = Q @ train_scores\n",
    "val_data = Q @ val_scores\n",
    "\n",
    "train_source = train_data[:N]\n",
    "train_target = train_data[N:]\n",
    "val_source = val_data[:N]\n",
    "val_target = val_data[N:]\n",
    "\n",
    "# zscore the data\n",
    "train_source = (train_source - train_source.mean(1, keepdim=True)) / train_source.std(1, keepdim=True)\n",
    "train_target = (train_target - train_target.mean(1, keepdim=True)) / train_target.std(1, keepdim=True)\n",
    "val_source = (val_source - val_source.mean(1, keepdim=True)) / val_source.std(1, keepdim=True)\n",
    "val_target = (val_target - val_target.mean(1, keepdim=True)) / val_target.std(1, keepdim=True)\n",
    "\n",
    "# zca_source = PCA().fit(train_source).get_zca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0f5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr = ReducedRankRegression(alpha=1e5, fit_intercept=True).fit(train_source.T.to('cpu'), train_target.T.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 5\n",
    "print(rrr.score(train_source.T.to('cpu'), train_target.T.to('cpu'), rank=rank))\n",
    "print(rrr.score(train_source.T.to('cpu'), train_target.T.to('cpu')))\n",
    "print(rrr.score(val_source.T, val_target.T, rank=rank, nonnegative=False))\n",
    "print(rrr.score(val_source.T, val_target.T, rank=rank, nonnegative=True))\n",
    "print(rrr.score(val_source.T, val_target.T, nonnegative=False))\n",
    "print(rrr.score(val_source.T, val_target.T, nonnegative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb25cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = train_source.size(0)\n",
    "num_hidden = [400] # [hyps[\"best_params\"][\"num_hidden\"]]\n",
    "num_latent = 5\n",
    "num_target_neurons = train_target.size(0)\n",
    "num_timepoints = train_source.size(1)\n",
    "\n",
    "# net0 = SVCANet(\n",
    "#     num_neurons,\n",
    "#     num_hidden,\n",
    "#     num_latent,\n",
    "#     num_target_neurons,\n",
    "#     activation = torch.nn.ReLU(),\n",
    "#     nonnegative = False, \n",
    "# ).to(device)\n",
    "\n",
    "# net1 = BetaVAE(\n",
    "#     num_neurons,\n",
    "#     num_hidden,\n",
    "#     num_latent,\n",
    "#     num_target_neurons,\n",
    "#     activation = torch.nn.ReLU(),\n",
    "#     nonnegative = False,\n",
    "# ).to(device)\n",
    "\n",
    "# net2 = HurdleNet(\n",
    "#     num_neurons,\n",
    "#     num_hidden,\n",
    "#     num_latent,\n",
    "#     num_target_neurons,\n",
    "#     activation = torch.nn.ReLU(),\n",
    "#     nonnegative = False,\n",
    "#     transparent_relu=True,\n",
    "# ).to(device)\n",
    "\n",
    "nets = [\n",
    "    constructor(\n",
    "        num_neurons,\n",
    "        num_hidden,\n",
    "        num_latent,\n",
    "        num_target_neurons,\n",
    "        activation = torch.nn.ReLU(),\n",
    "        nonnegative = True,\n",
    "        transparent_relu = True,\n",
    "    ).to(device)\n",
    "    for constructor in [SVCANet, SVCANet, SVCANet, SVCANet, HurdleNet, HurdleNet, HurdleNet, HurdleNet]\n",
    "]\n",
    "\n",
    "cols = 'kkkkrrrr'\n",
    "\n",
    "# nets = [net0, net1]\n",
    "betavae = [False for _ in range(len(nets))] #[False, True]\n",
    "\n",
    "# nets = [net0, net1]\n",
    "loss_functions = [torch.nn.MSELoss(reduction='sum') for _ in range(len(nets))]\n",
    "\n",
    "# beta = [1e1, 1e2, 1e3, 1e4] #[1 for _ in range(len(nets))]\n",
    "regularizers = [EmptyRegularizer() for _ in range(len(nets))]\n",
    "# regularizers = [BetaVAE_KLDiv(beta=b, reduction='sum') for b in beta]\n",
    "\n",
    "wd = 1e3 #hyps[\"best_params\"][\"weight_decay\"]\n",
    "lr = 1e-3 #hyps[\"best_params\"][\"lr\"]\n",
    "nl = 0 # hyps[\"best_params\"][\"noise_level\"]\n",
    "weight_decay = [wd for _ in range(len(nets))] \n",
    "opts = [torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd) for net, wd in zip(nets, weight_decay)]\n",
    "\n",
    "net_reg_weight = [0 for _ in range(len(nets))]\n",
    "noise_level = [nl for _ in range(len(nets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "batch_size = num_timepoints//10\n",
    "num_epochs = 800\n",
    "num_nets = len(nets)\n",
    "\n",
    "train_loss = torch.zeros((num_nets, num_epochs))\n",
    "train_reg = torch.zeros((num_nets, num_epochs))\n",
    "train_score = torch.zeros((num_nets, num_epochs))\n",
    "traintest_loss = torch.zeros((num_nets, num_epochs))\n",
    "traintest_score = torch.zeros((num_nets, num_epochs))\n",
    "\n",
    "train_source = train_source.to(device)\n",
    "train_target = train_target.to(device)\n",
    "test_source = test_source.to(device)\n",
    "test_target = test_target.to(device)\n",
    "\n",
    "for net in nets:\n",
    "    net.train()\n",
    "    \n",
    "progress = tqdm(range(num_epochs), desc='Training Networks')\n",
    "for epoch in progress:\n",
    "            \n",
    "    itime = torch.randperm(num_timepoints)[:batch_size]\n",
    "    \n",
    "    source_batch = train_source[:, itime].T\n",
    "    target_batch = train_target[:, itime].T\n",
    "\n",
    "    for opt in opts:\n",
    "        opt.zero_grad()\n",
    "\n",
    "    predictions = [net(source_batch + nl * torch.randn_like(source_batch)) for net, nl in zip(nets, noise_level)]\n",
    "    mulogvar = [pred[1:] if b else (torch.tensor(0, device=device), torch.tensor(0, device=device)) for pred, b in zip(predictions, betavae)]\n",
    "    predictions = [pred[0] if b else pred for pred, b in zip(predictions, betavae)]\n",
    "    losses = [loss_fn(pred, target_batch) for pred, loss_fn in zip(predictions, loss_functions)]\n",
    "    regs = []\n",
    "    for b, mlv, reg, pred in zip(betavae, mulogvar, regularizers, predictions):\n",
    "        if b:\n",
    "            regs.append(reg(*mlv))\n",
    "        else:\n",
    "            regs.append(reg(source_batch, pred))\n",
    "    full_losses = [loss + weight * reg for loss, reg, weight in zip(losses, regs, net_reg_weight)]\n",
    "    for loss in full_losses:\n",
    "        loss.backward()\n",
    "    \n",
    "    for opt in opts:\n",
    "        opt.step()\n",
    "    \n",
    "    scores = [net.score(source_batch, target_batch) for net in nets]\n",
    "\n",
    "    for inet in range(len(nets)):\n",
    "        train_loss[inet, epoch] = losses[inet].item()\n",
    "        train_reg[inet, epoch] = regs[inet].item()\n",
    "        train_score[inet, epoch] = scores[inet].item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for net in nets:\n",
    "                net.eval()\n",
    "            pred = nets[inet](test_source.T)\n",
    "            if betavae[inet]:\n",
    "                pred = pred[0]\n",
    "            traintest_loss[inet, epoch] = loss_functions[inet](pred, test_target.T).item()\n",
    "            traintest_score[inet, epoch] = nets[inet].score(test_source.T, test_target.T).item()\n",
    "            for net in nets:\n",
    "                net.train()\n",
    "        \n",
    "    progress.set_postfix({'Loss': losses[0].item(), 'Score': scores[0].item(), \"Reg\": regs[0].item()})\n",
    "\n",
    "for net in nets:\n",
    "    net.eval()\n",
    "    \n",
    "test_predictions = [net(test_source.T) for net in nets]\n",
    "test_mulogvar = [pred[1:] if b else (torch.tensor(0, device=device), torch.tensor(0, device=device)) for pred, b in zip(test_predictions, betavae)]\n",
    "test_predictions = [pred[0] if b else pred for pred, b in zip(test_predictions, betavae)]\n",
    "test_losses = [loss_fn(test_prediction, test_target.T) for test_prediction, loss_fn in zip(test_predictions, loss_functions)]\n",
    "test_regs = []\n",
    "for b, mlv, reg, pred in zip(betavae, test_mulogvar, regularizers, test_predictions):\n",
    "    if b:\n",
    "        test_regs.append(reg(*mlv))\n",
    "    else:\n",
    "        test_regs.append(reg(test_source.T, pred))\n",
    "test_scores = [net.score(test_source.T, test_target.T) for net in nets]\n",
    "\n",
    "for inet in range(len(nets)):\n",
    "    print(f\"Net{inet} Test Loss: {test_losses[inet].item():.3f}, \" +\n",
    "          f\"Test Score: {test_scores[inet].item():.3f}, \" + \n",
    "          f\"Test Reg: {test_regs[inet].item():.3f}\" + \n",
    "          f\"Maximum Test Score: {traintest_score[inet].max().item():.3f}\")\n",
    "\n",
    "# plot the training loss\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3), layout=\"constrained\")\n",
    "for inet in range(len(nets)):\n",
    "    ax[0].plot(train_loss[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[0].axhline(test_losses[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[1].plot(train_score[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[1].axhline(test_scores[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].set_title('Training Score')\n",
    "# ax[1].set_ylim(-5, 1.0)\n",
    "ax[1].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[2].plot(train_reg[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[2].axhline(test_regs[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Regularization')\n",
    "ax[2].set_title('Training Regularization')\n",
    "ax[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79492e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3), layout=\"constrained\")\n",
    "for inet in range(len(nets)):\n",
    "    ax[0].plot(traintest_loss[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[0].axhline(test_losses[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[1].plot(traintest_score[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[1].axhline(test_scores[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].set_title('Training Score')\n",
    "# ax[1].set_ylim(-5.1, 0.5) #torch.min(traintest_score[:, 2:]), 1.2*torch.max(traintest_score))\n",
    "ax[1].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[2].plot(train_reg[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[2].axhline(test_regs[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Regularization')\n",
    "ax[2].set_title('Training Regularization')\n",
    "ax[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svca = SVCA().fit(train_source, train_target)\n",
    "shared, total = svca.score(train_source, train_target)\n",
    "print(f\"{shared.sum() / total.sum() * 100:.2f}% of the variance is shared between the two groups.\")\n",
    "\n",
    "shared, total = svca.score(test_source, test_target)\n",
    "print(f\"{shared.sum() / total.sum() * 100:.2f}% of the variance is shared between the two groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5200fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea\n",
    "# How to go from one dataset to another? \n",
    "# One way: learn the SVD of the covariance\n",
    "\n",
    "# Question: \n",
    "# How does the SVD of gram between A and B related to a reduced rank solution of predicting B from A? \n",
    "\n",
    "# U S V.T = A.T @ B\n",
    "# AX = B --> X = (A.T @ A)^-1 @ A.T @ B\n",
    "# AX = B --> X = (A.T @ A)^-1 @ U S V.T\n",
    "# AX = B --> X = Q 1/D Q.T @ U S V.T\n",
    "\n",
    "# In general, if we have a SVD map of the gram matrix defining covariance between A and B, \n",
    "# then we can study how each mode of A/B maps onto each PC of A/B. \n",
    "\n",
    "# Simple:\n",
    "# 1. Learn the SVD of the gram matrix between A and B\n",
    "# 2. Learn the PCA of A and B\n",
    "# 3. Get the OLS solution to transform SVD modes to PCA modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b13e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# choose a session randomly that has registered imaging data\n",
    "vrexp = random.choice(sessiondb.iterSessions(imaging=True, vrRegistration=True))\n",
    "print(vrexp.sessionPrint()) # show which session you chose\n",
    "\n",
    "ospks = vrexp.loadone('mpci.roiActivityDeconvolvedOasis')\n",
    "keep_idx = vrexp.idxToPlanes(keep_planes=[1])\n",
    "ospks = ospks[:, keep_idx]\n",
    "\n",
    "time_split_prms = dict(\n",
    "    relative_size=[10, 1],\n",
    "    chunks_per_group=25,\n",
    "    num_buffer=10,\n",
    ")\n",
    "npop = Population(ospks.T, generate_splits=True, time_split_prms=time_split_prms)\n",
    "print(npop.size())\n",
    "\n",
    "# get eigenvalues of the full population to compare with simulated data appropriately\n",
    "npop_evals = PCA().fit(ospks.T).get_eigenvalues()\n",
    "\n",
    "train_source, train_target = npop.get_split_data(0, center=True)\n",
    "test_source, test_target = npop.get_split_data(1, center=True)\n",
    "\n",
    "print(train_source.shape, train_target.shape, test_source.shape, test_target.shape)\n",
    "\n",
    "data_cross = CrossCompare().fit(train_source, train_target)\n",
    "\n",
    "\n",
    "N = npop.size(0)//2\n",
    "T = 5000\n",
    "Ttest = 1000\n",
    "Q = torch.linalg.qr(torch.normal(0, 1, (2*N, 2*N)))[0]\n",
    "D = npop_evals[:2*N]\n",
    "\n",
    "train_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, T))\n",
    "test_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, Ttest))\n",
    "train_data = Q @ train_scores\n",
    "test_data = Q @ test_scores\n",
    "\n",
    "train_source = train_data[:N]\n",
    "train_target = train_data[N:]\n",
    "test_source = test_data[:N]\n",
    "test_target = test_data[N:]\n",
    "\n",
    "sim_cross = CrossCompare().fit(train_source, train_target)\n",
    "\n",
    "to_pca = True\n",
    "d_source_com, d_target_com, d_source_entropy, d_target_entropy = data_cross.analyze(to_pca=to_pca)\n",
    "source_com, target_com, source_entropy, target_entropy = sim_cross.analyze(to_pca=to_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b07e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(6, 6), layout=\"constrained\")\n",
    "\n",
    "d_crossmap = data_cross.u_to_pc if to_pca else data_cross.pc_to_u\n",
    "crossmap = sim_cross.u_to_pc if to_pca else sim_cross.pc_to_u\n",
    "\n",
    "ax[0, 0].imshow(torch.abs(d_crossmap), aspect='auto', cmap='pink', interpolation=\"None\")\n",
    "ax[0, 0].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[0, 0].set_ylabel(\"U Mode\" if to_pca else \"Principal Component\")\n",
    "\n",
    "ax[0, 1].imshow(torch.abs(crossmap), aspect='auto', cmap='pink', interpolation=\"None\")\n",
    "ax[0, 1].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[0, 1].set_ylabel(\"U Mode\" if to_pca else \"Principal Component\")\n",
    "\n",
    "ax[1, 0].axline((0, 0), slope=1, color='k', linewidth=0.5, linestyle='--')\n",
    "ax[1, 0].plot(source_com, color='k', label=\"Simulated\")\n",
    "ax[1, 0].plot(target_com, color='k')\n",
    "ax[1, 0].plot(d_source_com, color='b', label=\"Data\")\n",
    "ax[1, 0].plot(d_target_com, color='b')\n",
    "ax[1, 0].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[1, 0].set_ylabel('Center of Mass ' + (\"SVD Mode\" if to_pca else \"Principal Component\"))\n",
    "ax[1, 0].set_title('Map Dimension CoM')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "ax[1, 1].plot(source_entropy, color='k', label=\"Source\")\n",
    "ax[1, 1].plot(target_entropy, color='k')\n",
    "ax[1, 1].plot(d_source_entropy, color='b', label=\"Data\")\n",
    "ax[1, 1].plot(d_target_entropy, color='b')\n",
    "ax[1, 1].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[1, 1].set_ylabel('Entropy')\n",
    "ax[1, 1].set_title('Entropy')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3c4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sourcemap = sp.ndimage.gaussian_filter(torch.abs(data_cross.u_to_pc.T), 1)\n",
    "targetmap = sp.ndimage.gaussian_filter(torch.abs(data_cross.v_to_pc.T), 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), layout=\"constrained\", sharex=True, sharey=True)\n",
    "ax[0].imshow(sourcemap, cmap=\"hot\", interpolation='gaussian')\n",
    "ax[0].set_xlabel('U Mode')\n",
    "ax[0].set_ylabel('PC Mode')\n",
    "ax[0].set_title('U to PC Source')\n",
    "ax[1].imshow(targetmap, cmap=\"hot\")\n",
    "ax[1].set_title('V to PC Target')\n",
    "ax[1].set_xlabel('V Mode')\n",
    "ax[1].set_ylabel('PC Mode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119caff-5d69-484e-bb16-cc6b820c7d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Database Requirements: \n",
    "# ---------------------\n",
    "# GUI: db manager\n",
    "# - click on entry and do things:\n",
    "#                --> open file explorer to that session\n",
    "#                --> do suite2p\n",
    "#                --> do red cell management\n",
    "# - update table data? \n",
    "# ---------------------\n",
    "# Operational Commands: \n",
    "# - Automatically do suite2p \n",
    "# - Check if registration was done before a suite2p update\n",
    "\n",
    "# Further Requirements:\n",
    "# ---------------------\n",
    "# ROICaT Alignment Tools \n",
    "# Track Red Cell Consistency across days \n",
    "# Now that I've refactored the database code, need to update some things in documentation and probably elsewhere too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f846af3-d4a6-47e3-962b-680643416cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes from meeting with Kenneth:\n",
    "\n",
    "# - skewness (violin plot) of Control & Red -- \n",
    "#     - all - \n",
    "#     - just reliable -- for each session - \n",
    "\n",
    "# Subsample control data for scatter plot\n",
    "# Fisher z transformation (but label by original correlation...)\n",
    "# Question:\n",
    "# -- if reliable on 1 day, is it reliable on other days? \n",
    "# -- make a matrix with source and target, color by fraction of reliable on target out of those reliable on source\n",
    "# -- also do this with your session kernels for control and red\n",
    "# -- also do this for different reliability cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan for attack:\n",
    "# Make a suite of summary figures on a session by session basis and a multisession basis. \n",
    "# I just want to be able to look through a mouse's data and evaluate the behavior, the imaging data, and how well the tracking did.\n",
    "\n",
    "# Inclusions:\n",
    "# 1. Behavioral data (running speed and number of trials across any environments it was in -- also metadata about day in environment...)\n",
    "# 2. Imaging data (example snakes from all environments, both train/test comparisons and remapping comparisons)\n",
    "# 3. Red cell data (number of red cells per plane -- and some examples of red cells?)\n",
    "# 4. Tracking data (number of tracked cells per combination (full matrix!), number of tracked red cells, number of tracked reliable cells per environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROICaT Analysis\n",
    "# 1. Example data figures (from roicat_stats)\n",
    "#    - show two post-alignment FOVs, highlight a few tracked neurons and a few (nearby) un-tracked neurons with colors\n",
    "#    - below that, show the place field tuning in each session, color-coded the same way as the neurons ROI plot\n",
    "\n",
    "# 2. Analysis of ROICaT agreement with functional data\n",
    "#    - scatter plot of sConj & place field correlation (with \"labels\" pairs colored differently)\n",
    "#    - mouse by mouse, session by session mean lines comparing average place field correlation of same pairs with different pairs\n",
    "#        -- (imagining lines from 0->1 for mouse 1 of each sessions mean same/diff pfCorr, then also in 2->3 for mouse 2, and 4->5 for mouse 3, etc.)\n",
    "#        -- can also have a supplemental plot showing distribution of same/diff pfCorr across each session pair? \n",
    "\n",
    "# 3. Control analysis with null model test (empirical version with subsampled null distribution)\n",
    "\n",
    "# 4. Control analysis with bayesian model\n",
    "# -- get pfCorr_withinSession (this is pairs of ROIs within a session, and should be representative of pfCorr_diff_acrossSession)\n",
    "# -- normalize pfCorr_all_acrossSession by number of pairs, subtract density of pfCorr_withinSession\n",
    "# -- remaining pfCorr_remain_acrossSession = pfCorr_same_acrossSession\n",
    "\n",
    "\n",
    "# ---- note ----\n",
    "# - should probably include target reliable pairs not represented in the source only reliable category..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: remove multipage tiff from: C:\\Users\\Andrew\\Documents\\localData\\ATL012\\2023-02-09\\701\\suite2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a664d70-de8c-44fd-a1c4-4d4bf6388542",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_red = track.check_red_cell_consistency(idx_ses=idx_ses, keepPlanes=None, use_s2p=True)\n",
    "idx_has_red = idx_red[:, np.any(idx_red, axis=0)]\n",
    "idx_sort = np.argsort(-np.sum(idx_has_red,axis=0))\n",
    "idx_plot = idx_has_red[:, idx_sort]\n",
    "plt.close('all')\n",
    "plt.imshow(idx_plot, aspect='auto', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efb65e-f390-403d-b8c8-89f816683ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_red = track.check_red_cell_consistency(idx_ses=idx_ses, keepPlanes=None)\n",
    "idx_has_red = idx_red[:, np.sum(idx_red, axis=0)>0]\n",
    "idx_sort = np.argsort(-np.sum(idx_has_red,axis=0))\n",
    "idx_plot = idx_has_red[:, idx_sort]\n",
    "plt.close('all')\n",
    "plt.imshow(idx_plot, aspect='auto', interpolation='none')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
