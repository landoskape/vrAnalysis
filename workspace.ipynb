{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62912f35-2f40-4d93-872e-439919754adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# pd.options.display.width = 1000\n",
    "\n",
    "from vrAnalysis import analysis\n",
    "from vrAnalysis import helpers\n",
    "from vrAnalysis import database\n",
    "from vrAnalysis import tracking\n",
    "from vrAnalysis import session\n",
    "from vrAnalysis import registration\n",
    "from vrAnalysis import fileManagement as fm\n",
    "\n",
    "import faststats as fs\n",
    "\n",
    "# from vrAnalysis.uiDatabase import addEntryGUI\n",
    "from vrAnalysis.redgui import redCellGUI as rgui\n",
    "\n",
    "from dimilibi import CrossCompare\n",
    "from dimilibi import SVCANet, HurdleNet, BetaVAE\n",
    "from dimilibi import Population\n",
    "from dimilibi import SVCA\n",
    "from dimilibi import PCA\n",
    "from dimilibi import RidgeRegression, ReducedRankRegression\n",
    "from dimilibi import LocalSimilarity, FlexibleFilter, EmptyRegularizer, BetaVAE_KLDiv\n",
    "\n",
    "sessiondb = database.vrDatabase('vrSessions')\n",
    "mousedb = database.vrDatabase('vrMice')\n",
    "\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f802b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0, 1, 2, 3, 4, 5, 6, 7, 8], 3: [1, 2, 3, 4, 5, 6, 7, 8], 4: [6, 7, 8]}\n",
      "3 [1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "mouse_name = \"ATL058\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False, keep_planes=[1, 2, 3, 4], speedThreshold=1)\n",
    "env_stats = pcm.env_stats()\n",
    "print(env_stats)\n",
    "\n",
    "envs = list(env_stats.keys())\n",
    "first_session = [env_stats[env][0] for env in envs]\n",
    "idx_first_session = np.argsort(first_session)\n",
    "\n",
    "# use environment that was introduced second\n",
    "use_environment = envs[idx_first_session[1]]\n",
    "idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    # Attempt to use first environment if not enough sessions in second\n",
    "    use_environment = envs[idx_first_session[0]]\n",
    "    idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    print(f\"Skipping {mouse_name} due to not enough sessions!\")\n",
    "\n",
    "print(use_environment, idx_ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b98a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:53<00:00,  6.64s/it]\n",
      "Measuring reliability...: 100%|██████████| 8/8 [00:00<00:00, 319.99session/s]\n"
     ]
    }
   ],
   "source": [
    "envnum = use_environment\n",
    "max_diff = 4\n",
    "relcor_cutoff = 0.5\n",
    "smooth = 10\n",
    "\n",
    "bins_cor = np.linspace(-1, 1, 21)\n",
    "bins_mse = np.linspace(-4, 1, 21)\n",
    "centers_cor = helpers.edge2center(bins_cor)\n",
    "centers_mse = helpers.edge2center(bins_mse)\n",
    "\n",
    "def make_histograms(pcm, idx_ses):\n",
    "    pcm.load_pcss_data(idx_ses=idx_ses)\n",
    "    ctl_relcor = []\n",
    "    red_relcor = []\n",
    "    ctl_relmse = []\n",
    "    red_relmse = []\n",
    "    for idx in tqdm(idx_ses, desc=\"Measuring reliability...\", unit=\"session\"):\n",
    "        relmse, relcor = map(lambda x: x[0], pcm.pcss[idx].get_reliability_values(envnum=envnum))\n",
    "        idx_red = pcm.pcss[idx].vrexp.getRedIdx(keep_planes=pcm.keep_planes)\n",
    "        ctl_relcor.append(helpers.fractional_histogram(relcor[~idx_red], bins_cor)[0])\n",
    "        red_relcor.append(helpers.fractional_histogram(relcor[idx_red], bins_cor)[0])\n",
    "        ctl_relmse.append(helpers.fractional_histogram(relmse[~idx_red], bins_mse)[0])\n",
    "        red_relmse.append(helpers.fractional_histogram(relmse[idx_red], bins_mse)[0])\n",
    "\n",
    "    return ctl_relcor, red_relcor, ctl_relmse, red_relmse\n",
    "\n",
    "ctl_relcor, red_relcor, ctl_relmse, red_relmse = make_histograms(pcm, idx_ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c95042f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, len(idx_ses), figsize=(15, 7), layout=\"constrained\")\n",
    "for idx, (cc, rc, cm, rm) in enumerate(zip(ctl_relcor, red_relcor, ctl_relmse, red_relmse)):\n",
    "    ax[0, idx].plot(centers_cor, cc, linewidth=1.2, color=\"k\", label=\"Ctl\")\n",
    "    ax[0, idx].plot(centers_cor, rc, linewidth=1.2, color=\"r\", label=\"Red\")\n",
    "    ax[1, idx].plot(centers_mse, cm, linewidth=1.2, color=\"k\", label=\"Ctl\")\n",
    "    ax[1, idx].plot(centers_mse, rm, linewidth=1.2, color=\"r\", label=\"Red\")\n",
    "    ax[0, idx].set_title(f\"Session {idx_ses[idx]}\")\n",
    "    ax[0, idx].set_xlabel(\"Correlation\")\n",
    "    ax[1, idx].set_xlabel(\"MSE\")\n",
    "    ax[0, idx].set_ylabel(\"Fraction\")\n",
    "    ax[1, idx].set_ylabel(\"Fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b776cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [4, 5, 6], 3: [1, 2, 3, 4, 5, 6], 4: [0, 1, 2, 3, 4, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "mouse_name = \"ATL060\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False, keep_planes=[1, 2, 3, 4], speedThreshold=1)\n",
    "env_stats = pcm.env_stats()\n",
    "print(env_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bfb449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4] [4, 1, 0] 3\n"
     ]
    }
   ],
   "source": [
    "envs = list(env_stats.keys())\n",
    "first_session = [env_stats[env][0] for env in envs]\n",
    "idx_first_session = np.argsort(first_session)\n",
    "\n",
    "# use environment that was introduced second\n",
    "use_environment = envs[idx_first_session[1]]\n",
    "idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    # Attempt to use first environment if not enough sessions in second\n",
    "    use_environment = envs[idx_first_session[0]]\n",
    "    idx_ses = env_stats[use_environment][: min(12, len(env_stats[use_environment]))]\n",
    "\n",
    "if len(idx_ses) < 2:\n",
    "    print(f\"Skipping {mouse_name} due to not enough sessions!\")\n",
    "\n",
    "print(envs, first_session, use_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3f7e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:   7%|▋         | 1/14 [00:01<00:23,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] 378 53 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  14%|█▍        | 2/14 [00:03<00:21,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3] 347 48 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  21%|██▏       | 3/14 [00:04<00:17,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4] 178 30 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  29%|██▊       | 4/14 [00:06<00:15,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5] 161 30 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  36%|███▌      | 5/14 [00:08<00:14,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] 332 46 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  43%|████▎     | 6/14 [00:09<00:12,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4] 179 34 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  50%|█████     | 7/14 [00:11<00:10,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5] 172 32 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  57%|█████▋    | 8/14 [00:12<00:09,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 6] 127 31 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  64%|██████▍   | 9/14 [00:14<00:07,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4] 169 36 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  71%|███████▏  | 10/14 [00:15<00:06,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5] 152 34 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  79%|███████▊  | 11/14 [00:16<00:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6] 130 31 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  86%|████████▌ | 12/14 [00:18<00:02,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5] 243 50 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing correlations:  93%|█████████▎| 13/14 [00:19<00:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6] 208 48 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6] 148 41 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "envnum = use_environment\n",
    "max_diff = 4\n",
    "relcor_cutoff = 0.5\n",
    "smooth = 10\n",
    "\n",
    "def make_corrs(pcm, idx_ses, max_diff=None):\n",
    "    pcm.load_pcss_data(idx_ses=idx_ses)\n",
    "    idx_pairs = helpers.all_pairs(idx_ses)\n",
    "    if max_diff is not None:\n",
    "        idx_pairs = idx_pairs[np.abs(idx_pairs[:, 1] - idx_pairs[:, 0]) <= max_diff]\n",
    "    ses_diffs = idx_pairs[:, 1] - idx_pairs[:, 0]\n",
    "    ctlcorrs = []\n",
    "    redcorrs = []\n",
    "    for idx in tqdm(idx_pairs, desc=\"computing correlations\", leave=False):\n",
    "        spkmaps, _, relcor, _, _, idx_red, _ = pcm.get_spkmaps(envnum, trials=\"full\", pop_nan=True, smooth=smooth, average=True, idx_ses=idx)\n",
    "        any_rel = np.any(np.stack(relcor) > relcor_cutoff, axis=0)\n",
    "        any_red = np.any(np.stack(idx_red), axis=0)\n",
    "        print(idx, np.sum(any_rel), np.sum(any_red), np.sum(any_rel & any_red))\n",
    "        ctlmaps = [s[~any_red & any_rel] for s in spkmaps]\n",
    "        redmaps = [s[any_red & any_rel] for s in spkmaps]\n",
    "        ctlcorrs.append(helpers.vectorCorrelation(ctlmaps[0], ctlmaps[1], axis=1))\n",
    "        redcorrs.append(helpers.vectorCorrelation(redmaps[0], redmaps[1], axis=1))\n",
    "    return ctlcorrs, redcorrs, ses_diffs\n",
    "\n",
    "ctlcorrs, redcorrs, ses_diffs = make_corrs(pcm, idx_ses, max_diff=max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e67dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-1, 1, 11)\n",
    "centers = helpers.edge2center(bins)\n",
    "\n",
    "num_diffs = len(np.unique(ses_diffs))\n",
    "ctl_counts = np.zeros((num_diffs, len(centers)))\n",
    "red_counts = np.zeros((num_diffs, len(centers)))\n",
    "for idiff in range(num_diffs):\n",
    "    idx = ses_diffs == (idiff + 1)\n",
    "    c_ctlcorrs = np.concatenate([c for i, c in enumerate(ctlcorrs) if idx[i]])\n",
    "    c_redcorrs = np.concatenate([c for i, c in enumerate(redcorrs) if idx[i]])\n",
    "    ctl_counts[idiff] = helpers.fractional_histogram(c_ctlcorrs, bins=bins)[0]\n",
    "    red_counts[idiff] = helpers.fractional_histogram(c_redcorrs, bins=bins)[0]\n",
    "\n",
    "fig, ax = plt.subplots(2, num_diffs, figsize=(12, 4), layout=\"constrained\")\n",
    "for i in range(num_diffs):\n",
    "    ax[0, i].plot(centers, ctl_counts[i], color=\"k\", lw=1)\n",
    "    ax[0, i].plot(centers, red_counts[i], color=\"r\", lw=1)\n",
    "    ax[0, i].set_title(f\"$\\Delta$ Session: {i + 1}\")\n",
    "    ax[1, i].plot(centers, red_counts[i] - ctl_counts[i], color=\"k\", lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f2fe5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:15<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1093, 194), (1093, 194), (1093, 194), (1093, 194)]\n",
      "[(1093,), (1093,), (1093,), (1093,)]\n",
      "[15, 18, 17, 15] 22\n",
      "[(1071, 194), (1071, 194), (1071, 194), (1071, 194)]\n",
      "[(22, 194), (22, 194), (22, 194), (22, 194)]\n",
      "[(1071,), (1071,), (1071,), (1071,)]\n",
      "[(22,), (22,), (22,), (22,)]\n"
     ]
    }
   ],
   "source": [
    "envnum = 3\n",
    "idx_ses = [1, 2, 3, 4]\n",
    "num_ses = len(idx_ses)\n",
    "spkmaps, relmse, relcor, pfloc, pfidx, idx_red, roi_idx = pcm.get_spkmaps(envnum, trials=\"full\", average=True, idx_ses=idx_ses)\n",
    "any_red = np.any(np.stack(idx_red), axis=0)\n",
    "print([s.shape for s in spkmaps])\n",
    "print([i.shape for i in idx_red])\n",
    "print([np.sum(i) for i in idx_red], np.sum(any_red))\n",
    "\n",
    "ctlmaps = [s[~any_red] for s in spkmaps]\n",
    "ctlidx = [np.argsort(p[~any_red]) for p in pfloc]\n",
    "redmaps = [s[any_red] for s in spkmaps]\n",
    "redidx = [np.argsort(p[any_red]) for p in pfloc]\n",
    "\n",
    "print([s.shape for s in ctlmaps])\n",
    "print([s.shape for s in redmaps])\n",
    "print([s.shape for s in ctlidx])\n",
    "print([s.shape for s in redidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "65085116",
   "metadata": {},
   "outputs": [],
   "source": [
    "iref = 3\n",
    "\n",
    "ctl_corr_to_ref = [helpers.vectorCorrelation(cm, ctlmaps[iref], axis=1) for cm in ctlmaps]\n",
    "red_corr_to_ref = [helpers.vectorCorrelation(rm, redmaps[iref], axis=1) for rm in redmaps]\n",
    "\n",
    "max_pos = ctlmaps[0].shape[1]\n",
    "num_ctl = ctlmaps[0].shape[0]\n",
    "num_red = redmaps[0].shape[0]\n",
    "extents = [[0, max_pos, 0, num] for num in [num_ctl, num_red]]\n",
    "\n",
    "# Change default font size\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig, ax = plt.subplots(2, num_ses, figsize=(12, 6), layout=\"constrained\")\n",
    "for ises in range(num_ses):\n",
    "    ax[0, ises].imshow(ctlmaps[ises][ctlidx[iref]], extent=extents[0], aspect=\"auto\", vmin=0, vmax=2, cmap=\"hot\", interpolation=\"none\")\n",
    "    ax[1, ises].imshow(redmaps[ises][redidx[iref]], extent=extents[1], aspect=\"auto\", vmin=0, vmax=2, cmap=\"hot\", interpolation=\"none\")\n",
    "    ax[0, ises].set_yticks([])\n",
    "    ax[1, ises].set_yticks([])\n",
    "    ax[1, ises].set_xlabel(f\"Virtual Position (cm)\")\n",
    "    if ises==0:\n",
    "        ax[1, ises].set_ylabel(\"Red Cells\")\n",
    "        ax[0, ises].set_ylabel(\"Control Cells\")\n",
    "    ax[0, ises].set_title((\"REFERENCE\\n\" if ises==iref else \"\") + f\"Session {idx_ses[ises]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eb98690",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-1, 1, 9)\n",
    "centers = helpers.edge2center(bins)\n",
    "\n",
    "ctl_corr_hist = [helpers.fractional_histogram(c, bins=bins)[0] for c in ctl_corr_to_ref]\n",
    "red_corr_hist = [helpers.fractional_histogram(c, bins=bins)[0] for c in red_corr_to_ref]\n",
    "\n",
    "fig, ax = plt.subplots(1, num_ses-1, figsize=(6, 4), layout=\"constrained\")\n",
    "iplot = 0\n",
    "for ises in range(num_ses):\n",
    "    if ises == iref:\n",
    "        continue\n",
    "    ax[iplot].plot(centers, ctl_corr_hist[ises], color=\"k\", lw=1)\n",
    "    ax[iplot].plot(centers, red_corr_hist[ises], color=\"r\", lw=1)\n",
    "    iplot += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fe9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4570971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL027/2023-08-08/701\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "\n",
    "mouse_name = \"ATL027\"\n",
    "ses = random.choice(sessiondb.iterSessions(mouseName=mouse_name, experimentID=3, imaging=True))\n",
    "print(ses.sessionPrint())\n",
    "\n",
    "# Load the session\n",
    "pcss = analysis.placeCellSingleSession(ses, keep_planes=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d0531b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pcss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[273], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m average \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m smooth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m spkmaps \u001b[38;5;241m=\u001b[39m \u001b[43mpcss\u001b[49m\u001b[38;5;241m.\u001b[39mget_spkmap(average\u001b[38;5;241m=\u001b[39maverage, smooth\u001b[38;5;241m=\u001b[39msmooth, trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m num_pos \u001b[38;5;241m=\u001b[39m spkmaps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m average:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pcss' is not defined"
     ]
    }
   ],
   "source": [
    "average = True\n",
    "smooth = None\n",
    "spkmaps = pcss.get_spkmap(average=average, smooth=smooth, trials=\"full\")\n",
    "num_pos = spkmaps[0].shape[-1]\n",
    "if not average:\n",
    "    env_trials = [s.shape[1] for s in spkmaps]\n",
    "    min_trials = min(env_trials)\n",
    "    idx_use_trials = [np.random.permutation(etr)[:min_trials] for etr in env_trials]\n",
    "    spkmaps = [s[:, idx_use_trials[i]] for i, s in enumerate(spkmaps)]\n",
    "    # each row is a neuron with each trial concatenated along columns\n",
    "    spkmaps = [s.reshape(s.shape[0], -1) for s in spkmaps]\n",
    "\n",
    "reliable_only = True\n",
    "if reliable_only:\n",
    "    idx_reliable = pcss.get_reliable(cutoffs=(0.3, 0.6))\n",
    "    any_reliable = np.any(np.stack(idx_reliable, axis=0), axis=0)\n",
    "    spkmaps = [s[any_reliable] for s in spkmaps]\n",
    "\n",
    "pos_colormaps = [\"coolwarm\", \"coolwarm\", \"coolwarm\"] #, \"spring\", \"cool\"]\n",
    "env_colors = [\"k\", \"r\", \"b\"]\n",
    "\n",
    "pos_colors = [mpl.colormaps[cm](np.linspace(0, 1, num_pos)) for cm in pos_colormaps]\n",
    "env_colors = [np.tile(np.array(mpl.colors.to_rgba(c)).reshape(1, -1), (num_pos, 1)) for c in env_colors]\n",
    "if not average:\n",
    "    pos_colors = [np.tile(pc, (min_trials, 1)) for pc in pos_colors]\n",
    "    env_colors = [np.tile(ec, (min_trials, 1)) for ec in env_colors]\n",
    "print([s.shape for s in spkmaps])\n",
    "print([s.shape for s in env_colors], [s.shape for s in pos_colors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a7727978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390, 2) (195, 2) (585, 2)\n"
     ]
    }
   ],
   "source": [
    "test_env = 2\n",
    "\n",
    "train_data = np.concatenate([spkmaps[envidx] for envidx in range(len(spkmaps)) if envidx != test_env], axis=1)\n",
    "test_data = spkmaps[test_env]\n",
    "\n",
    "train_colors_pos = np.concatenate([pos_colors[envidx] for envidx in range(len(spkmaps)) if envidx != test_env], axis=0)\n",
    "test_colors_pos = pos_colors[test_env]\n",
    "train_colors_env = np.concatenate([env_colors[envidx] for envidx in range(len(spkmaps)) if envidx != test_env], axis=0)\n",
    "test_colors_env = env_colors[test_env]\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=5, n_components=2).fit(train_data.T)\n",
    "train_embedding = reducer.transform(train_data.T)\n",
    "test_embedding = reducer.transform(test_data.T)\n",
    "\n",
    "reducer_full = umap.UMAP(n_neighbors=5, n_components=2).fit(np.concatenate(spkmaps, axis=1).T)\n",
    "full_embedding = reducer_full.transform(np.concatenate(spkmaps, axis=1).T)\n",
    "\n",
    "print(train_embedding.shape, test_embedding.shape, full_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c4d328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "ax[0, 0].scatter(train_embedding[:, 0], train_embedding[:, 1], c=train_colors_pos, s=10)\n",
    "ax[0, 0].scatter(test_embedding[:, 0], test_embedding[:, 1], c=test_colors_pos, s=10)\n",
    "ax[0, 1].scatter(train_embedding[:, 0], train_embedding[:, 1], c=train_colors_env, s=10)\n",
    "ax[0, 1].scatter(test_embedding[:, 0], test_embedding[:, 1], c=test_colors_env, s=10)\n",
    "ax[1, 0].scatter(full_embedding[:, 0], full_embedding[:, 1], c=np.concatenate(pos_colors, axis=0), s=10)\n",
    "ax[1, 1].scatter(full_embedding[:, 0], full_embedding[:, 1], c=np.concatenate(env_colors, axis=0), s=10)\n",
    "\n",
    "for a in ax.flatten():\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "for a in ax[1, :]:\n",
    "    a.set_xlabel(\"UMAP 1\")\n",
    "for a in ax[:, 0]:\n",
    "    a.set_ylabel(\"UMAP 2\")\n",
    "ax[0, 0].set_title(\"Color by Position\")\n",
    "ax[0, 1].set_title(\"Color by Environment\")\n",
    "ax[0, 0].set_ylabel(\"Train(BlackRed) vs Test(BLUE)\\n\\n\\nUMAP 2\")\n",
    "ax[1, 0].set_ylabel(\"Full Embedding\\n\\n\\nUMAP 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ee67980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "spkmaps = pcss.get_spkmap(average=True, trials=\"full\")\n",
    "idx_reliable = pcss.get_reliable(cutoffs=(0.3, 0.6))\n",
    "any_reliable = np.any(np.stack(idx_reliable, axis=0), axis=0)\n",
    "\n",
    "def get_kernels(data):\n",
    "    dot_product = np.dot(data.T, data)\n",
    "    norms = np.linalg.norm(data, axis=0)\n",
    "    cosine_angle = dot_product / np.outer(norms, norms)\n",
    "    kernel = np.corrcoef(data.T)\n",
    "    return cosine_angle, kernel\n",
    "\n",
    "angle, kernel = get_kernels(np.concatenate(spkmaps, axis=1))\n",
    "rel_angle, rel_kernel = get_kernels(np.concatenate(spkmaps, axis=1)[any_reliable])\n",
    "\n",
    "edges = [num_pos * i for i in range(1, len(spkmaps))]\n",
    "extent = [0, angle.shape[0], 0, angle.shape[1]]\n",
    "ticks = [num_pos/2 + num_pos * i for i in range(len(spkmaps))]\n",
    "labels = [f\"Env {i}\" for i in range(len(spkmaps))]\n",
    "\n",
    "cmap = \"bwr\"\n",
    "\n",
    "# Create figure and gridspec\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "gs = gridspec.GridSpec(2, 3, width_ratios=[1, 1, 0.2])\n",
    "\n",
    "# Create the main plots\n",
    "ax00 = fig.add_subplot(gs[0, 0])\n",
    "ax01 = fig.add_subplot(gs[0, 1])\n",
    "ax10 = fig.add_subplot(gs[1, 0])\n",
    "ax11 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Create colorbar axis that spans both rows\n",
    "cax = fig.add_subplot(gs[:, 2])  # This spans both rows\n",
    "\n",
    "# Create the plots\n",
    "im = ax00.imshow(angle, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "ax01.imshow(kernel, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "ax10.imshow(rel_angle, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "ax11.imshow(rel_kernel, extent=extent, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "# Add grid lines\n",
    "for edge in edges:\n",
    "    for ax in [ax00, ax01, ax10, ax11]:\n",
    "        ax.axhline(edge, color=\"k\", lw=0.5)\n",
    "        ax.axvline(edge, color=\"k\", lw=0.5)\n",
    "for ax in [ax00, ax01, ax10, ax11]:\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticks([])\n",
    "for ax in [ax00, ax10]:\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(reversed(labels))\n",
    "ax00.set_title(\"Cosine Angle\")\n",
    "ax01.set_title(\"Correlation\")\n",
    "ax00.set_ylabel(\"All cells\")\n",
    "ax10.set_ylabel(\"Reliable cells\")\n",
    "\n",
    "# Create colorbar\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb1608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9aaa3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from vrAnalysis2.external.pettit2022 import find_pettit_harvey_sessions, data_path\n",
    "sessions = find_pettit_harvey_sessions(data_path / \"dataFolder\")\n",
    "\n",
    "behavior = sessions[0].behavior\n",
    "spks = sessions[0].spks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from vrAnalysis2 import files\n",
    "from vrAnalysis import database\n",
    "sessiondb = database.vrDatabase('vrSessions')\n",
    "from typing import Union\n",
    "\n",
    "def find_experiment_options(root_dir: Union[str, Path]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Find all vrExperimentOptions.json files in the given directory and its subdirectories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root_dir : str or Path\n",
    "        The root directory to start the search from\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[Path]\n",
    "        List of paths to all matching files\n",
    "    \"\"\"\n",
    "    def make_identifier(pth: Path) -> list[str]:\n",
    "        return \"_\".join(list(reversed([p.stem for p in list(pth.parents)[:3]])))\n",
    "    \n",
    "    root_path = Path(root_dir)\n",
    "    all_paths = list(root_path.rglob(\"vrExperimentOptions.json\"))\n",
    "    session_identifier = [make_identifier(pth) for pth in all_paths]\n",
    "    return all_paths, session_identifier\n",
    "\n",
    "pths, sids = find_experiment_options(files.local_data_path())\n",
    "csesids = [sessiondb.sessionPrint(joinby=\"_\") for sessiondb in sessiondb.iterSessions(useDefault=True)]\n",
    "\n",
    "for sid in sids:\n",
    "    if sid not in csesids:\n",
    "        print(\"oops\", sid)\n",
    "\n",
    "for ses in sessiondb.iterSessions():\n",
    "    csesid = ses.sessionPrint(joinby=\"_\")\n",
    "    print(csesid, csesid in sids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70933a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessiondb.printSessions(mouseName=\"ATL057\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fb78f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(d, length_scale=1.0):\n",
    "    \"\"\"Covariance function r(x - x') for scalar distance d\"\"\"\n",
    "    return np.exp(-0.5 * (d**2) / length_scale**2)\n",
    "\n",
    "def generate_cov(x, length_scale=1.0):\n",
    "    \"\"\"Generate covariance matrix K(x, x') for a given x\"\"\"\n",
    "    distances = np.abs(x - x[0])\n",
    "    first_row = r(distances, length_scale)\n",
    "    return sp.linalg.toeplitz(first_row)\n",
    "\n",
    "L = 0.2\n",
    "NP = 801\n",
    "N = 1e6\n",
    "sigma = 0.015\n",
    "theta = 1.3\n",
    "x = np.linspace(0, L, NP)  # 1D space from 0 to 10\n",
    "K = generate_cov(x, length_scale=sigma)  # Generate covariance matrix\n",
    "h = np.random.multivariate_normal(mean=np.zeros(len(x)), cov=K, size=int(N))  # Generate h(x) as a sample from GP[0, r(x - x')]\n",
    "f = np.maximum(0, h - theta)\n",
    "Ch = np.cov(h.T)\n",
    "Cf = np.cov(f.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0f1a7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting spectra data for CR_Hippocannula6\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for CR_Hippocannula7\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL022\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL027\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL028\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL020\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL012\n",
      "Successfully loaded temporary data for variance structure analysis.\n",
      "Getting spectra data for ATL045\n",
      "Successfully loaded temporary data for variance structure analysis.\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from vrAnalysis.helpers import AttributeDict, cutoff_type, positive_float\n",
    "from vrAnalysis.analysis.variance_structure import load_spectra_data\n",
    "\n",
    "MOUSE_NAMES = [\n",
    "    \"CR_Hippocannula6\",\n",
    "    \"CR_Hippocannula7\",\n",
    "    \"ATL022\",\n",
    "    \"ATL027\",\n",
    "    \"ATL028\",\n",
    "    \"ATL020\",\n",
    "    \"ATL012\",\n",
    "    \"ATL045\",\n",
    "]\n",
    "CUTOFFS = (0.4, 0.7)\n",
    "MAXCUTOFFS = None\n",
    "\n",
    "def get_spectra(mouse_name, args):\n",
    "    \"\"\"method for analyzing and plotting spectra with cvPCA and cvFOURIER analyses\"\"\"\n",
    "    # load spectra data (use temp if it matches)\n",
    "    track = tracking.tracker(mouse_name)  # get tracker object for mouse\n",
    "    pcm = analysis.placeCellMultiSession(track, autoload=False)  # open up place cell multi session analysis object (don't autoload!!!)\n",
    "\n",
    "    single_args = AttributeDict(vars(args))\n",
    "    single_args[\"mouse_name\"] = mouse_name\n",
    "\n",
    "    spectra_dictionary = load_spectra_data(pcm, single_args, save_as_temp=False, reload=False)\n",
    "\n",
    "    # return the dictionary\n",
    "    return spectra_dictionary\n",
    "\n",
    "\n",
    "def handle_inputs(inputs=[\"--do-spectra\"]):\n",
    "    \"\"\"method for creating and parsing input arguments\"\"\"\n",
    "    parser = ArgumentParser(description=\"do summary plots for a mouse\")\n",
    "    parser.add_argument(\n",
    "        \"--mouse-names\",\n",
    "        type=str,\n",
    "        nargs=\"*\",\n",
    "        default=\"processed\",\n",
    "        help=\"which mice to compare (list of mouse names, or like default), (default='all')\",\n",
    "    )\n",
    "    parser.add_argument(\"--cutoffs\", nargs=\"*\", type=cutoff_type, default=CUTOFFS, help=f\"cutoffs for reliability (default={CUTOFFS})\")\n",
    "    parser.add_argument(\"--maxcutoffs\", nargs=\"*\", type=cutoff_type, default=MAXCUTOFFS, help=\"maxcutoffs for reliability cells (default=None)\")\n",
    "    parser.add_argument(\"--do-spectra\", default=False, action=\"store_true\", help=\"create spectrum plots for mouse (default=False)\")\n",
    "    parser.add_argument(\"--dist-step\", default=1, type=float, help=\"dist-step for creating spkmaps (default=1cm)\")\n",
    "    parser.add_argument(\"--smooth\", default=0.1, type=positive_float, help=\"smoothing width for spkmaps (default=0.1cm)\")\n",
    "    parser.add_argument(\"--reload-spectra-data\", default=False, action=\"store_true\", help=\"reload spectra data (default=False)\")\n",
    "    args = parser.parse_args(inputs)\n",
    "\n",
    "    # if mouse_names is \"all\", get all mouse names from the database\n",
    "    if args.mouse_names == \"all\":\n",
    "        # mousedb = database.vrDatabase(\"vrSessions\")\n",
    "        mousedb = database.vrDatabase(\"vrMice\")\n",
    "        df = mousedb.getTable(trackerExists=True)\n",
    "        mouse_names = df[\"mouseName\"].unique()\n",
    "        args.mouse_names = mouse_names\n",
    "    elif args.mouse_names == \"processed\":\n",
    "        args.mouse_names = MOUSE_NAMES\n",
    "\n",
    "    # return the parsed arguments\n",
    "    return args\n",
    "\n",
    "# analyze spectra and make plots\n",
    "args = handle_inputs()\n",
    "pcms = []\n",
    "spectra_data = []\n",
    "for mouse in MOUSE_NAMES:\n",
    "    print(f\"Getting spectra data for {mouse}\")\n",
    "    spectra_data.append(get_spectra(mouse, args))  # Each is a dictionary of all the spectral output data\n",
    "    c_track = tracking.tracker(mouse)\n",
    "    c_pcm = analysis.placeCellMultiSession(c_track, autoload=False)\n",
    "    pcms.append(c_pcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e4d0b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uSessionID\n",
      "mouseName\n",
      "sessionDate\n",
      "sessionID\n",
      "experimentType\n",
      "experimentID\n",
      "variableGain\n",
      "behavior\n",
      "imaging\n",
      "faceCamera\n",
      "vrEnvironments\n",
      "headPlateRotation\n",
      "numPlanes\n",
      "planeSeparation\n",
      "pockelsPercentage\n",
      "objectiveRotation\n",
      "vrRegistration\n",
      "suite2p\n",
      "suite2pQC\n",
      "redCellQC\n",
      "sessionQC\n",
      "scratchJustification\n",
      "logtime\n",
      "sessionNotes\n",
      "suite2pDate\n",
      "vrRegistrationDate\n",
      "vrRegistrationError\n",
      "vrRegistrationException\n",
      "redCellQCDate\n",
      "vrBehaviorVersion\n",
      "dontTrack\n"
     ]
    }
   ],
   "source": [
    "for fieldName in sessiondb.tableData()[0]:\n",
    "    print(fieldName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a1ae4079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL020/2023-04-05/701\n",
      "ATL022/2023-04-06/701\n",
      "ATL022/2023-03-27/701\n",
      "ATL023/2023-04-28/702\n",
      "ATL045/2024-01-26/701\n",
      "ATL020/2023-03-31/701\n",
      "ATL022/2023-03-27/701\n",
      "ATL020/2023-04-04/701\n",
      "ATL045/2024-01-24/701\n",
      "ATL027/2023-07-21/701\n"
     ]
    }
   ],
   "source": [
    "ises = np.random.choice(sessiondb.iterSessions(imaging=True, vrRegistration=True, experimentID=1), 10)\n",
    "for ses in ises: print(ses.sessionPrint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4c3f6fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10996, 200), (14021, 200), (13940, 200), (12310, 200), (11086, 200), (11902, 200), (13940, 200), (10254, 200), (14780, 200), (12839, 200)]\n"
     ]
    }
   ],
   "source": [
    "pcss = [analysis.placeCellSingleSession(ses) for ses in ises]\n",
    "spkmaps = [p.get_spkmap(average=True, trials=\"full\")[0] for p in pcss]\n",
    "print([s.shape for s in spkmaps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e26dd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_nan = np.any(np.stack([np.any(np.isnan(s), axis=0) for s in spkmaps], axis=0), axis=0)\n",
    "spkmaps = [s[:, ~idx_nan] for s in spkmaps]\n",
    "kernels = [np.cov(s.T) for s in spkmaps]\n",
    "def get_kfunc(kernel, rows):\n",
    "    kfunc = []\n",
    "    for r in range(rows):\n",
    "        kfunc.append(kernel[r][r:])\n",
    "    max_length = max([len(k) for k in kfunc])\n",
    "    for r in range(rows):\n",
    "        kfunc[r] = np.concatenate([kfunc[r], np.zeros(max_length - len(kfunc[r]))])\n",
    "    return np.stack(kfunc)\n",
    "kfuns = [get_kfunc(k, 100) for k in kernels]\n",
    "avg_kfuns = np.stack([np.mean(k, axis=0) for k in kfuns])\n",
    "avg_kfuns = avg_kfuns / np.max(avg_kfuns, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0ad45799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01009633 0.01075494]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fpath = Path(r\"C:\\Users\\Andrew\\Documents\\GitHub\\vrAnalysis\\figures\\plots_for_dataclub_241111\")\n",
    "orange = np.array([241, 80, 15]) / 255\n",
    "\n",
    "def lorentz(x, alpha, magnitude):\n",
    "    return magnitude * alpha / (x**2 + alpha**2)\n",
    "\n",
    "# Fit the data\n",
    "popt, pcov = curve_fit(lorentz, x, Cf[0] / max(Cf[0]), p0=[1.0, 1.0], bounds=(0, np.inf))\n",
    "print(popt)\n",
    "\n",
    "rlorentz = lorentz(x, popt[0], popt[1])\n",
    "\n",
    "xcm = x * 1000\n",
    "Lcm = L * 1000\n",
    "xcm_kf = np.linspace(0, max(xcm), avg_kfuns.shape[1])\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, 4, figsize=(18, 4), layout=\"constrained\")\n",
    "ax[0].imshow(Cf / np.max(Cf), extent=[0, Lcm, 0, Lcm], cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[1].imshow(sp.linalg.toeplitz(rlorentz), extent=[0, Lcm, 0, Lcm], cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[2].plot(xcm, Cf[0] / max(Cf[0]), c=orange, label=\"f-Covariance\")\n",
    "ax[2].plot(xcm, rlorentz, c='k', label=\"Lorentz\")\n",
    "ax[2].plot(xcm_kf, np.mean(avg_kfuns, axis=0), c='b', label=\"data\")\n",
    "ax[3].imshow(kernels[0] / np.max(kernels[0]), extent=[0, 100, 0, 100], cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "\n",
    "ax[2].set_xlim(0, 120)\n",
    "\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[0].set_title(\"f(x) Kernel\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[1].set_title(\"Lorentz Kernel\")\n",
    "ax[2].set_xlabel(\"Displacement\")\n",
    "ax[2].set_ylabel(\"Correlation\")\n",
    "ax[2].legend(loc=\"upper right\", fontsize=14)\n",
    "\n",
    "ax[3].set_xlabel(\"Position\")\n",
    "ax[3].set_ylabel(\"Position\")\n",
    "ax[3].set_title(\"Data Kernel\")\n",
    "plt.show()\n",
    "helpers.save_figure(fig, fpath / \"lorentz_comparison_withdata.png\")\n",
    "\n",
    "# # w, v = helpers.smart_pca(Ch)\n",
    "# wf, vf = helpers.smart_pca(Cf)\n",
    "# w = w / np.sum(w)\n",
    "# wf = wf / np.sum(wf)\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(7, 5), layout=\"constrained\")\n",
    "# ax.plot(range(1, len(w)+1), w, label=\"Gaussian Kernel\", linewidth=2, color=\"k\")\n",
    "# ax.plot(range(1, len(wf)+1), wf, label=\"Thresholded Kernel\", linewidth=2, color=orange)\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_xlim(0, 81)\n",
    "# ax.set_ylim(1e-10, 1)\n",
    "# ax.legend(loc=\"upper right\")\n",
    "# ax.text(17, 3e-4, \"<---leading values are linear\", color=orange)\n",
    "# plt.show()\n",
    "# helpers.save_figure(fig, fpath / \"GP_Model_1.png\")\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(7, 5), layout=\"constrained\")\n",
    "# ax.plot(range(1, len(w)+1), w, label=\"Gaussian Kernel\", linewidth=2, color=\"k\")\n",
    "# # ax.plot(range(1, len(wf)+1), wf, label=\"Thresholded Kernel\", linewidth=2, color=\"b\")\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_xlim(0, 81)\n",
    "# ax.set_ylim(1e-10, 1)\n",
    "# ax.legend(loc=\"upper right\")\n",
    "# # ax.text(17, 3e-4, \"<---leading values are linear\", color=\"b\")\n",
    "# helpers.save_figure(fig, fpath / \"GP_Model_0.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c81b788b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((801,), (801,), (801, 801))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape, wf.shape, Ch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c0110c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = helpers.smart_pca(Cf) #sp.linalg.toeplitz(rlorentz))\n",
    "\n",
    "plt.plot(w)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also get real place field data\n",
    "mouse_name = \"ATL027\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False)\n",
    "ises = 8\n",
    "pcss = analysis.placeCellSingleSession(pcm.pcss[ises].vrexp, keep_planes=[1, 2, 3, 4], autoload=False)\n",
    "split_params = dict(total_folds=2, train_folds=1)\n",
    "pcss.define_train_test_split(**split_params)\n",
    "pcss.load_data(new_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b796225",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, P, T = 3000, 200, 100\n",
    "\n",
    "xpos = np.linspace(0, P, P)\n",
    "\n",
    "method = \"relugp\"\n",
    "\n",
    "if method == \"rbf\":\n",
    "    pf_loc = np.linspace(0, P, N) # place field location\n",
    "    pf_width = 1.0 * np.random.rand(N) + 2.5 # place field width\n",
    "    pf_basis = np.exp(-(pf_loc[:, None] - xpos[None, :]) ** 2 / 2 / pf_width[:, None] ** 2) # shape of place field\n",
    "    \n",
    "elif method == \"relugp\":\n",
    "    def r(d, length_scale=1.0):\n",
    "        \"\"\"Covariance function r(x - x') for scalar distance d\"\"\"\n",
    "        return np.exp(-0.5 * (d**2) / length_scale**2)\n",
    "\n",
    "    def generate_cov(x, length_scale=1.0):\n",
    "        \"\"\"Generate covariance matrix K(x, x') for a given x\"\"\"\n",
    "        distances = np.abs(x - x[0])\n",
    "        first_row = r(distances, length_scale)\n",
    "        return sp.linalg.toeplitz(first_row)\n",
    "    \n",
    "    L = 200 / 1000\n",
    "    fs = 0.001\n",
    "    NP = int(L / fs)\n",
    "    sigma = 0.015\n",
    "    theta = 1.3\n",
    "    x = np.linspace(0, L, NP)  # 1D space from 0 to 10\n",
    "    K = generate_cov(x, length_scale=sigma)  # Generate covariance matrix\n",
    "    h = np.random.multivariate_normal(mean=np.zeros(len(x)), cov=K, size=N*10)  # Generate h(x) as a sample from GP[0, r(x - x')]\n",
    "    pf_basis = np.maximum(0, h - theta)\n",
    "    idx_with_pf = np.where(np.any(pf_basis > 0, axis=1))[0]\n",
    "    pf_basis = pf_basis[idx_with_pf]\n",
    "    pf_basis = pf_basis[np.random.permutation(pf_basis.shape[0])[:N]]\n",
    "    if pf_basis.shape[0] < N:\n",
    "        raise ValueError(\"Not enough place fields\")\n",
    "    \n",
    "    idx_sort = np.argsort(np.argmax(pf_basis, axis=1))\n",
    "    pf_basis = pf_basis[idx_sort]\n",
    "    pf_loc = np.argmax(pf_basis, axis=1)\n",
    "    pf_basis = pf_basis / np.max(pf_basis, axis=1)[:, None]\n",
    "    \n",
    "    print(np.sum(np.any(pf_basis > 0, axis=1)) / N)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Method not recognized\")\n",
    "\n",
    "# Generate some place field properties\n",
    "beta_val = 0.1\n",
    "prob_pf = np.random.beta(beta_val, beta_val, N)\n",
    "# prob_pf = np.random.rand(N) ** 5.0 # probability of expressing place field\n",
    "noise_value = 0.5\n",
    "\n",
    "# Generate place field data\n",
    "pf_trial = np.random.rand(N, T) < prob_pf[:, None] # place field expression per trial\n",
    "pf_activity = pf_basis[:, :, None] * pf_trial[:, None, :] # place field activity\n",
    "noise_activity = np.random.randn(N, P, T) * noise_value # noise activity\n",
    "data = pf_activity + noise_activity\n",
    "\n",
    "train_data = np.mean(data[:, :, :T//2], axis=2)\n",
    "test_data = np.mean(data[:, :, T//2:], axis=2)\n",
    "\n",
    "# Get real place field data\n",
    "envidx = 1\n",
    "train_spkmaps = pcss.get_spkmap(trials=\"train\", average=True)\n",
    "test_spkmaps = pcss.get_spkmap(trials=\"test\", average=True)\n",
    "idx_nan = np.any(np.stack([np.any(np.isnan(spkmap), axis=0) for spkmap in train_spkmaps+test_spkmaps]), axis=0)\n",
    "train_spkmaps = [spkmap[:, ~idx_nan] for spkmap in train_spkmaps]\n",
    "test_spkmaps = [spkmap[:, ~idx_nan] for spkmap in test_spkmaps]\n",
    "train_spkmap = train_spkmaps[envidx]\n",
    "test_spkmap = test_spkmaps[envidx]\n",
    "\n",
    "# Run cvPCA Analyses\n",
    "nc = 80\n",
    "cvpca = helpers.cvPCA(train_data.T, test_data.T, nc=nc)\n",
    "truev = helpers.cvPCA(pf_basis.T, pf_basis.T, nc=nc)\n",
    "\n",
    "# Run on real mouse data\n",
    "cvpca_mouse = helpers.cvPCA(train_spkmap.T, test_spkmap.T, nc=nc)\n",
    "\n",
    "cvpca_v = helpers.smart_pca(train_data, centered=True)[1][:, :nc]\n",
    "train_proj = cvpca_v.T @ (train_data - train_data.mean(axis=1, keepdims=True))\n",
    "test_proj = cvpca_v.T @ (test_data - test_data.mean(axis=1, keepdims=True))\n",
    "\n",
    "ineg = np.where(cvpca < 0)[0]\n",
    "if len(ineg) == 0:\n",
    "    ineg = [30]\n",
    "\n",
    "norm = lambda x: x / np.sum(x)\n",
    "\n",
    "xv = range(1, nc + 1)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6, 6), layout=\"constrained\")\n",
    "ax[0, 0].imshow(train_data, aspect=\"auto\", cmap=\"inferno\", interpolation=\"none\")\n",
    "ax[0, 0].set_title(\"Train Data\")\n",
    "ax[0, 1].imshow(test_data, aspect=\"auto\", cmap=\"inferno\", interpolation=\"none\")\n",
    "ax[0, 1].set_title(\"Test Data\")\n",
    "ax[1, 0].plot(xv, norm(cvpca), c=\"k\")\n",
    "ax[1, 0].plot(xv, norm(truev), c=\"r\")\n",
    "ax[1, 0].plot(xv, norm(cvpca_mouse), c=\"b\")\n",
    "ax[1, 0].set_xlabel(\"Component\")\n",
    "ax[1, 0].set_ylabel(\"C-V Variance\")\n",
    "# ax[1, 0].set_xscale(\"log\")\n",
    "ax[1, 0].set_yscale(\"log\")\n",
    "ax[1, 1].plot(train_proj[ineg[0]], \"k\", label=\"Train\")\n",
    "ax[1, 1].plot(test_proj[ineg[0]], \"b\", label=\"Test\")\n",
    "ax[1, 1].set_xlabel(\"Train Projection onto Component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "642a0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_name = \"ATL022\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False)\n",
    "ises = 7\n",
    "pcss = analysis.placeCellSingleSession(pcm.pcss[ises].vrexp, keep_planes=[1, 2, 3, 4], autoload=False)\n",
    "split_params = dict(total_folds=2, train_folds=1)\n",
    "pcss.define_train_test_split(**split_params)\n",
    "pcss.load_data(new_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spkmaps = pcss.get_spkmap(average=True, smooth=0.1, trials=\"train\")\n",
    "idx_trials = [np.argsort(ti) for ti in pcss.idxFullTrialEachEnv]\n",
    "spkmaps = [spkmap[:, itt] for spkmap, itt in zip(pcss.get_spkmap(average=False, smooth=0.1, trials=\"full\"), idx_trials)]\n",
    "spks = pcss.prepare_spks()\n",
    "\n",
    "idx_nan = np.any(\n",
    "    np.stack([np.any(np.isnan(t), axis=0) for t in train_spkmaps] + [np.any(np.isnan(t), axis=(0, 1)) for t in spkmaps]), axis=0\n",
    ")\n",
    "train_spkmaps = [t[:, ~idx_nan] for t in train_spkmaps]\n",
    "spkmaps = [t[:, :, ~idx_nan] for t in spkmaps]\n",
    "\n",
    "# Measure noise on test trials\n",
    "noise = [te - tr[:, None, :] for tr, te in zip(train_spkmaps, spkmaps)]\n",
    "print([t.shape for t in noise], [t.shape for t in spkmaps])\n",
    "\n",
    "# Flattened (bin by bin across the session)\n",
    "noise = [t.transpose((0, 1, 2)).reshape(t.shape[0], -1) for t in noise]\n",
    "print([t.shape for t in noise])\n",
    "\n",
    "noisecorr = [np.corrcoef(t.T) for t in noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29827a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = len(train_spkmaps)\n",
    "fig, ax = plt.subplots(1, num_envs, figsize=(5 * num_envs, 5), layout=\"constrained\")\n",
    "for i, tnc in enumerate(noisecorr):\n",
    "    ax[i].imshow(tnc, aspect=\"auto\", cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "611ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_name = \"ATL027\"\n",
    "track = tracking.tracker(mouse_name)\n",
    "pcm = analysis.placeCellMultiSession(track, autoload=False)\n",
    "ises = 12\n",
    "pcss = analysis.placeCellSingleSession(pcm.pcss[ises].vrexp, keep_planes=[1, 2, 3, 4], autoload=False)\n",
    "split_params = dict(total_folds=2, train_folds=1)\n",
    "pcss.define_train_test_split(**split_params)\n",
    "pcss.load_data(new_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eafcd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spkmaps = pcss.get_spkmap(average=True, smooth=0.1, trials=\"train\")\n",
    "test_spkmaps = pcss.get_spkmap(average=True, smooth=0.1, trials=\"test\")\n",
    "\n",
    "idx_nan = np.any(np.stack([np.any(np.isnan(t), axis=0) for t in train_spkmaps] + [np.any(np.isnan(t), axis=0) for t in test_spkmaps]), axis=0)\n",
    "train_spkmaps = [t[:, ~idx_nan] for t in train_spkmaps]\n",
    "test_spkmaps = [t[:, ~idx_nan] for t in test_spkmaps]\n",
    "\n",
    "train_cov = [np.cov(t.T) for t in train_spkmaps]\n",
    "test_cov = [np.cov(t.T) for t in test_spkmaps]\n",
    "cv_cov = [helpers.abcov(tr.T, te.T) for tr, te in zip(train_spkmaps, test_spkmaps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "023009d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_cvf, basis = helpers.get_fourier_basis(train_spkmaps[0].shape[1], Fs=pcss.distStep)\n",
    "num_components = basis.shape[0]\n",
    "\n",
    "s = [helpers.cvPCA(tr.T, te.T, nc=num_components) for tr, te in zip(train_spkmaps, test_spkmaps)]\n",
    "corr, cos_train, sin_train, cos_test, sin_test = helpers.named_transpose([helpers.cvFOURIER(tr, te, basis, covariance=True) for tr, te in zip(train_spkmaps, test_spkmaps)])\n",
    "corrsum = [np.mean(c, axis=0) for c in corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c4bf5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "envidx = 1\n",
    "\n",
    "num_neurons, num_bins = train_spkmaps[0].shape\n",
    "\n",
    "train_show = train_cov[envidx]\n",
    "test_show = test_cov[envidx]\n",
    "cv_show = cv_cov[envidx]\n",
    "\n",
    "# pad with numbins//2 zeros on each side\n",
    "shift_center = np.arange(num_bins)\n",
    "roll_center = num_bins // 2\n",
    "\n",
    "pad_matrix = np.full((num_bins, roll_center), np.nan)\n",
    "train_show_pad = np.hstack([pad_matrix, train_show, pad_matrix])\n",
    "test_show_pad = np.hstack([pad_matrix, test_show, pad_matrix])\n",
    "cv_show_pad = np.hstack([pad_matrix, cv_show, pad_matrix])\n",
    "\n",
    "# Roll each row to be centered on the peak\n",
    "train_show_roll = np.array([np.roll(row, roll_center - p) for row, p in zip(train_show_pad, shift_center)])\n",
    "test_show_roll = np.array([np.roll(row, roll_center-p) for row, p in zip(test_show_pad, shift_center)])\n",
    "cv_show_roll = np.array([np.roll(row, roll_center-p) for row, p in zip(cv_show_pad, shift_center)])\n",
    "\n",
    "freq, train_show_power = sp.signal.welch(train_show, axis=1, fs=1, scaling=\"density\")\n",
    "_, test_show_power = sp.signal.welch(test_show, axis=1, fs=1, scaling=\"density\")\n",
    "_, cv_show_power = sp.signal.welch(cv_show, axis=1, fs=1, scaling=\"density\")\n",
    "\n",
    "freq_ac, train_ac_power = sp.signal.welch(np.nanmean(train_show_roll, axis=0), fs=1, scaling=\"density\")\n",
    "_, test_ac_power = sp.signal.welch(np.nanmean(test_show_roll, axis=0), fs=1, scaling=\"density\")\n",
    "_, cv_ac_power = sp.signal.welch(np.nanmean(cv_show_roll, axis=0), fs=1, scaling=\"density\")\n",
    "\n",
    "middle_band = slice(100, num_bins*3-100)\n",
    "freq_ac_mid, train_ac_power_mid = sp.signal.welch(np.nanmean(train_show_roll, axis=0)[middle_band], fs=1, scaling=\"density\")\n",
    "_, test_ac_power_mid = sp.signal.welch(np.nanmean(test_show_roll, axis=0)[middle_band], fs=1, scaling=\"density\")\n",
    "_, cv_ac_power_mid = sp.signal.welch(np.nanmean(cv_show_roll, axis=0)[middle_band], fs=1, scaling=\"density\")\n",
    "\n",
    "\n",
    "f_xvals = np.arange(len(freq)) + 1\n",
    "\n",
    "cmap = mpl.colormaps[\"inferno\"]\n",
    "cmap.set_bad(color=[0.2, 0.2, 0.2])\n",
    "\n",
    "vmin = 0\n",
    "vmax = 0.1 #np.nanmax(train_show_roll)\n",
    "\n",
    "ymax = np.nanmax(np.nanmean(train_show_roll, axis=0)) * 1.1\n",
    "pmax = np.nanmax(np.nanmean(train_show_power, axis=0)) * 1.1\n",
    "\n",
    "norm = lambda x: x / np.sum(x)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot stuff\n",
    "xvals = np.arange(3*num_bins) - (3*num_bins)//2\n",
    "fig, ax = plt.subplots(4, 3, figsize=(14, 12), sharex=\"row\", sharey=\"row\", layout=\"constrained\")\n",
    "# autocorrelation maps\n",
    "ax[0, 0].imshow(train_show_pad, aspect='auto', cmap=cmap, vmin=0, vmax=vmax)\n",
    "ax[0, 1].imshow(test_show_pad, aspect='auto', cmap=cmap, vmin=0, vmax=vmax)\n",
    "ax[0, 2].imshow(cv_show_pad, aspect='auto', cmap=cmap, vmin=0, vmax=vmax)\n",
    "\n",
    "# average autocorrelation\n",
    "ax[1, 0].plot(np.nanmean(train_show_roll, axis=0), label=\"Train\")\n",
    "ax[1, 1].plot(np.nanmean(test_show_roll, axis=0), label=\"Test\")\n",
    "ax[1, 2].plot(np.nanmean(cv_show_roll, axis=0), label=\"CV\")\n",
    "ax[1, 0].set_ylim([0, ymax])\n",
    "\n",
    "# power - train/test/cv - \n",
    "# black: full, red: over average autocorrelation, blue: over middle band\n",
    "ax[2, 0].plot(freq, np.nanmean(train_show_power, axis=0), c='k', label=\"Train\")\n",
    "ax[2, 1].plot(freq, np.nanmean(test_show_power, axis=0), c='k', label=\"Test\")\n",
    "ax[2, 2].plot(freq, np.nanmean(cv_show_power, axis=0), c='k', label=\"CV\")\n",
    "ax[2, 0].plot(freq_ac, train_ac_power.T, c='r', label=\"Train AC\")\n",
    "ax[2, 1].plot(freq_ac, test_ac_power.T, c='r', label=\"Test AC\")\n",
    "ax[2, 2].plot(freq_ac, cv_ac_power.T, c='r', label=\"CV AC\")\n",
    "ax[2, 0].plot(freq_ac_mid, train_ac_power_mid.T, c='b', label=\"Train AC Mid\")\n",
    "ax[2, 1].plot(freq_ac_mid, test_ac_power_mid.T, c='b', label=\"Test AC Mid\")\n",
    "ax[2, 2].plot(freq_ac_mid, cv_ac_power_mid.T, c='b', label=\"CV AC Mid\")\n",
    "ax[2, 0].set_yscale('log')\n",
    "ax[2, 1].set_yscale('log')\n",
    "ax[2, 2].set_yscale('log')\n",
    "\n",
    "# cross-validated variance\n",
    "# left, green: cvpca\n",
    "# left, magenta: cv-fourier average\n",
    "# left, black: fourier power autocorr\n",
    "# middle, green: cv-fourier cosine\n",
    "# right, green: cv-fourier sine\n",
    "\n",
    "ax[3, 0].plot(freqs_cvf, norm(s[envidx]), c='g', label=\"Train\")\n",
    "ax[3, 1].plot(freqs_cvf, norm(corr[envidx][0]), c='g', label=\"Correlation - Cosine\")\n",
    "ax[3, 2].plot(freqs_cvf, norm(corr[envidx][1]), c='g', label=\"Correlation - Sine\")\n",
    "\n",
    "ax[3, 0].plot(freqs_cvf, norm(corrsum[envidx]), c='m', label=\"Correlation - SumFourier\")\n",
    "\n",
    "ax[3, 0].plot(freq, norm(np.nanmean(cv_show_power, axis=0)), c='k', label=\"CV\")\n",
    "ax[3, 1].plot(freq, norm(np.nanmean(cv_show_power, axis=0)), c='k', label=\"CV\")\n",
    "ax[3, 2].plot(freq, norm(np.nanmean(cv_show_power, axis=0)), c='k', label=\"CV\")\n",
    "ax[3, 0].set_xlabel(\"Frequency\")\n",
    "ax[3, 1].set_xlabel(\"Frequency\")\n",
    "ax[3, 2].set_xlabel(\"Frequency\")\n",
    "ax[3, 0].set_yscale('log')\n",
    "ax[3, 1].set_yscale('log')\n",
    "ax[3, 2].set_yscale('log')\n",
    "\n",
    "pmin = min([np.nanmin(np.nanmean(train_show_power, axis=0)), np.nanmin(np.nanmean(test_show_power, axis=0)), np.nanmin(np.nanmean(cv_show_power, axis=0))])\n",
    "\n",
    "ax[0, 0].set_title(\"Train\")\n",
    "ax[0, 1].set_title(\"Test\")\n",
    "ax[0, 2].set_title(\"CV\")\n",
    "ax[0, 0].set_ylabel(\"Position\")\n",
    "ax[1, 0].set_ylabel(\"AutoCorr\\n(average row of cov)\")\n",
    "ax[2, 0].set_ylabel(\"Power Spectrum\")\n",
    "\n",
    "ax[0, 0].set_xlabel(\"Position\")\n",
    "ax[0, 1].set_xlabel(\"Position\")\n",
    "ax[0, 2].set_xlabel(\"Position\")\n",
    "\n",
    "ax[1, 0].set_xlabel(\"Position\")\n",
    "ax[1, 1].set_xlabel(\"Position\")\n",
    "ax[1, 2].set_xlabel(\"Position\")\n",
    "\n",
    "ax[2, 0].set_xlabel(\"Frequency\")\n",
    "ax[2, 1].set_xlabel(\"Frequency\")\n",
    "ax[2, 2].set_xlabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "baf1353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = helpers.smart_pca(cv_show)\n",
    "_, wf = sp.signal.welch(cv_show[cv_show.shape[0]//2], fs=1, scaling=\"density\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), layout=\"constrained\")\n",
    "ax[0].plot(range(len(freqs_cvf)), norm(s[envidx]), c='k', label=\"cvPCA\")\n",
    "ax[0].plot(range(len(freqs_cvf)), norm(corrsum[envidx]), c='r', label=\"cv-Fourier\")\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_ylim([1e-5, 1])\n",
    "ax[0].set_xlabel(\"Dimensions\")\n",
    "ax[0].set_ylabel(\"Relative Variance\")\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "\n",
    "ax[1].plot(range(len(freqs_cvf)), norm(w[:len(freqs_cvf)]), c='k', label=\"cvPCA\")\n",
    "ax[1].plot(range(len(freqs_cvf)), norm(wf[:len(freqs_cvf)]), c='b', label=\"True\")\n",
    "ax[1].set_yscale('log')\n",
    "# ax[1].set_ylim([1e-5, 1])\n",
    "ax[1].set_xlabel(\"Dimensions\")\n",
    "ax[1].set_ylabel(\"Relative Variance\")\n",
    "ax[1].legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247fd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyses and work to do:\n",
    "\n",
    "# ROICaT Figure:\n",
    "# - add a \"print pair data\" button to the interactive viewer (and maybe even a \"save figure\" button?)\n",
    "# - build an example figure with the ROICaT data (can be simple, just make it soon)\n",
    "\n",
    "# LBM-s3d:\n",
    "# - get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bbdd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Management:\n",
    "# I need a way to report how many sessions the mouse has experienced each environment, independent of \n",
    "# which environments are represented in imaging sessions (which is how I'm doing it now...)\n",
    "\n",
    "# Required Updates: \n",
    "# need to update the placeCellMultiSession object to reflect changes to spkmap code\n",
    "# anything that uses pcss.get_place_field (pcmm make_snake_data and make_paired_snake)\n",
    "\n",
    "# Compare cvPCA analyses with eigenspectrum of spontaneous data unrelated to SVCA\n",
    "# And I want to start with the rastermap on projected place field data\n",
    "\n",
    "# Compare cvPCA to SVCA (do a hybrid: use cvPCA to get the spatial PCs, then apply those to the SVCA split)\n",
    "\n",
    "# Buzsaki Data:\n",
    "# https://crcns.org/data-sets/hc/hc-3 -- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4097350/\n",
    "# https://app.globus.org/file-manager?origin_id=188a6110-96db-11eb-b7a9-f57b2d55370d&origin_path=%2FVargaV%2F&two_pane=false - https://buzsakilab.com/wp/animals/?frm_search&project=67125&frm-page-14333=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471f554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Dataclub 240429: \n",
    "# -- need to consolidate all my figures (especially for the last few slides in a script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b225b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Meeting with Kenneth Plan:\n",
    "# - Relate to Kernel Matrices:\n",
    "#   - https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c09_p291-326.pdf\n",
    "#   - First, look at the kernel matrices (the position x position covariance matrices for each environment/session)\n",
    "#   - Study the structure, and how it changes over time. \n",
    "#   - Compare the cross-validated kernel and the non-cv kernel matrix and compare their changes over time\n",
    "#            - notes about ^^, this will tell us how much changes in eigenspectrum relate to reliability across trials vs the shape of the kernel matrix etc...\n",
    "# - Studies of non-place cells:\n",
    "#   - Look at the eigenspectrum from non-place cells, suppose as a function of the reliability...\n",
    "#   - Do cross-validated decoding from non-place cells\n",
    "# - Discussion of SVCA results\n",
    "#   - SVCA dimensionality could have issues with noise estimation... the same way the trial expanded cvPCA plots did...\n",
    "#   - Predict cell2 group from cell1 group, and predict cell2 group from their cross-validated place field, compare variance explained and overlap in variance explained\n",
    "# - Rastermap: \n",
    "#   - need to find a way to remove expected spatial activity from full spike trace data (then maybe do rastermap again?)\n",
    "# - Signal to Noise\n",
    "#   - For each ROI, measure activity in center of place field, outside of place field on a linear track, and outside the track (or in other environments)\n",
    "# - Measure spontaneous periods of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9078fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIMILIBI GOALS:\n",
    "# Compare best networks to ridge regression for a bunch of sessions.\n",
    "# Ridge Regression:\n",
    "#   - need to optimize ridge parameter: I can use a simple grid search on a log-space for this in two stages\n",
    "#   - Note: I tested (for one session) if the best ridge parameter is the same for full-rank and low-rank, and it was. \n",
    "#   - setup a train/val/test split program and then fit the model to each session, and record the results for several ranks\n",
    "# Networks: \n",
    "#   - for a subset of ranks, train a standard network and a beta-VAE network, record results for each session\n",
    "#   - to validate, just train a network on lots of epochs, store the evaluation test score throughout training, and \n",
    "#     save the full trajectory across training along with the best test score and the associated epoch number.\n",
    "#   - Note: for BetaVAELoss, will need to separate the loss into reconstruction and KL divergence, for proper saving.\n",
    "# Analysis / summary:\n",
    "#   - plot summary curves across mice for each rank, color-coded by RRR, BetaVAE, and SVCANet\n",
    "#   - probably also compare for each mouse somehow? Maybe categorized dot plots separated by rank? \n",
    "#   - I also want to compare how the validation scores improve over time for the network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('CR_Hippocannula6', '2022-08-26', '702') # test this because performance improved for 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a session randomly that has registered imaging data and a single environment\n",
    "vrexp = random.choice(sessiondb.iterSessions(imaging=True, vrRegistration=True, experimentID=1))\n",
    "print(vrexp.sessionPrint()) # show which session you chose\n",
    "\n",
    "keep_planes = [1, 2]\n",
    "onefile = \"mpci.roiActivityDeconvolvedOasis\"\n",
    "ospks = vrexp.loadone(onefile)\n",
    "keep_idx = vrexp.idxToPlanes(keep_planes=keep_planes)\n",
    "ospks = ospks[:, keep_idx]\n",
    "time_split_prms = dict(\n",
    "    num_groups=3,\n",
    "    relative_size=[5, 5, 1], #[5, 5, 1],\n",
    "    chunks_per_group=-3, # 25\n",
    "    num_buffer=3, # usually use default (which is 10)\n",
    ")\n",
    "npop = Population(ospks.T, generate_splits=True, time_split_prms=time_split_prms)\n",
    "print(npop.size())\n",
    "\n",
    "pcss = analysis.placeCellSingleSession(vrexp, keep_planes=keep_planes, onefile=onefile, autoload=True)\n",
    "assert len(pcss.environments) == 1, \"Only one environment is supported for this analysis\"\n",
    "\n",
    "train_source, train_target = npop.get_split_data(0, center=False, scale=True, pre_split=False, scale_type=\"preserve\")\n",
    "test_source, test_target = npop.get_split_data(1, center=False, scale=True, pre_split=False, scale_type=\"preserve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf5382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a comparison of cvPCA and SVCA\n",
    "svca = SVCA(centered=True).fit(train_source, train_target)\n",
    "\n",
    "# Get place fields\n",
    "envnum = pcss.environments[0]\n",
    "train_spkmap = pcss.get_spkmap(envnum=envnum, average=True, trials=\"train\")[0]\n",
    "source_spkmap = train_spkmap[npop.cell_split_indices[0]]\n",
    "target_spkmap = train_spkmap[npop.cell_split_indices[1]]\n",
    "\n",
    "test_spkmap = pcss.get_spkmap(envnum=envnum, average=True, trials=\"test\")[0]\n",
    "source_spkmap_test = test_spkmap[npop.cell_split_indices[0]]\n",
    "target_spkmap_test = test_spkmap[npop.cell_split_indices[1]]\n",
    "\n",
    "idx_nan = np.any(np.isnan(source_spkmap), axis=0) | np.any(np.isnan(target_spkmap), axis=0)\n",
    "source_pca = PCA().fit(source_spkmap[:, ~idx_nan])\n",
    "target_pca = PCA().fit(target_spkmap[:, ~idx_nan])\n",
    "source_components = source_pca.get_components()\n",
    "target_components = target_pca.get_components()\n",
    "\n",
    "# Compare the PCA map of train to test trials on the source data\n",
    "idx_nan = np.any(np.isnan(source_spkmap_test), axis=0)\n",
    "source_pca_test = PCA().fit(source_spkmap_test[:, ~idx_nan])\n",
    "traintest_map = np.dot(source_components.T, source_pca_test.get_components())\n",
    "\n",
    "# For U, V, and components, each column is a component (so each row is a neuron)\n",
    "source_map = np.dot(source_components.T, svca.U)\n",
    "target_map = np.dot(target_components.T, svca.V)\n",
    "\n",
    "vmin = min(source_map.min(), target_map.min(), traintest_map.min())\n",
    "vmax = max(source_map.max(), target_map.max(), traintest_map.max())\n",
    "\n",
    "# Take weighted average across axis 0 (for each SV, which PF PCs are it composed of?)\n",
    "idx = np.arange(source_map.shape[0]).reshape(-1, 1)\n",
    "source_map_avg = np.sum(np.abs(source_map) * idx, axis=0) / np.sum(np.abs(source_map), axis=0)\n",
    "target_map_avg = np.sum(np.abs(target_map) * idx, axis=0) / np.sum(np.abs(target_map), axis=0)\n",
    "pc_map_avg = np.sum(np.abs(traintest_map) * idx, axis=0) / np.sum(np.abs(traintest_map), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbae25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get SV activity projections, \n",
    "u_activity = fs.zscore(np.array(svca.U.T @ npop.data[npop.cell_split_indices[0]]), axis=1)\n",
    "v_activity = fs.zscore(np.array(svca.V.T @ npop.data[npop.cell_split_indices[1]]), axis=1)\n",
    "u_rawspkmap = helpers.getBehaviorAndSpikeMaps(vrexp, onefile=u_activity.T)[3]\n",
    "v_rawspkmap = helpers.getBehaviorAndSpikeMaps(vrexp, onefile=v_activity.T)[3]\n",
    "uspkmap_train = pcss.get_spkmap(envnum=envnum, average=True, trials=\"train\", rawspkmap=u_rawspkmap)[0]\n",
    "vspkmap_train = pcss.get_spkmap(envnum=envnum, average=True, trials=\"train\", rawspkmap=v_rawspkmap)[0]\n",
    "uspkmap_test = pcss.get_spkmap(envnum=envnum, average=True, trials=\"test\", rawspkmap=u_rawspkmap)[0]\n",
    "vspkmap_test = pcss.get_spkmap(envnum=envnum, average=True, trials=\"test\", rawspkmap=v_rawspkmap)[0]\n",
    "\n",
    "def select_env(tup, idx):\n",
    "    return list(map(lambda x: x[idx], tup))\n",
    "\n",
    "urelmse, urelcor = select_env(pcss.get_reliability_values(envnum=envnum, rawspkmap=u_rawspkmap), 0)\n",
    "vrelmse, vrelcor = select_env(pcss.get_reliability_values(envnum=envnum, rawspkmap=v_rawspkmap), 0)\n",
    "relmse, relcor = select_env(pcss.get_reliability_values(envnum=envnum), 0)\n",
    "\n",
    "u_rel_idx = urelcor > 0.6\n",
    "v_rel_idx = vrelcor > 0.6\n",
    "rel_idx = relcor > 0.6\n",
    "\n",
    "uidx = pcss.get_place_field(uspkmap_train[u_rel_idx], method=\"max\")[1]\n",
    "vidx = pcss.get_place_field(vspkmap_train[v_rel_idx], method=\"max\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5103f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.dimilibi.helpers import make_position_basis, filter_timepoints\n",
    "frame_position, frame_environment, environments = vrexp.get_frame_behavior(speedThreshold=1)\n",
    "valid_u_activity, valid_position, valid_environment = filter_timepoints(u_activity.T, frame_position, frame_environment)\n",
    "position_basis = make_position_basis(valid_position, valid_environment, num_basis=10)\n",
    "\n",
    "upospop = Population(valid_u_activity.T, time_split_prms={\"num_groups\": 2, \"relative_size\": [5, 1], \"chunks_per_group\": -3, \"num_buffer\": 3}, dtype=torch.float32)\n",
    "train_valid_u = upospop.apply_split(valid_u_activity.T, 0)\n",
    "test_valid_u = upospop.apply_split(valid_u_activity.T, 1)\n",
    "train_pos_basis = upospop.apply_split(position_basis.T, 0)\n",
    "test_pos_basis = upospop.apply_split(position_basis.T, 1)\n",
    "\n",
    "print(train_valid_u.shape, train_pos_basis.shape, test_valid_u.shape, test_pos_basis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = RidgeRegression(alpha=1e3, fit_intercept=True).fit(train_valid_u.T, train_pos_basis.T)\n",
    "print(rmodel.score(test_valid_u.T, test_pos_basis.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded97af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "tspkmapidx = pcss.get_place_field(train_spkmap[rel_idx], method=\"max\")[1]\n",
    "print(vmin, vmax)\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 6), layout=\"constrained\")\n",
    "ax[0, 0].imshow(uspkmap_train[u_rel_idx][uidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[0, 1].imshow(vspkmap_train[v_rel_idx][vidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[0, 2].imshow(train_spkmap[rel_idx][tspkmapidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[1, 0].imshow(uspkmap_test[u_rel_idx][uidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[1, 1].imshow(vspkmap_test[v_rel_idx][vidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[1, 2].imshow(test_spkmap[rel_idx][tspkmapidx], aspect=\"auto\", cmap=\"bwr\", vmin=vmin, vmax=vmax, interpolation=\"none\")\n",
    "ax[0, 3].ecdf(urelmse[~np.isnan(urelmse)], label=\"U\")\n",
    "ax[0, 3].ecdf(vrelmse[~np.isnan(vrelmse)], label=\"V\")\n",
    "ax[0, 3].ecdf(relmse[~np.isnan(relmse)], label=\"PCA\")\n",
    "ax[1, 3].ecdf(urelcor[~np.isnan(urelcor)], label=\"U\")\n",
    "ax[1, 3].ecdf(vrelcor[~np.isnan(vrelcor)], label=\"V\")\n",
    "ax[1, 3].ecdf(relcor[~np.isnan(relcor)], label=\"PCA\")\n",
    "ax[0, 3].set_xlabel(\"Reliability (method 1)\")\n",
    "ax[1, 3].set_xlabel(\"Reliability (method 2)\")\n",
    "ax[0, 3].set_ylabel(\"Cumulative probability\")\n",
    "ax[1, 3].set_ylabel(\"Cumulative probability\")\n",
    "ax[0, 3].legend()\n",
    "ax[1, 3].legend()\n",
    "ax[0, 3].set_xlim(-2, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f97ee395",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_components = source_components.shape[1]\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3), layout=\"constrained\")\n",
    "ax[0].imshow(np.abs(source_map[:, :max_components]), aspect=\"auto\", interpolation=\"none\", cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[1].imshow(np.abs(target_map[:, :max_components]), aspect=\"auto\", interpolation=\"none\", cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[2].imshow(np.abs(traintest_map[:, :max_components]), aspect=\"auto\", interpolation=\"none\", cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "ax[3].scatter(np.abs(source_map[:, :max_components].flatten()), np.abs(target_map[:, :max_components].flatten()), s=1)\n",
    "\n",
    "# Link axes 0 and 1\n",
    "ax[1].sharex(ax[0])\n",
    "ax[1].sharey(ax[0])\n",
    "ax[2].sharex(ax[0])\n",
    "ax[2].sharey(ax[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b4d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64077205",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = False\n",
    "scale = True\n",
    "pre_split = False\n",
    "scale_type = \"preserve\"\n",
    "\n",
    "train_source, train_target = npop.get_split_data(0, center=center, scale=scale, pre_split=pre_split, scale_type=scale_type)\n",
    "val_source, val_target = npop.get_split_data(1, center=center, scale=scale, pre_split=pre_split, scale_type=scale_type)\n",
    "test_source, test_target = npop.get_split_data(2, center=center, scale=scale, pre_split=pre_split, scale_type=scale_type)\n",
    "\n",
    "get_whitening = False\n",
    "\n",
    "if get_whitening:\n",
    "    zca_source = PCA().fit(train_source).get_zca().to(device)\n",
    "    zca_val = PCA().fit(val_source).get_zca().to(device)\n",
    "    zca_target = PCA().fit(train_target).get_zca().to(device)\n",
    "\n",
    "print(train_source.shape, train_target.shape, val_source.shape, val_target.shape, test_source.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc3aa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get eigenvalues of the full population to compare with simulated data appropriately\n",
    "# npop_evals = PCA().fit(ospks.T).get_eigenvalues()\n",
    "\n",
    "N = npop.size(0) // 2\n",
    "T = 8000\n",
    "Ttest = 1000\n",
    "Q = torch.linalg.qr(torch.normal(0, 1, (2*N, 2*N)))[0]\n",
    "D = npop_evals[:2*N]\n",
    "\n",
    "train_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, T))\n",
    "val_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, Ttest))\n",
    "train_data = Q @ train_scores\n",
    "val_data = Q @ val_scores\n",
    "\n",
    "train_source = train_data[:N]\n",
    "train_target = train_data[N:]\n",
    "val_source = val_data[:N]\n",
    "val_target = val_data[N:]\n",
    "\n",
    "# zscore the data\n",
    "train_source = (train_source - train_source.mean(1, keepdim=True)) / train_source.std(1, keepdim=True)\n",
    "train_target = (train_target - train_target.mean(1, keepdim=True)) / train_target.std(1, keepdim=True)\n",
    "val_source = (val_source - val_source.mean(1, keepdim=True)) / val_source.std(1, keepdim=True)\n",
    "val_target = (val_target - val_target.mean(1, keepdim=True)) / val_target.std(1, keepdim=True)\n",
    "\n",
    "# zca_source = PCA().fit(train_source).get_zca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0f5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rrr = ReducedRankRegression(alpha=1e5, fit_intercept=True).fit(train_source.T.to('cpu'), train_target.T.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 5\n",
    "print(rrr.score(train_source.T.to('cpu'), train_target.T.to('cpu'), rank=rank))\n",
    "print(rrr.score(train_source.T.to('cpu'), train_target.T.to('cpu')))\n",
    "print(rrr.score(val_source.T, val_target.T, rank=rank, nonnegative=False))\n",
    "print(rrr.score(val_source.T, val_target.T, rank=rank, nonnegative=True))\n",
    "print(rrr.score(val_source.T, val_target.T, nonnegative=False))\n",
    "print(rrr.score(val_source.T, val_target.T, nonnegative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb25cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = train_source.size(0)\n",
    "num_hidden = [400] # [hyps[\"best_params\"][\"num_hidden\"]]\n",
    "num_latent = 5\n",
    "num_target_neurons = train_target.size(0)\n",
    "num_timepoints = train_source.size(1)\n",
    "\n",
    "# net0 = SVCANet(\n",
    "#     num_neurons,\n",
    "#     num_hidden,\n",
    "#     num_latent,\n",
    "#     num_target_neurons,\n",
    "#     activation = torch.nn.ReLU(),\n",
    "#     nonnegative = False, \n",
    "# ).to(device)\n",
    "\n",
    "# net1 = BetaVAE(\n",
    "#     num_neurons,\n",
    "#     num_hidden,\n",
    "#     num_latent,\n",
    "#     num_target_neurons,\n",
    "#     activation = torch.nn.ReLU(),\n",
    "#     nonnegative = False,\n",
    "# ).to(device)\n",
    "\n",
    "# net2 = HurdleNet(\n",
    "#     num_neurons,\n",
    "#     num_hidden,\n",
    "#     num_latent,\n",
    "#     num_target_neurons,\n",
    "#     activation = torch.nn.ReLU(),\n",
    "#     nonnegative = False,\n",
    "#     transparent_relu=True,\n",
    "# ).to(device)\n",
    "\n",
    "nets = [\n",
    "    constructor(\n",
    "        num_neurons,\n",
    "        num_hidden,\n",
    "        num_latent,\n",
    "        num_target_neurons,\n",
    "        activation = torch.nn.ReLU(),\n",
    "        nonnegative = True,\n",
    "        transparent_relu = True,\n",
    "    ).to(device)\n",
    "    for constructor in [SVCANet, SVCANet, SVCANet, SVCANet, HurdleNet, HurdleNet, HurdleNet, HurdleNet]\n",
    "]\n",
    "\n",
    "cols = 'kkkkrrrr'\n",
    "\n",
    "# nets = [net0, net1]\n",
    "betavae = [False for _ in range(len(nets))] #[False, True]\n",
    "\n",
    "# nets = [net0, net1]\n",
    "loss_functions = [torch.nn.MSELoss(reduction='sum') for _ in range(len(nets))]\n",
    "\n",
    "# beta = [1e1, 1e2, 1e3, 1e4] #[1 for _ in range(len(nets))]\n",
    "regularizers = [EmptyRegularizer() for _ in range(len(nets))]\n",
    "# regularizers = [BetaVAE_KLDiv(beta=b, reduction='sum') for b in beta]\n",
    "\n",
    "wd = 1e3 #hyps[\"best_params\"][\"weight_decay\"]\n",
    "lr = 1e-3 #hyps[\"best_params\"][\"lr\"]\n",
    "nl = 0 # hyps[\"best_params\"][\"noise_level\"]\n",
    "weight_decay = [wd for _ in range(len(nets))] \n",
    "opts = [torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd) for net, wd in zip(nets, weight_decay)]\n",
    "\n",
    "net_reg_weight = [0 for _ in range(len(nets))]\n",
    "noise_level = [nl for _ in range(len(nets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "batch_size = num_timepoints//10\n",
    "num_epochs = 800\n",
    "num_nets = len(nets)\n",
    "\n",
    "train_loss = torch.zeros((num_nets, num_epochs))\n",
    "train_reg = torch.zeros((num_nets, num_epochs))\n",
    "train_score = torch.zeros((num_nets, num_epochs))\n",
    "traintest_loss = torch.zeros((num_nets, num_epochs))\n",
    "traintest_score = torch.zeros((num_nets, num_epochs))\n",
    "\n",
    "train_source = train_source.to(device)\n",
    "train_target = train_target.to(device)\n",
    "test_source = test_source.to(device)\n",
    "test_target = test_target.to(device)\n",
    "\n",
    "for net in nets:\n",
    "    net.train()\n",
    "    \n",
    "progress = tqdm(range(num_epochs), desc='Training Networks')\n",
    "for epoch in progress:\n",
    "            \n",
    "    itime = torch.randperm(num_timepoints)[:batch_size]\n",
    "    \n",
    "    source_batch = train_source[:, itime].T\n",
    "    target_batch = train_target[:, itime].T\n",
    "\n",
    "    for opt in opts:\n",
    "        opt.zero_grad()\n",
    "\n",
    "    predictions = [net(source_batch + nl * torch.randn_like(source_batch)) for net, nl in zip(nets, noise_level)]\n",
    "    mulogvar = [pred[1:] if b else (torch.tensor(0, device=device), torch.tensor(0, device=device)) for pred, b in zip(predictions, betavae)]\n",
    "    predictions = [pred[0] if b else pred for pred, b in zip(predictions, betavae)]\n",
    "    losses = [loss_fn(pred, target_batch) for pred, loss_fn in zip(predictions, loss_functions)]\n",
    "    regs = []\n",
    "    for b, mlv, reg, pred in zip(betavae, mulogvar, regularizers, predictions):\n",
    "        if b:\n",
    "            regs.append(reg(*mlv))\n",
    "        else:\n",
    "            regs.append(reg(source_batch, pred))\n",
    "    full_losses = [loss + weight * reg for loss, reg, weight in zip(losses, regs, net_reg_weight)]\n",
    "    for loss in full_losses:\n",
    "        loss.backward()\n",
    "    \n",
    "    for opt in opts:\n",
    "        opt.step()\n",
    "    \n",
    "    scores = [net.score(source_batch, target_batch) for net in nets]\n",
    "\n",
    "    for inet in range(len(nets)):\n",
    "        train_loss[inet, epoch] = losses[inet].item()\n",
    "        train_reg[inet, epoch] = regs[inet].item()\n",
    "        train_score[inet, epoch] = scores[inet].item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for net in nets:\n",
    "                net.eval()\n",
    "            pred = nets[inet](test_source.T)\n",
    "            if betavae[inet]:\n",
    "                pred = pred[0]\n",
    "            traintest_loss[inet, epoch] = loss_functions[inet](pred, test_target.T).item()\n",
    "            traintest_score[inet, epoch] = nets[inet].score(test_source.T, test_target.T).item()\n",
    "            for net in nets:\n",
    "                net.train()\n",
    "        \n",
    "    progress.set_postfix({'Loss': losses[0].item(), 'Score': scores[0].item(), \"Reg\": regs[0].item()})\n",
    "\n",
    "for net in nets:\n",
    "    net.eval()\n",
    "    \n",
    "test_predictions = [net(test_source.T) for net in nets]\n",
    "test_mulogvar = [pred[1:] if b else (torch.tensor(0, device=device), torch.tensor(0, device=device)) for pred, b in zip(test_predictions, betavae)]\n",
    "test_predictions = [pred[0] if b else pred for pred, b in zip(test_predictions, betavae)]\n",
    "test_losses = [loss_fn(test_prediction, test_target.T) for test_prediction, loss_fn in zip(test_predictions, loss_functions)]\n",
    "test_regs = []\n",
    "for b, mlv, reg, pred in zip(betavae, test_mulogvar, regularizers, test_predictions):\n",
    "    if b:\n",
    "        test_regs.append(reg(*mlv))\n",
    "    else:\n",
    "        test_regs.append(reg(test_source.T, pred))\n",
    "test_scores = [net.score(test_source.T, test_target.T) for net in nets]\n",
    "\n",
    "for inet in range(len(nets)):\n",
    "    print(f\"Net{inet} Test Loss: {test_losses[inet].item():.3f}, \" +\n",
    "          f\"Test Score: {test_scores[inet].item():.3f}, \" + \n",
    "          f\"Test Reg: {test_regs[inet].item():.3f}\" + \n",
    "          f\"Maximum Test Score: {traintest_score[inet].max().item():.3f}\")\n",
    "\n",
    "# plot the training loss\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3), layout=\"constrained\")\n",
    "for inet in range(len(nets)):\n",
    "    ax[0].plot(train_loss[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[0].axhline(test_losses[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[1].plot(train_score[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[1].axhline(test_scores[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].set_title('Training Score')\n",
    "# ax[1].set_ylim(-5, 1.0)\n",
    "ax[1].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[2].plot(train_reg[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[2].axhline(test_regs[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Regularization')\n",
    "ax[2].set_title('Training Regularization')\n",
    "ax[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79492e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9, 3), layout=\"constrained\")\n",
    "for inet in range(len(nets)):\n",
    "    ax[0].plot(traintest_loss[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[0].axhline(test_losses[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[1].plot(traintest_score[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[1].axhline(test_scores[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].set_title('Training Score')\n",
    "# ax[1].set_ylim(-5.1, 0.5) #torch.min(traintest_score[:, 2:]), 1.2*torch.max(traintest_score))\n",
    "ax[1].legend()\n",
    "for inet in range(len(nets)):\n",
    "    ax[2].plot(train_reg[inet], c=cols[inet], label=f\"net{inet}\")\n",
    "    ax[2].axhline(test_regs[inet].item(), linestyle='--', c=cols[inet])\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Regularization')\n",
    "ax[2].set_title('Training Regularization')\n",
    "ax[2].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svca = SVCA().fit(train_source, train_target)\n",
    "shared, total = svca.score(train_source, train_target)\n",
    "print(f\"{shared.sum() / total.sum() * 100:.2f}% of the variance is shared between the two groups.\")\n",
    "\n",
    "shared, total = svca.score(test_source, test_target)\n",
    "print(f\"{shared.sum() / total.sum() * 100:.2f}% of the variance is shared between the two groups.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5200fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea\n",
    "# How to go from one dataset to another? \n",
    "# One way: learn the SVD of the covariance\n",
    "\n",
    "# Question: \n",
    "# How does the SVD of gram between A and B related to a reduced rank solution of predicting B from A? \n",
    "\n",
    "# U S V.T = A.T @ B\n",
    "# AX = B --> X = (A.T @ A)^-1 @ A.T @ B\n",
    "# AX = B --> X = (A.T @ A)^-1 @ U S V.T\n",
    "# AX = B --> X = Q 1/D Q.T @ U S V.T\n",
    "\n",
    "# In general, if we have a SVD map of the gram matrix defining covariance between A and B, \n",
    "# then we can study how each mode of A/B maps onto each PC of A/B. \n",
    "\n",
    "# Simple:\n",
    "# 1. Learn the SVD of the gram matrix between A and B\n",
    "# 2. Learn the PCA of A and B\n",
    "# 3. Get the OLS solution to transform SVD modes to PCA modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b13e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# choose a session randomly that has registered imaging data\n",
    "vrexp = random.choice(sessiondb.iterSessions(imaging=True, vrRegistration=True))\n",
    "print(vrexp.sessionPrint()) # show which session you chose\n",
    "\n",
    "ospks = vrexp.loadone('mpci.roiActivityDeconvolvedOasis')\n",
    "keep_idx = vrexp.idxToPlanes(keep_planes=[1])\n",
    "ospks = ospks[:, keep_idx]\n",
    "\n",
    "time_split_prms = dict(\n",
    "    relative_size=[10, 1],\n",
    "    chunks_per_group=25,\n",
    "    num_buffer=10,\n",
    ")\n",
    "npop = Population(ospks.T, generate_splits=True, time_split_prms=time_split_prms)\n",
    "print(npop.size())\n",
    "\n",
    "# get eigenvalues of the full population to compare with simulated data appropriately\n",
    "npop_evals = PCA().fit(ospks.T).get_eigenvalues()\n",
    "\n",
    "train_source, train_target = npop.get_split_data(0, center=True)\n",
    "test_source, test_target = npop.get_split_data(1, center=True)\n",
    "\n",
    "print(train_source.shape, train_target.shape, test_source.shape, test_target.shape)\n",
    "\n",
    "data_cross = CrossCompare().fit(train_source, train_target)\n",
    "\n",
    "\n",
    "N = npop.size(0)//2\n",
    "T = 5000\n",
    "Ttest = 1000\n",
    "Q = torch.linalg.qr(torch.normal(0, 1, (2*N, 2*N)))[0]\n",
    "D = npop_evals[:2*N]\n",
    "\n",
    "train_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, T))\n",
    "test_scores = torch.diag(D) @ torch.normal(0, 1, (2*N, Ttest))\n",
    "train_data = Q @ train_scores\n",
    "test_data = Q @ test_scores\n",
    "\n",
    "train_source = train_data[:N]\n",
    "train_target = train_data[N:]\n",
    "test_source = test_data[:N]\n",
    "test_target = test_data[N:]\n",
    "\n",
    "sim_cross = CrossCompare().fit(train_source, train_target)\n",
    "\n",
    "to_pca = True\n",
    "d_source_com, d_target_com, d_source_entropy, d_target_entropy = data_cross.analyze(to_pca=to_pca)\n",
    "source_com, target_com, source_entropy, target_entropy = sim_cross.analyze(to_pca=to_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b07e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(6, 6), layout=\"constrained\")\n",
    "\n",
    "d_crossmap = data_cross.u_to_pc if to_pca else data_cross.pc_to_u\n",
    "crossmap = sim_cross.u_to_pc if to_pca else sim_cross.pc_to_u\n",
    "\n",
    "ax[0, 0].imshow(torch.abs(d_crossmap), aspect='auto', cmap='pink', interpolation=\"None\")\n",
    "ax[0, 0].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[0, 0].set_ylabel(\"U Mode\" if to_pca else \"Principal Component\")\n",
    "\n",
    "ax[0, 1].imshow(torch.abs(crossmap), aspect='auto', cmap='pink', interpolation=\"None\")\n",
    "ax[0, 1].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[0, 1].set_ylabel(\"U Mode\" if to_pca else \"Principal Component\")\n",
    "\n",
    "ax[1, 0].axline((0, 0), slope=1, color='k', linewidth=0.5, linestyle='--')\n",
    "ax[1, 0].plot(source_com, color='k', label=\"Simulated\")\n",
    "ax[1, 0].plot(target_com, color='k')\n",
    "ax[1, 0].plot(d_source_com, color='b', label=\"Data\")\n",
    "ax[1, 0].plot(d_target_com, color='b')\n",
    "ax[1, 0].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[1, 0].set_ylabel('Center of Mass ' + (\"SVD Mode\" if to_pca else \"Principal Component\"))\n",
    "ax[1, 0].set_title('Map Dimension CoM')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "ax[1, 1].plot(source_entropy, color='k', label=\"Source\")\n",
    "ax[1, 1].plot(target_entropy, color='k')\n",
    "ax[1, 1].plot(d_source_entropy, color='b', label=\"Data\")\n",
    "ax[1, 1].plot(d_target_entropy, color='b')\n",
    "ax[1, 1].set_xlabel(\"Principal Component\" if to_pca else \"SVD Mode\")\n",
    "ax[1, 1].set_ylabel('Entropy')\n",
    "ax[1, 1].set_title('Entropy')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3c4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sourcemap = sp.ndimage.gaussian_filter(torch.abs(data_cross.u_to_pc.T), 1)\n",
    "targetmap = sp.ndimage.gaussian_filter(torch.abs(data_cross.v_to_pc.T), 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), layout=\"constrained\", sharex=True, sharey=True)\n",
    "ax[0].imshow(sourcemap, cmap=\"hot\", interpolation='gaussian')\n",
    "ax[0].set_xlabel('U Mode')\n",
    "ax[0].set_ylabel('PC Mode')\n",
    "ax[0].set_title('U to PC Source')\n",
    "ax[1].imshow(targetmap, cmap=\"hot\")\n",
    "ax[1].set_title('V to PC Target')\n",
    "ax[1].set_xlabel('V Mode')\n",
    "ax[1].set_ylabel('PC Mode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119caff-5d69-484e-bb16-cc6b820c7d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Database Requirements: \n",
    "# ---------------------\n",
    "# GUI: db manager\n",
    "# - click on entry and do things:\n",
    "#                --> open file explorer to that session\n",
    "#                --> do suite2p\n",
    "#                --> do red cell management\n",
    "# - update table data? \n",
    "# ---------------------\n",
    "# Operational Commands: \n",
    "# - Automatically do suite2p \n",
    "# - Check if registration was done before a suite2p update\n",
    "\n",
    "# Further Requirements:\n",
    "# ---------------------\n",
    "# ROICaT Alignment Tools \n",
    "# Track Red Cell Consistency across days \n",
    "# Now that I've refactored the database code, need to update some things in documentation and probably elsewhere too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f846af3-d4a6-47e3-962b-680643416cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes from meeting with Kenneth:\n",
    "\n",
    "# - skewness (violin plot) of Control & Red -- \n",
    "#     - all - \n",
    "#     - just reliable -- for each session - \n",
    "\n",
    "# Subsample control data for scatter plot\n",
    "# Fisher z transformation (but label by original correlation...)\n",
    "# Question:\n",
    "# -- if reliable on 1 day, is it reliable on other days? \n",
    "# -- make a matrix with source and target, color by fraction of reliable on target out of those reliable on source\n",
    "# -- also do this with your session kernels for control and red\n",
    "# -- also do this for different reliability cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan for attack:\n",
    "# Make a suite of summary figures on a session by session basis and a multisession basis. \n",
    "# I just want to be able to look through a mouse's data and evaluate the behavior, the imaging data, and how well the tracking did.\n",
    "\n",
    "# Inclusions:\n",
    "# 1. Behavioral data (running speed and number of trials across any environments it was in -- also metadata about day in environment...)\n",
    "# 2. Imaging data (example snakes from all environments, both train/test comparisons and remapping comparisons)\n",
    "# 3. Red cell data (number of red cells per plane -- and some examples of red cells?)\n",
    "# 4. Tracking data (number of tracked cells per combination (full matrix!), number of tracked red cells, number of tracked reliable cells per environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROICaT Analysis\n",
    "# 1. Example data figures (from roicat_stats)\n",
    "#    - show two post-alignment FOVs, highlight a few tracked neurons and a few (nearby) un-tracked neurons with colors\n",
    "#    - below that, show the place field tuning in each session, color-coded the same way as the neurons ROI plot\n",
    "\n",
    "# 2. Analysis of ROICaT agreement with functional data\n",
    "#    - scatter plot of sConj & place field correlation (with \"labels\" pairs colored differently)\n",
    "#    - mouse by mouse, session by session mean lines comparing average place field correlation of same pairs with different pairs\n",
    "#        -- (imagining lines from 0->1 for mouse 1 of each sessions mean same/diff pfCorr, then also in 2->3 for mouse 2, and 4->5 for mouse 3, etc.)\n",
    "#        -- can also have a supplemental plot showing distribution of same/diff pfCorr across each session pair? \n",
    "\n",
    "# 3. Control analysis with null model test (empirical version with subsampled null distribution)\n",
    "\n",
    "# 4. Control analysis with bayesian model\n",
    "# -- get pfCorr_withinSession (this is pairs of ROIs within a session, and should be representative of pfCorr_diff_acrossSession)\n",
    "# -- normalize pfCorr_all_acrossSession by number of pairs, subtract density of pfCorr_withinSession\n",
    "# -- remaining pfCorr_remain_acrossSession = pfCorr_same_acrossSession\n",
    "\n",
    "\n",
    "# ---- note ----\n",
    "# - should probably include target reliable pairs not represented in the source only reliable category..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74b665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2144063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ROICaT within session matching\n",
    "import pickle\n",
    "\n",
    "mouseName = 'ATL012'\n",
    "sessionDate = '2023-02-28'\n",
    "sessionID = '701'\n",
    "\n",
    "ses = session.vrExperiment(mouseName, sessionDate, sessionID)\n",
    "print(ses.sessionPrint())\n",
    "\n",
    "data = ses.loadone('mpci.roiActivityDeconvolvedOasis')\n",
    "\n",
    "filepath = ses.sessionPath() / f\"{mouseName}.within_session.ROICaT.tracking.results.pkl\"\n",
    "with open(filepath, 'rb') as f:\n",
    "    roicat = pickle.load(f)\n",
    "\n",
    "num_clusters = np.max(roicat['clusters']['labels'])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = analysis.sameCellCandidates(ses, keepPlanes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explorer = analysis.clusterExplorerROICaT(scc, roicat['clusters']['labels'], keepPlanes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_number = np.zeros(num_clusters)\n",
    "cc_within = np.zeros(num_clusters)\n",
    "for ic in range(num_clusters):\n",
    "    cidx = roicat['clusters']['labels']==ic\n",
    "    cdata = data[:, cidx]\n",
    "    ccorr = np.corrcoef(cdata.T)\n",
    "    idx_offdiag = np.triu(np.ones_like(ccorr, dtype=bool), k=1)\n",
    "    cvals = ccorr[idx_offdiag]\n",
    "    cc_within[ic] = np.mean(cvals)\n",
    "    cc_number[ic] = np.sum(cidx)\n",
    "\n",
    "plt.close('all')\n",
    "plt.scatter(cc_number, cc_within)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_clus = np.random.randint(0, num_clusters)\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(range(data.shape[0]), data[:, roicat['clusters']['labels']==i_clus])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(roicat['clusters']['labels']>-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucids in list of lists for requested sessions\n",
    "ucids = [[[] for _ in range(num_ses)] for _ in range(num_planes)]\n",
    "for planeidx, results in enumerate([self.results[p] for p in keepPlanes]):\n",
    "    for sesidx, idx in enumerate(idx_ses):\n",
    "        ucids[planeidx][sesidx] = results['clusters']['labels_bySession'][idx]\n",
    "\n",
    "# this is the number of unique IDs per plane\n",
    "num_ucids = [max([np.max(u) for u in ucid])+1 for ucid in ucids]\n",
    "\n",
    "# this is a boolean array of size (number unique IDs x num sessions) where there is a 1 if a unique ROI is found in each session\n",
    "roicat_index = [np.zeros((nucids, num_ses), dtype=bool) for nucids in num_ucids]\n",
    "for planeidx, ucid in enumerate(ucids):\n",
    "    for sesidx, uc in enumerate(ucid):\n",
    "        cindex = uc[uc >= 0] # index of ROIs found in this session\n",
    "        roicat_index[planeidx][cindex, sesidx] = True # label found ROI with True\n",
    "\n",
    "return ucids, roicat_index\n",
    "\n",
    "    # get ucids and 1s index for requested sessions\n",
    "    ucids, roicat_index = self.prepare_tracking_idx(idx_ses=idx_ses, keepPlanes=keepPlanes)\n",
    "    \n",
    "    # list of UCIDs in all requested sessions (a list of the UCIDs...)\n",
    "    idx_in_ses = [np.where(np.all(rindex, axis=1))[0] for rindex in roicat_index]\n",
    "    \n",
    "    # For each plane & session, a sorted index to the suite2p ROI to recreate the list of UCIDs\n",
    "    idx_to_ucid = [[helpers.index_in_target(iis, uc)[1] for uc in ucid] for (iis, ucid) in zip(idx_in_ses, ucids)]\n",
    "    \n",
    "    # cumulative number of ROIs before eacg plane (in numeric order of planes using sorted(self.plane_names))\n",
    "    roi_per_plane = self.roi_per_plane[keepPlanes][:, idx_ses]\n",
    "    roi_plane_offset = np.cumsum(np.vstack((np.zeros((1,num_ses),dtype=int), roi_per_plane[:-1])), axis=0)\n",
    "\n",
    "    # A straightforward numpy array of (numSessions, numROIs) containing the indices to retrieve tracked and sorted ROIs\n",
    "    return np.concatenate([np.stack([offset+ucid for offset, ucid in zip(offsets, ucids)], axis=1) for offsets, ucids in zip(roi_plane_offset, idx_to_ucid)], axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: remove multipage tiff from: C:\\Users\\Andrew\\Documents\\localData\\ATL012\\2023-02-09\\701\\suite2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a664d70-de8c-44fd-a1c4-4d4bf6388542",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_red = track.check_red_cell_consistency(idx_ses=idx_ses, keepPlanes=None, use_s2p=True)\n",
    "idx_has_red = idx_red[:, np.any(idx_red, axis=0)]\n",
    "idx_sort = np.argsort(-np.sum(idx_has_red,axis=0))\n",
    "idx_plot = idx_has_red[:, idx_sort]\n",
    "plt.close('all')\n",
    "plt.imshow(idx_plot, aspect='auto', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efb65e-f390-403d-b8c8-89f816683ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_red = track.check_red_cell_consistency(idx_ses=idx_ses, keepPlanes=None)\n",
    "idx_has_red = idx_red[:, np.sum(idx_red, axis=0)>0]\n",
    "idx_sort = np.argsort(-np.sum(idx_has_red,axis=0))\n",
    "idx_plot = idx_has_red[:, idx_sort]\n",
    "plt.close('all')\n",
    "plt.imshow(idx_plot, aspect='auto', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c15844-230b-4428-940c-3ba768fa60c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c29ad-d229-468b-994d-60daa446d163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e92fd-45f5-4d4c-bf2b-cc36e0fecfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9c31e-793c-475d-b9f6-1131dcd8678b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd1a9c-0bc0-4da3-b608-d7d9342894d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf37659-8093-414a-83ab-38613aee581b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105147fb-abb0-4e3a-b342-7e0a49278a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e7090-cccc-4b6b-934c-ae6305fe1f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbe85a-f153-4621-a834-f69f3fb03b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5934ad-507d-47b7-af57-c99acd431200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcb8db-9aeb-425e-87b0-e890fe107f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ce9b1-d5aa-46f7-98ec-9eff1bd68bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a6b79-666d-43b8-a6f9-2694815c8de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
