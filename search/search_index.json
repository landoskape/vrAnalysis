{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"vrAnalysis2 Documentation","text":"<p>Welcome to the documentation for vrAnalysis2, a comprehensive Python package for processing and analyzing virtual reality (VR) behavioral and imaging experiments.</p>"},{"location":"#overview","title":"Overview","text":"<p>vrAnalysis2 is designed to work with: - Behavioral data from vrControl experiments - Imaging data processed with suite2p - Database management using Microsoft Access databases (or other SQL databases with minimal modifications) - Session tracking following the Alyx directory structure</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Database Management: Track and manage VR session data with an easy-to-use database interface</li> <li>Session Registration: Automated preprocessing of behavioral and imaging data</li> <li>Data Processing: Generate spike maps, occupancy maps, and other spatial representations</li> <li>Cell Tracking: Track cells across sessions for longitudinal analysis</li> <li>Multi-session Analysis: Analyze data across groups of sessions</li> <li>Quality Control: Built-in tools for session quality control and annotation</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Installation Guide - Get started with vrAnalysis2</li> <li>Quickstart Tutorial - Learn the basics</li> <li>Module Documentation - Detailed module descriptions</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#package-structure","title":"Package Structure","text":"<p>vrAnalysis2 is organized into several main modules:</p> <ul> <li><code>database</code>: Database management and session tracking</li> <li><code>sessions</code>: Session data loading and management</li> <li><code>registration</code>: Data preprocessing and registration workflows</li> <li><code>processors</code>: Data processing pipelines (e.g., spike maps)</li> <li><code>analysis</code>: Analysis tools and utilities</li> <li><code>tracking</code>: Cell tracking across sessions</li> <li><code>multisession</code>: Multi-session analysis capabilities</li> <li><code>helpers</code>: Utility functions and helpers</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you encounter issues or have questions:</p> <ol> <li>Check the Quickstart Guide for common workflows</li> <li>Review the Module Documentation for specific features</li> <li>Consult the API Reference for detailed function signatures</li> </ol>"},{"location":"GITHUB_PAGES_SETUP/","title":"GitHub Pages Setup Guide","text":"<p>This guide will help you set up GitHub Pages for the vrAnalysis2 documentation.</p>"},{"location":"GITHUB_PAGES_SETUP/#step-1-enable-github-pages-in-repository-settings","title":"Step 1: Enable GitHub Pages in Repository Settings","text":"<p>Note: You can do this step before or after pushing the workflow. The <code>gh-pages</code> branch will be automatically created by the GitHub Actions workflow on its first run.</p> <ol> <li>Go to your repository on GitHub: <code>https://github.com/landoskape/vrAnalysis</code></li> <li>Click on Settings (in the repository navigation bar)</li> <li>Scroll down to Pages in the left sidebar</li> <li>Under Source, select:</li> <li>Source: <code>Deploy from a branch</code></li> <li>Branch: <code>gh-pages</code> (this branch will be created automatically by the workflow)</li> <li>Folder: <code>/ (root)</code></li> <li>Click Save</li> </ol>"},{"location":"GITHUB_PAGES_SETUP/#step-2-verify-github-actions-permissions","title":"Step 2: Verify GitHub Actions Permissions","text":"<p>The GitHub Actions workflow needs write permissions to deploy:</p> <ol> <li>In the same Settings page, go to Actions \u2192 General</li> <li>Scroll to Workflow permissions</li> <li>Select Read and write permissions</li> <li>Check Allow GitHub Actions to create and approve pull requests (optional but recommended)</li> <li>Click Save</li> </ol>"},{"location":"GITHUB_PAGES_SETUP/#step-3-push-the-workflow","title":"Step 3: Push the Workflow","text":"<p>The workflow file is already created at <code>.github/workflows/docs.yml</code>. Simply commit and push it:</p> <pre><code>git add .github/workflows/docs.yml mkdocs.yml\ngit commit -m \"Add GitHub Actions workflow for documentation deployment\"\ngit push origin main\n</code></pre>"},{"location":"GITHUB_PAGES_SETUP/#step-4-monitor-the-deployment","title":"Step 4: Monitor the Deployment","text":"<ol> <li>Go to the Actions tab in your repository</li> <li>You should see a workflow run called \"Deploy Documentation\"</li> <li>Click on it to see the build progress</li> <li>Important: The workflow will automatically create the <code>gh-pages</code> branch on its first run</li> <li>Once it completes successfully:</li> <li>The <code>gh-pages</code> branch will be created (if it doesn't exist)</li> <li>Your documentation will be available at: <code>https://landoskape.github.io/vrAnalysis/</code></li> </ol>"},{"location":"GITHUB_PAGES_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"GITHUB_PAGES_SETUP/#workflow-fails","title":"Workflow Fails","text":"<p>If the workflow fails: 1. Check the Actions tab for error messages 2. Common issues:    - Missing dependencies (should be handled by the workflow)    - Python version issues (workflow uses Python 3.11)    - Build errors (check mkdocs build locally first)</p>"},{"location":"GITHUB_PAGES_SETUP/#documentation-not-updating","title":"Documentation Not Updating","text":"<ul> <li>Make sure you pushed to the <code>main</code> branch</li> <li>The workflow only runs on pushes to <code>main</code> (or manual triggers)</li> <li>Wait a few minutes for GitHub Pages to update (can take 5-10 minutes)</li> </ul>"},{"location":"GITHUB_PAGES_SETUP/#custom-domain","title":"Custom Domain","text":"<p>If you want to use a custom domain: 1. Add a <code>CNAME</code> file to the <code>docs/</code> directory with your domain 2. Update the workflow file to set <code>cname: true</code> 3. Configure DNS settings as per GitHub Pages instructions</p>"},{"location":"GITHUB_PAGES_SETUP/#manual-deployment","title":"Manual Deployment","text":"<p>You can also manually trigger the workflow: 1. Go to Actions tab 2. Select \"Deploy Documentation\" workflow 3. Click Run workflow 4. Select branch and click Run workflow</p>"},{"location":"GITHUB_PAGES_SETUP/#local-testing","title":"Local Testing","text":"<p>Before pushing, test the build locally:</p> <pre><code># Install dependencies\npip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Build the site\nmkdocs build\n\n# Serve locally to preview\nmkdocs serve\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to vrAnalysis2!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and clone the repository</li> <li>Create a conda environment:</li> </ol> <pre><code>conda env create -f environment.yml\nconda activate vrAnalysis\n</code></pre> <ol> <li>Install in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints where possible</li> <li>Write docstrings in NumPy format</li> <li>Keep line length to 150 characters (per <code>pyproject.toml</code>)</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Add docstrings to all public functions and classes</li> <li>Use NumPy-style docstrings</li> <li>Update relevant documentation pages when adding features</li> <li>Include examples in docstrings where helpful</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure existing tests pass</li> <li>Test with real data when possible</li> </ul>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ul> <li>Create a new branch for your changes</li> <li>Write clear commit messages</li> <li>Update documentation as needed</li> <li>Ensure all tests pass before submitting</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, please open an issue on GitHub.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide will help you install vrAnalysis2 and its dependencies.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher (note tested, might be even higher because of typing)</li> <li>Conda or Mamba (recommended for managing dependencies, use Mamba for faster installation!)</li> <li>Git (for cloning the repository)</li> </ul>"},{"location":"installation/#installation_1","title":"Installation","text":"<p>This method allows you to edit the code and is recommended if you plan to contribute or modify the package.</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/landoskape/vrAnalysis\ncd vrAnalysis\n</code></pre> <ol> <li>Create a conda environment from the provided <code>environment.yml</code>:</li> </ol> <pre><code>conda env create -f environment.yml\n# Or use mamba for faster installation:\nmamba env create -f environment.yml\n</code></pre> <ol> <li>Activate the environment:</li> </ol> <pre><code>conda activate vrAnalysis  # or whatever name is specified in environment.yml\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>After installation, you have to configure paths for your data:</p> <ol> <li>Data Directory: Set your local data path in <code>vrAnalysis2/files.py</code>:</li> </ol> <pre><code>def local_data_path() -&gt; Path:\n    return Path(\"C:/path/to/your/data\")\n</code></pre> <ol> <li>Database Paths: Configure database paths in <code>vrAnalysis2/database.py</code> using the <code>get_database_metadata()</code> function.</li> </ol>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that vrAnalysis2 is installed correctly:</p> <pre><code>import vrAnalysis2\nfrom vrAnalysis2.sessions import B2Session, create_b2session\nfrom vrAnalysis2.database import get_database_metadata\n\nprint(\"vrAnalysis2 installed successfully!\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Database Connection Errors: Ensure you have the appropriate database drivers installed (e.g., Microsoft Access ODBC drivers for <code>.accdb</code> files).</p> </li> <li> <p>Import Errors: Make sure you've activated the correct conda environment and installed all dependencies.</p> </li> <li> <p>Path Issues: Verify that your data paths are correctly configured and accessible.</p> </li> </ol>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation, check out the Quickstart Guide to begin using vrAnalysis2.</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#architecture","title":"Architecture","text":"<p>vrAnalysis2 is organized around a few core concepts:</p>"},{"location":"overview/#sessions","title":"Sessions","text":"<p>A session represents a single experimental run. Each session contains: - Behavioral data (position, velocity, rewards, etc.) - Imaging data (calcium traces, ROIs, etc.) - Metadata (mouse name, date, session ID, etc.)</p> <p>Sessions are represented by the <code>B2Session</code> class, which provides methods to load and access data.</p>"},{"location":"overview/#database","title":"Database","text":"<p>The database tracks all sessions and their metadata. It uses Microsoft Access (<code>.accdb</code>) files by default, but can be adapted to other SQL databases.</p> <p>The <code>SessionDatabase</code> class provides methods to: - Query sessions based on criteria - Add new sessions - Update session metadata - Track registration status and quality control flags</p>"},{"location":"overview/#registration","title":"Registration","text":"<p>Registration is the process of preprocessing raw experimental data. This includes: - Loading behavioral data from Timeline files - Processing imaging data from suite2p outputs - Running OASIS deconvolution - Processing red cell annotations - Aligning data in time</p> <p>Registration creates standardized data structures that can be used for analysis.</p>"},{"location":"overview/#processors","title":"Processors","text":"<p>Processors transform session data into analysis-ready formats. For example: - <code>SpikeMapProcessor</code>: Creates spatial maps of neural activity - Other processors generate various representations of the data</p>"},{"location":"overview/#tracking","title":"Tracking","text":"<p>Tracking identifies the same cells across multiple sessions. This enables longitudinal analysis of how individual cells change over time.</p>"},{"location":"overview/#data-flow","title":"Data Flow","text":"<pre><code>Raw Data (Timeline, suite2p)\n    \u2193\nRegistration (B2Registration)\n    \u2193\nSession Data (B2Session)\n    \u2193\nProcessing (Processors)\n    \u2193\nAnalysis (Analysis Tools)\n</code></pre>"},{"location":"overview/#directory-structure","title":"Directory Structure","text":"<p>vrAnalysis2 expects data organized in an Alyx-style structure:</p> <pre><code>localData/\n\u251c\u2500\u2500 mouse001/\n\u2502   \u251c\u2500\u2500 2024-01-15/\n\u2502   \u2502   \u251c\u2500\u2500 001/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 suite2p/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 timeline/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 002/\n\u2502   \u2514\u2500\u2500 2024-01-16/\n\u2514\u2500\u2500 mouse002/\n</code></pre>"},{"location":"overview/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Session-Centric: All operations revolve around session objects</li> <li>Database-Driven: Session metadata is tracked in a database</li> <li>Lazy Loading: Data is loaded on-demand to minimize memory usage</li> <li>Caching: Intermediate results are cached to speed up repeated operations</li> <li>Modularity: Each component can be used independently</li> </ol>"},{"location":"overview/#common-workflows","title":"Common Workflows","text":""},{"location":"overview/#1-daily-registration-workflow","title":"1. Daily Registration Workflow","text":"<ol> <li>Add new sessions to database using GUI</li> <li>Run registration for new sessions</li> <li>Review and QC registered sessions</li> <li>Update database with QC status</li> </ol>"},{"location":"overview/#2-analysis-workflow","title":"2. Analysis Workflow","text":"<ol> <li>Query database for sessions of interest</li> <li>Load sessions into <code>B2Session</code> objects</li> <li>Process data using processors</li> <li>Perform analysis</li> <li>Visualize and save results</li> </ol>"},{"location":"overview/#3-longitudinal-analysis-workflow","title":"3. Longitudinal Analysis Workflow","text":"<ol> <li>Query database for sessions from same mouse</li> <li>Load sessions and track cells across sessions</li> <li>Analyze changes in tracked cells</li> <li>Compare across experimental conditions</li> </ol>"},{"location":"overview/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>vrAnalysis2 integrates with: - suite2p: For calcium imaging data processing - vrControl: For behavioral data collection - Timeline (Rigbox): For experimental event tracking - ROICaT: For ROI classification (via <code>roicat_support</code>) - OASIS: For calcium trace deconvolution</p>"},{"location":"overview/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Quickstart Guide for hands-on examples</li> <li>Explore Module Documentation for detailed information</li> <li>Check the API Reference for complete function signatures</li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will walk you through the basic workflows in vrAnalysis2.</p>"},{"location":"quickstart/#creating-a-session","title":"Creating a Session","text":"<p>The core object in vrAnalysis2 is the <code>B2Session</code>, which represents a single experimental session.</p>"},{"location":"quickstart/#basic-session-creation","title":"Basic Session Creation","text":"<pre><code>from vrAnalysis2.sessions import create_b2session\n\n# Create a session with default parameters\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n</code></pre>"},{"location":"quickstart/#custom-session-parameters","title":"Custom Session Parameters","text":"<p>You can customize how data is loaded using <code>B2SessionParams</code>:</p> <pre><code>from vrAnalysis2.sessions import create_b2session\n\n# Create a session with custom parameters\nparams = {\n    \"spks_type\": \"significant\",  # Use significant transients\n    \"keep_planes\": [0, 1, 2],    # Only load specific planes\n    \"good_labels\": [\"c\", \"d\"],   # Keep only \"c\" and \"d\" classified ROIs\n    \"exclude_silent_rois\": True   # Exclude ROIs with no activity\n}\n\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\",\n    params=params\n)\n</code></pre>"},{"location":"quickstart/#working-with-the-database","title":"Working with the Database","text":""},{"location":"quickstart/#querying-sessions","title":"Querying Sessions","text":"<pre><code>from vrAnalysis2.database import get_database\n\n# Get database metadata\ndb = get_database(\"vrSessions\")\n\n# Query sessions\nsessions = db.get_sessions(\n    conditions={\"mouseName\": \"mouse001\"},\n    sessionQC=True  # Only get QC'd sessions\n)\n\n# Access session data\nfor session_row in sessions:\n    print(f\"Session: {session_row['sessionDate']} - {session_row['sessionID']}\")\n</code></pre>"},{"location":"quickstart/#adding-sessions-to-database","title":"Adding Sessions to Database","text":"<p>Use the GUI to add new sessions:</p> <pre><code>from vrAnalysis2.uilib.add_entry_gui import add_entry_gui\n\n# Open the database entry GUI\nadd_entry_gui(\"vrSessions\")\n</code></pre>"},{"location":"quickstart/#registration-workflow","title":"Registration Workflow","text":"<p>Registration is the process of preprocessing and aligning behavioral and imaging data.</p> <pre><code>from vrAnalysis2.registration import B2Registration\nfrom vrAnalysis2.sessions.b2session import B2RegistrationOpts\n\n# Create registration options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,  # Run OASIS deconvolution\n    redCellProcessing=True,\n    neuropilCoefficient=0.7,\n    tau=1.5,\n    fs=6\n)\n\n# Create registration object\nregistration = B2Registration(\n    mouse_name=\"mouse001\",\n    date_string=\"2024-01-15\",\n    session_id=\"001\",\n    opts=opts\n)\n\n# Run registration\nregistration.register()\n</code></pre>"},{"location":"quickstart/#processing-spike-maps","title":"Processing Spike Maps","text":"<p>Generate spatial representations of neural activity:</p> <pre><code>from vrAnalysis2.processors.spkmaps import SpkmapProcessor\n\n# Create processor\nprocessor = SpkmapProcessor(session)\n\n# Generate maps\nmaps = processor.process(\n    bin_size=5.0,  # 5 cm bins\n    by_environment=True,  # Separate by environment\n    rois_first=True\n)\n\n# Access maps\noccupancy_map = maps.occmap\nspike_map = maps.spkmap\nspeed_map = maps.speedmap\n</code></pre>"},{"location":"quickstart/#tracking-cells-across-sessions","title":"Tracking Cells Across Sessions","text":"<p>Track the same cells across multiple sessions:</p> <pre><code>from vrAnalysis2.tracking import TrackedPair\n\n# Load two sessions\nsession1 = create_b2session(\"mouse001\", \"2024-01-15\", \"001\")\nsession2 = create_b2session(\"mouse001\", \"2024-01-16\", \"001\")\n\n# Track cells\ntracked = TrackedPair(session1, session2)\nmatched_pairs = tracked.get_matched_pairs()\n\n# Access matched ROIs\nfor roi1_idx, roi2_idx in matched_pairs:\n    print(f\"ROI {roi1_idx} in session 1 matches ROI {roi2_idx} in session 2\")\n</code></pre>"},{"location":"quickstart/#multi-session-analysis","title":"Multi-Session Analysis","text":"<p>Analyze data across multiple sessions:</p> <pre><code>from vrAnalysis2.multisession import MultiSession\nfrom vrAnalysis2.database import get_database_metadata, SessionDatabase\n\n# Get database\ndb_meta = get_database_metadata(\"vrSessions\")\ndb = SessionDatabase(**db_meta)\n\n# Get sessions for a mouse\nsessions_data = db.get_sessions(conditions={\"mouseName\": \"mouse001\"})\n\n# Create multi-session object\nmulti = MultiSession(sessions_data)\n\n# Perform analysis across sessions\n# (specific analysis methods depend on your needs)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Database Management</li> <li>Explore Session Configuration</li> <li>Understand Registration Workflows</li> <li>Check out Analysis Tools</li> </ul>"},{"location":"api/analysis/","title":"Analysis API Reference","text":"<p>The analysis functionality is distributed across several modules:</p> <ul> <li><code>vrAnalysis2.syd</code>: Analysis tools for place cells, reliability, and plasticity</li> <li><code>vrAnalysis2.analysis.same_cell_candidates</code>: Tools for identifying same cell candidates</li> <li><code>vrAnalysis2.analysis.tracked_plasticity</code>: Analysis of tracked cell plasticity</li> </ul> <p>Note: The <code>vrAnalysis2.analysis</code> package is not directly importable as a module. Refer to individual submodules for specific functionality.</p>"},{"location":"api/database/","title":"Database API Reference","text":""},{"location":"api/database/#vrAnalysis2.database","title":"<code>database</code>","text":"<p>Database management module for VR analysis sessions.</p> <p>This module provides classes and functions for interacting with Microsoft Access databases used to track VR session data. It includes base database functionality and specialized session database management with support for registration workflows, suite2p processing tracking, and quality control operations.</p>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase","title":"<code>BaseDatabase</code>","text":"Source code in <code>vrAnalysis2/database.py</code> <pre><code>class BaseDatabase:\n    def __init__(self, db_name: str):\n        \"\"\"\n        Initialize a new database instance.\n\n        This constructor initializes a new instance of the BaseDatabase class. It sets the default\n        values for the table name, database name, and database path. It is built to work with\n        the Microsoft Access application; however, a few small changes can make it compatible with\n        other SQL-based database systems.\n\n        Parameters\n        ----------\n        db_name : str, required\n            The name of the database to access.\n\n        Example\n        -------\n        &gt;&gt;&gt; db = BaseDatabase('vrSessions')\n        &gt;&gt;&gt; print(vrdb.table_name)\n        'sessiondb'\n        &gt;&gt;&gt; print(vrdb.db_name)\n        'vrDatabase'\n\n        Notes\n        -----\n        - This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.\n        - If you are using this on a new system, then you should edit your path, database name, and default table in that function.\n        \"\"\"\n\n        metadata = get_database_metadata(db_name)\n        self.db_path = metadata[\"db_path\"]\n        self.db_name = metadata[\"db_name\"]\n        self.db_ext = metadata[\"db_ext\"]\n        self.table_name = metadata[\"table_name\"]\n        self.uid = metadata[\"uid\"]\n        self.backup_path = metadata[\"backup_path\"]\n        self.host_type = host_types[self.db_ext]\n        self.unique_fields = self.process_unique_fields(metadata[\"unique_fields\"])\n        self.default_conditions = metadata[\"default_conditions\"]\n\n    def process_unique_fields(self, fields: List[Union[str, Tuple[str, type]]]) -&gt; List[Tuple[str, type]]:\n        \"\"\"\n        Process and validate unique field definitions.\n\n        Converts unique field specifications into a standardized format where each\n        field is a tuple of (field_name, field_type). String fields can be specified\n        as just the name and will default to str type.\n\n        Parameters\n        ----------\n        fields : list\n            List of field specifications. Each can be:\n            - A string (field name, defaults to str type)\n            - A tuple of (field_name, type) where type is a Python type class\n\n        Returns\n        -------\n        list\n            List of tuples, each containing (field_name, field_type).\n\n        Raises\n        ------\n        ValueError\n            If a field specification is not a string or a valid (string, type) tuple.\n        \"\"\"\n        ufields = []\n        for f in fields:\n            if isinstance(f, tuple) and len(f) == 2 and type(f[1]) == type and isinstance(f[0], str):\n                ufields.append(f)\n            elif isinstance(f, str):\n                ufields.append((f, str))\n            else:\n                raise ValueError(f\"unique field {f} must be a string or a string-type tuple\")\n        return ufields\n\n    def get_dbfile(self) -&gt; Path:\n        \"\"\"\n        Get the full path to the database file.\n\n        Returns\n        -------\n        Path\n            Path object pointing to the database file.\n        \"\"\"\n        return Path(self.db_path) / (self.db_name + self.db_ext)\n\n    def save_backup(self, return_out: bool = False) -&gt; Optional[CompletedProcess]:\n        \"\"\"Save a backup of the database to the backup path specified in metadata.\n\n        Parameters\n        ----------\n        return_out : bool, optional\n            If True, return the output of the robocopy command.\n\n        Returns\n        -------\n        CompletedProcess or None\n            CompletedProcess object from the robocopy command (if return_out is True).\n            Returns None if return_out is False.\n        \"\"\"\n        source_path = self.db_path\n        target_path = self.backup_path\n        source_file = self.db_name + self.db_ext\n        robocopy_arguments = f\"robocopy {source_path} {target_path} {source_file}\"\n        outs = run(robocopy_arguments, capture_output=True, text=True)\n        if return_out:\n            return outs\n\n    def connect(self) -&gt; pyodbc.Connection:\n        \"\"\"\n        Establish a connection to the database.\n\n        Creates a pyodbc connection using the appropriate driver string based on\n        the database file extension. Currently configured for Microsoft Access\n        databases (.accdb, .mdb files).\n\n        Returns\n        -------\n        pyodbc.Connection\n            Database connection object.\n\n        Raises\n        ------\n        AssertionError\n            If the host type for the database extension is not supported.\n\n        Notes\n        -----\n        To support additional database types:\n        1. Determine the appropriate pyodbc driver string for your database\n        2. Add it to the driver_string dictionary in this method\n        3. Update the host_types dictionary at the module level to map your\n           file extension to the driver key\n\n        See https://www.connectionstrings.com/ for driver string examples.\n        \"\"\"\n        driver_string = {\"access\": r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\" + rf\"DBQ={self.get_dbfile()};\"}\n\n        # Make sure connections are possible for this hosttype\n        failure_message = (\n            f\"Requested host_type ({self.host_type}) is not available. The only ones that are coded are: {[k for k in driver_string.keys()]}\\n\\n\"\n            f\"For support with writing a driver string for a different host, use the fantastic website: https://www.connectionstrings.com/\"\n        )\n        assert self.host_type in driver_string, failure_message\n\n        # Return a connection to the database\n        return pyodbc.connect(driver_string[self.host_type])\n\n    @contextmanager\n    def open_cursor(self, commit_changes: bool = False) -&gt; Generator[pyodbc.Cursor, None, None]:\n        \"\"\"\n        Context manager to open a database cursor and manage connections.\n\n        This context manager provides a convenient way to open a cursor to the database,\n        perform database operations, and manage connections. It also allows you to\n        commit changes if needed.\n\n        Parameters\n        ----------\n        commit_changes : bool, optional\n            Whether to commit changes to the database. Default is False.\n\n        Yields\n        ------\n        pyodbc.Cursor\n            A database cursor for executing SQL queries.\n\n        Raises\n        ------\n        Exception\n            If an error occurs while connecting to the database.\n\n        Example\n        -------\n        Use the context manager to perform database operations:\n\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     cursor.execute(\"SELECT * FROM your_table\")\n\n        \"\"\"\n        try:\n            # Attempt to open a cursor to the database\n            conn = self.connect()\n            cursor = conn.cursor()\n            yield cursor\n        except Exception as ex:\n            print(f\"An exception occurred while trying to connect to {self.db_name}!\")\n            print(ex)\n            raise ex\n        else:\n            # if no exception was raised, commit changes\n            if commit_changes:\n                conn.commit()\n        finally:\n            # Always close the cursor and connection\n            cursor.close()\n            conn.close()\n\n    # == display meta data for database ==\n    def show_metadata(self) -&gt; None:\n        \"\"\"\n        Display metadata associated with the open database.\n\n        Prints information about the database location, name, table, unique ID field,\n        backup path, and default filtering conditions.\n        \"\"\"\n        print(f\"{self.host_type} database located at {self.db_path}\")\n        print(f\"Database name: {self.db_name}{self.db_ext}, table name: {self.table_name}, with uid: {self.uid}\")\n        if self.backup_path is not None:\n            print(f\"Backup path located at: {self.backup_path}\")\n        else:\n            print(f\"No backup path specified...\")\n        if self.default_conditions:\n            print(f\"Default database filters:\")\n            for key, val in self.default_conditions.items():\n                print(\"  \", self.construct_filter_string(key, self.process_filter_value(val)))\n        else:\n            print(f\"No default filters.\")\n\n    # == retrieve table data ==\n    def table_data(self) -&gt; Tuple[List[str], List[Tuple[Any, ...]]]:\n        \"\"\"\n        Retrieve data and field names from the specified table.\n\n        This method retrieves the field names and table elements from the table specified\n        in the `BaseDatabase` instance.\n\n        Returns\n        -------\n        tuple\n            A tuple containing two elements:\n            - A list of strings representing the field names of the table.\n            - A list of tuples representing the data rows of the table.\n        \"\"\"\n        with self.open_cursor() as cursor:\n            field_names = [col.column_name for col in cursor.columns(table=self.table_name)]\n            cursor.execute(f\"SELECT * FROM {self.table_name}\")\n            table_elements = cursor.fetchall()\n\n        return field_names, table_elements\n\n    def get_table(self, use_default: bool = True, **kw_conditions: Any) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieve data from table in database and return as dataframe with optional filtering.\n\n        This method retrieves all data from the primary table in the database specified in\n        BaseDatabase instance. It automatically filters the data using the defaultConditions\n        defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is\n        a conflict.\n\n        Parameters\n        ----------\n        use_default : bool, default=True\n            Use default conditions if true, if False ignore them\n        **kw_conditions : dict, optional\n            Additional filtering conditions as keyword arguments.\n            Each condition should match a column name in the table.\n            Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair.\n            The operation defaults to '==', but you can use anything that works as a df query.\n\n            Examples:\n                - Simple equality: ``imaging=True`` filters where imaging column equals True\n                - Comparison operators: ``sessionID=(5, '&gt;')`` filters where sessionID &gt; 5\n                - Multiple conditions: ``imaging=True, mouseName='ATL028'`` applies AND logic\n\n            Note: this is limited in the sense that empty data can't be identified with key:None.\n            (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())\n\n        Returns\n        -------\n        df : pandas dataframe\n            A dataframe containing the filtered data from the primary database table.\n\n        Example\n        -------\n        &gt;&gt;&gt; vrdb = YourDatabaseClass()\n        &gt;&gt;&gt; df = vrdb.get_table(imaging=True)\n        &gt;&gt;&gt; df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))\n        \"\"\"\n\n        field_names, table_data = self.table_data()\n        df = pd.DataFrame.from_records(table_data, columns=field_names)\n        conditions = copy(self.default_conditions) if use_default else {}\n        conditions.update(kw_conditions)\n        if conditions:\n            for key, val in conditions.items():\n                assert key in field_names, f\"{key} is not a column name in {self.table_name}\"\n                conditions[key] = self.process_filter_value(val)  # make sure it's a value/operation pair\n            query = \" &amp; \".join([self.construct_filter_string(key, val_op_tuple) for key, val_op_tuple in conditions.items()])\n            df = df.query(query)\n        return df\n\n    def process_filter_value(self, val: Union[Any, Tuple[Any, str]]) -&gt; Tuple[Any, str]:\n        \"\"\"\n        Ensure filter value has an operation associated with it.\n\n        Filters are passed to pandas DataFrame queries as {key}{operation}{value}.\n        This method ensures each value is a tuple of (value, operation), defaulting\n        to '==' if no operation is specified.\n\n        Parameters\n        ----------\n        val : any or tuple\n            Filter value. If a tuple, should be (value, operation). If not a tuple,\n            will be converted to (val, '==').\n\n        Returns\n        -------\n        tuple\n            Tuple of (value, operation) for use in DataFrame queries.\n        \"\"\"\n        if not isinstance(val, tuple):\n            val = (val, \"==\")\n        return val\n\n    def construct_filter_string(self, key: str, val_op_tuple: Tuple[Any, str]) -&gt; str:\n        \"\"\"\n        Construct a string to be used as a pandas DataFrame query expression.\n\n        Parameters\n        ----------\n        key : str\n            Column name to filter on.\n        val_op_tuple : tuple\n            Tuple of (value, operation) where operation is a comparison operator\n            (e.g., '==', '!=', '&gt;', '&lt;').\n\n        Returns\n        -------\n        str\n            Query string in the format `column_name`operator'value'.\n        \"\"\"\n        val, op = val_op_tuple\n        return f\"`{key}`{op}{val!r}\"\n\n    # == methods for adding records and updating information to the database ==\n    def create_update_statement(self, field: str, uid: Any) -&gt; str:\n        \"\"\"\n        Create an SQL UPDATE statement for a single field and record.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n        uid : any\n            Unique identifier value for the record to update.\n\n        Returns\n        -------\n        str\n            SQL UPDATE statement with parameter placeholder.\n\n        Example\n        -------\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)\n        \"\"\"\n        return f\"UPDATE {self.table_name} set {field} = ? WHERE {self.uid} = {uid}\"\n\n    def create_update_many_statement(self, field: str) -&gt; str:\n        \"\"\"\n        Create an SQL UPDATE statement for batch updating a single field.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n\n        Returns\n        -------\n        str\n            SQL UPDATE statement with parameter placeholders for value and uid.\n\n        Example\n        -------\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     stmt = self.create_update_many_statement(\"fieldName\")\n        ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])\n        \"\"\"\n        return f\"UPDATE {self.table_name} set {field} = ? where {self.uid} = ?\"\n\n    def update_database_field(self, field: str, val: Any, **kw_conditions: Any) -&gt; None:\n        \"\"\"\n        Update a database field for all records matching specified conditions.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n        val : any\n            Value to set for the field.\n        **kw_conditions : dict, optional\n            Filtering conditions to identify records to update.\n            See get_table() documentation for filtering syntax.\n            Examples: ``mouseName='ATL028'``, ``imaging=True``\n\n        Raises\n        ------\n        AssertionError\n            If the specified field is not in the database table.\n        \"\"\"\n        assert field in self.table_data()[0], f\"Requested field ({field}) is not in table. Use 'self.table_data()[0]' to see available fields.\"\n        df = self.get_table(**kw_conditions)\n        update_statement = self.create_update_many_statement(field)\n        uids = df[self.uid].tolist()  # uids of all sessions requested\n        val_as_list = [val] * len(uids)\n        print(f\"Setting {field}={val} for all requested records...\")\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(update_statement, zip(val_as_list, uids))\n\n    # == method for adding a record to the database ==\n    def add_record(self, insert_statement: str, columns: List[str], values: List[Any]) -&gt; str:\n        \"\"\"\n        Add a single record to the database.\n\n        First checks if a record with matching unique field values already exists.\n        If so, prevents duplicate insertion and returns a message. Otherwise,\n        adds the new record to the database.\n\n        Parameters\n        ----------\n        insert_statement : str\n            SQL INSERT statement with parameter placeholders.\n        columns : list\n            List of column names matching the insert statement.\n        values : list\n            List of values to insert, corresponding to the columns.\n\n        Returns\n        -------\n        str\n            Success or duplicate record message.\n        \"\"\"\n        d = dict(zip(columns, values))\n        unique_values = [d[uf[0]] for uf in self.unique_fields]  # get values associated with unique fields\n        for ii, uv in enumerate(unique_values):\n            if isinstance(uv, date) or isinstance(uv, datetime):\n                # this is required for communicating with Access\n                unique_values[ii] = uv.strftime(\"%Y-%m-%d\")\n        unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n        if self.get_record(*unique_values, verbose=False) is not None:\n            print(f\"Record already exists for {unique_combo}\")\n            return f\"Record already exists for {unique_combo}\"\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(insert_statement, values)\n            print(f\"Successfully added new record for {unique_combo}\")\n        return \"Successfully added new record\"\n\n    def get_record(self, *unique_values: Any, verbose: bool = True) -&gt; Optional[pd.Series]:\n        \"\"\"\n        Retrieve single record from table in database and return as dataframe.\n\n        This method retrieves a single record(row) from the table in the database. The metadata for\n        each database defines a set of fields that comprise a unique set (each combination of values\n        for the unique fields is only represented once in the database).\n\n        Parameters\n        ----------\n        *unique_values: variable length list of values associated with the unique fields\n            - must be the same length as self.uniqueFields\n            - the second value of the uniqueField tuple (string by default) determines how\n              to query the unique value\n\n        Returns\n        -------\n        record : pandas Series\n\n        Example\n        -------\n        &gt;&gt;&gt; vrdb = YourDatabaseClass()\n        &gt;&gt;&gt; record = vrdb.get_record(*unique_conditions)\n        \"\"\"\n\n        # Check if correct values are provided\n        if len(unique_values) != len(self.unique_fields):\n            expected_list = \", \".join([uf[0] for uf in self.unique_fields])\n            raise ValueError(f\"{len(unique_values)} values provided but *get_record* is expecting values for: {expected_list}\")\n\n        # Get table and compare\n        df = self.get_table()\n        for uf, uv in zip(self.unique_fields, unique_values):\n            if uf[1] == str:\n                df = df[df[uf[0]] == uv]\n            elif uf[1] == datetime:\n                df = df[df[uf[0]].apply(lambda sd: sd.strftime(\"%Y-%m-%d\")) == uv]\n            elif uf[1] == int:\n                df = df[df[uf[0]] == int(uv)]\n            else:\n                raise ValueError(f\"uniqueField type ({uf[1]}) not recognized, add the appropriate query to this method!\")\n\n        if len(df) == 0:\n            if verbose:\n                unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n                print(f\"No session found under: {unique_combo}\")\n            return None\n\n        if len(df) &gt; 1:\n            unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n            raise ValueError(f\"Multiple sessions found under: {unique_combo}\")\n        return df.iloc[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.__init__","title":"<code>__init__(db_name)</code>","text":"<p>Initialize a new database instance.</p> <p>This constructor initializes a new instance of the BaseDatabase class. It sets the default values for the table name, database name, and database path. It is built to work with the Microsoft Access application; however, a few small changes can make it compatible with other SQL-based database systems.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>(str, required)</code> <p>The name of the database to access.</p> required Example <p>db = BaseDatabase('vrSessions') print(vrdb.table_name) 'sessiondb' print(vrdb.db_name) 'vrDatabase'</p> Notes <ul> <li>This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.</li> <li>If you are using this on a new system, then you should edit your path, database name, and default table in that function.</li> </ul> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def __init__(self, db_name: str):\n    \"\"\"\n    Initialize a new database instance.\n\n    This constructor initializes a new instance of the BaseDatabase class. It sets the default\n    values for the table name, database name, and database path. It is built to work with\n    the Microsoft Access application; however, a few small changes can make it compatible with\n    other SQL-based database systems.\n\n    Parameters\n    ----------\n    db_name : str, required\n        The name of the database to access.\n\n    Example\n    -------\n    &gt;&gt;&gt; db = BaseDatabase('vrSessions')\n    &gt;&gt;&gt; print(vrdb.table_name)\n    'sessiondb'\n    &gt;&gt;&gt; print(vrdb.db_name)\n    'vrDatabase'\n\n    Notes\n    -----\n    - This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.\n    - If you are using this on a new system, then you should edit your path, database name, and default table in that function.\n    \"\"\"\n\n    metadata = get_database_metadata(db_name)\n    self.db_path = metadata[\"db_path\"]\n    self.db_name = metadata[\"db_name\"]\n    self.db_ext = metadata[\"db_ext\"]\n    self.table_name = metadata[\"table_name\"]\n    self.uid = metadata[\"uid\"]\n    self.backup_path = metadata[\"backup_path\"]\n    self.host_type = host_types[self.db_ext]\n    self.unique_fields = self.process_unique_fields(metadata[\"unique_fields\"])\n    self.default_conditions = metadata[\"default_conditions\"]\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.add_record","title":"<code>add_record(insert_statement, columns, values)</code>","text":"<p>Add a single record to the database.</p> <p>First checks if a record with matching unique field values already exists. If so, prevents duplicate insertion and returns a message. Otherwise, adds the new record to the database.</p> <p>Parameters:</p> Name Type Description Default <code>insert_statement</code> <code>str</code> <p>SQL INSERT statement with parameter placeholders.</p> required <code>columns</code> <code>list</code> <p>List of column names matching the insert statement.</p> required <code>values</code> <code>list</code> <p>List of values to insert, corresponding to the columns.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Success or duplicate record message.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def add_record(self, insert_statement: str, columns: List[str], values: List[Any]) -&gt; str:\n    \"\"\"\n    Add a single record to the database.\n\n    First checks if a record with matching unique field values already exists.\n    If so, prevents duplicate insertion and returns a message. Otherwise,\n    adds the new record to the database.\n\n    Parameters\n    ----------\n    insert_statement : str\n        SQL INSERT statement with parameter placeholders.\n    columns : list\n        List of column names matching the insert statement.\n    values : list\n        List of values to insert, corresponding to the columns.\n\n    Returns\n    -------\n    str\n        Success or duplicate record message.\n    \"\"\"\n    d = dict(zip(columns, values))\n    unique_values = [d[uf[0]] for uf in self.unique_fields]  # get values associated with unique fields\n    for ii, uv in enumerate(unique_values):\n        if isinstance(uv, date) or isinstance(uv, datetime):\n            # this is required for communicating with Access\n            unique_values[ii] = uv.strftime(\"%Y-%m-%d\")\n    unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n    if self.get_record(*unique_values, verbose=False) is not None:\n        print(f\"Record already exists for {unique_combo}\")\n        return f\"Record already exists for {unique_combo}\"\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.execute(insert_statement, values)\n        print(f\"Successfully added new record for {unique_combo}\")\n    return \"Successfully added new record\"\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.connect","title":"<code>connect()</code>","text":"<p>Establish a connection to the database.</p> <p>Creates a pyodbc connection using the appropriate driver string based on the database file extension. Currently configured for Microsoft Access databases (.accdb, .mdb files).</p> <p>Returns:</p> Type Description <code>Connection</code> <p>Database connection object.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the host type for the database extension is not supported.</p> Notes <p>To support additional database types: 1. Determine the appropriate pyodbc driver string for your database 2. Add it to the driver_string dictionary in this method 3. Update the host_types dictionary at the module level to map your    file extension to the driver key</p> <p>See https://www.connectionstrings.com/ for driver string examples.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def connect(self) -&gt; pyodbc.Connection:\n    \"\"\"\n    Establish a connection to the database.\n\n    Creates a pyodbc connection using the appropriate driver string based on\n    the database file extension. Currently configured for Microsoft Access\n    databases (.accdb, .mdb files).\n\n    Returns\n    -------\n    pyodbc.Connection\n        Database connection object.\n\n    Raises\n    ------\n    AssertionError\n        If the host type for the database extension is not supported.\n\n    Notes\n    -----\n    To support additional database types:\n    1. Determine the appropriate pyodbc driver string for your database\n    2. Add it to the driver_string dictionary in this method\n    3. Update the host_types dictionary at the module level to map your\n       file extension to the driver key\n\n    See https://www.connectionstrings.com/ for driver string examples.\n    \"\"\"\n    driver_string = {\"access\": r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\" + rf\"DBQ={self.get_dbfile()};\"}\n\n    # Make sure connections are possible for this hosttype\n    failure_message = (\n        f\"Requested host_type ({self.host_type}) is not available. The only ones that are coded are: {[k for k in driver_string.keys()]}\\n\\n\"\n        f\"For support with writing a driver string for a different host, use the fantastic website: https://www.connectionstrings.com/\"\n    )\n    assert self.host_type in driver_string, failure_message\n\n    # Return a connection to the database\n    return pyodbc.connect(driver_string[self.host_type])\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.construct_filter_string","title":"<code>construct_filter_string(key, val_op_tuple)</code>","text":"<p>Construct a string to be used as a pandas DataFrame query expression.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Column name to filter on.</p> required <code>val_op_tuple</code> <code>tuple</code> <p>Tuple of (value, operation) where operation is a comparison operator (e.g., '==', '!=', '&gt;', '&lt;').</p> required <p>Returns:</p> Type Description <code>str</code> <p>Query string in the format <code>column_name</code>operator'value'.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def construct_filter_string(self, key: str, val_op_tuple: Tuple[Any, str]) -&gt; str:\n    \"\"\"\n    Construct a string to be used as a pandas DataFrame query expression.\n\n    Parameters\n    ----------\n    key : str\n        Column name to filter on.\n    val_op_tuple : tuple\n        Tuple of (value, operation) where operation is a comparison operator\n        (e.g., '==', '!=', '&gt;', '&lt;').\n\n    Returns\n    -------\n    str\n        Query string in the format `column_name`operator'value'.\n    \"\"\"\n    val, op = val_op_tuple\n    return f\"`{key}`{op}{val!r}\"\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.create_update_many_statement","title":"<code>create_update_many_statement(field)</code>","text":"<p>Create an SQL UPDATE statement for batch updating a single field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL UPDATE statement with parameter placeholders for value and uid.</p> Example <p>with self.open_cursor(commit_changes=True) as cursor: ...     stmt = self.create_update_many_statement(\"fieldName\") ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def create_update_many_statement(self, field: str) -&gt; str:\n    \"\"\"\n    Create an SQL UPDATE statement for batch updating a single field.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n\n    Returns\n    -------\n    str\n        SQL UPDATE statement with parameter placeholders for value and uid.\n\n    Example\n    -------\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     stmt = self.create_update_many_statement(\"fieldName\")\n    ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])\n    \"\"\"\n    return f\"UPDATE {self.table_name} set {field} = ? where {self.uid} = ?\"\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.create_update_statement","title":"<code>create_update_statement(field, uid)</code>","text":"<p>Create an SQL UPDATE statement for a single field and record.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <code>uid</code> <code>any</code> <p>Unique identifier value for the record to update.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL UPDATE statement with parameter placeholder.</p> Example <p>with self.open_cursor(commit_changes=True) as cursor: ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def create_update_statement(self, field: str, uid: Any) -&gt; str:\n    \"\"\"\n    Create an SQL UPDATE statement for a single field and record.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n    uid : any\n        Unique identifier value for the record to update.\n\n    Returns\n    -------\n    str\n        SQL UPDATE statement with parameter placeholder.\n\n    Example\n    -------\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)\n    \"\"\"\n    return f\"UPDATE {self.table_name} set {field} = ? WHERE {self.uid} = {uid}\"\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.get_dbfile","title":"<code>get_dbfile()</code>","text":"<p>Get the full path to the database file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the database file.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def get_dbfile(self) -&gt; Path:\n    \"\"\"\n    Get the full path to the database file.\n\n    Returns\n    -------\n    Path\n        Path object pointing to the database file.\n    \"\"\"\n    return Path(self.db_path) / (self.db_name + self.db_ext)\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.get_record","title":"<code>get_record(*unique_values, verbose=True)</code>","text":"<p>Retrieve single record from table in database and return as dataframe.</p> <p>This method retrieves a single record(row) from the table in the database. The metadata for each database defines a set of fields that comprise a unique set (each combination of values for the unique fields is only represented once in the database).</p> <p>Parameters:</p> Name Type Description Default <code>*unique_values</code> <code>Any</code> <ul> <li>must be the same length as self.uniqueFields</li> <li>the second value of the uniqueField tuple (string by default) determines how   to query the unique value</li> </ul> <code>()</code> <p>Returns:</p> Name Type Description <code>record</code> <code>pandas Series</code> Example <p>vrdb = YourDatabaseClass() record = vrdb.get_record(*unique_conditions)</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def get_record(self, *unique_values: Any, verbose: bool = True) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Retrieve single record from table in database and return as dataframe.\n\n    This method retrieves a single record(row) from the table in the database. The metadata for\n    each database defines a set of fields that comprise a unique set (each combination of values\n    for the unique fields is only represented once in the database).\n\n    Parameters\n    ----------\n    *unique_values: variable length list of values associated with the unique fields\n        - must be the same length as self.uniqueFields\n        - the second value of the uniqueField tuple (string by default) determines how\n          to query the unique value\n\n    Returns\n    -------\n    record : pandas Series\n\n    Example\n    -------\n    &gt;&gt;&gt; vrdb = YourDatabaseClass()\n    &gt;&gt;&gt; record = vrdb.get_record(*unique_conditions)\n    \"\"\"\n\n    # Check if correct values are provided\n    if len(unique_values) != len(self.unique_fields):\n        expected_list = \", \".join([uf[0] for uf in self.unique_fields])\n        raise ValueError(f\"{len(unique_values)} values provided but *get_record* is expecting values for: {expected_list}\")\n\n    # Get table and compare\n    df = self.get_table()\n    for uf, uv in zip(self.unique_fields, unique_values):\n        if uf[1] == str:\n            df = df[df[uf[0]] == uv]\n        elif uf[1] == datetime:\n            df = df[df[uf[0]].apply(lambda sd: sd.strftime(\"%Y-%m-%d\")) == uv]\n        elif uf[1] == int:\n            df = df[df[uf[0]] == int(uv)]\n        else:\n            raise ValueError(f\"uniqueField type ({uf[1]}) not recognized, add the appropriate query to this method!\")\n\n    if len(df) == 0:\n        if verbose:\n            unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n            print(f\"No session found under: {unique_combo}\")\n        return None\n\n    if len(df) &gt; 1:\n        unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n        raise ValueError(f\"Multiple sessions found under: {unique_combo}\")\n    return df.iloc[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.get_table","title":"<code>get_table(use_default=True, **kw_conditions)</code>","text":"<p>Retrieve data from table in database and return as dataframe with optional filtering.</p> <p>This method retrieves all data from the primary table in the database specified in BaseDatabase instance. It automatically filters the data using the defaultConditions defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is a conflict.</p> <p>Parameters:</p> Name Type Description Default <code>use_default</code> <code>bool</code> <p>Use default conditions if true, if False ignore them</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions as keyword arguments. Each condition should match a column name in the table. Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair. The operation defaults to '==', but you can use anything that works as a df query.</p> <p>Examples:     - Simple equality: <code>imaging=True</code> filters where imaging column equals True     - Comparison operators: <code>sessionID=(5, '&gt;')</code> filters where sessionID &gt; 5     - Multiple conditions: <code>imaging=True, mouseName='ATL028'</code> applies AND logic</p> <p>Note: this is limited in the sense that empty data can't be identified with key:None. (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas dataframe</code> <p>A dataframe containing the filtered data from the primary database table.</p> Example <p>vrdb = YourDatabaseClass() df = vrdb.get_table(imaging=True) df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def get_table(self, use_default: bool = True, **kw_conditions: Any) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieve data from table in database and return as dataframe with optional filtering.\n\n    This method retrieves all data from the primary table in the database specified in\n    BaseDatabase instance. It automatically filters the data using the defaultConditions\n    defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is\n    a conflict.\n\n    Parameters\n    ----------\n    use_default : bool, default=True\n        Use default conditions if true, if False ignore them\n    **kw_conditions : dict, optional\n        Additional filtering conditions as keyword arguments.\n        Each condition should match a column name in the table.\n        Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair.\n        The operation defaults to '==', but you can use anything that works as a df query.\n\n        Examples:\n            - Simple equality: ``imaging=True`` filters where imaging column equals True\n            - Comparison operators: ``sessionID=(5, '&gt;')`` filters where sessionID &gt; 5\n            - Multiple conditions: ``imaging=True, mouseName='ATL028'`` applies AND logic\n\n        Note: this is limited in the sense that empty data can't be identified with key:None.\n        (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())\n\n    Returns\n    -------\n    df : pandas dataframe\n        A dataframe containing the filtered data from the primary database table.\n\n    Example\n    -------\n    &gt;&gt;&gt; vrdb = YourDatabaseClass()\n    &gt;&gt;&gt; df = vrdb.get_table(imaging=True)\n    &gt;&gt;&gt; df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))\n    \"\"\"\n\n    field_names, table_data = self.table_data()\n    df = pd.DataFrame.from_records(table_data, columns=field_names)\n    conditions = copy(self.default_conditions) if use_default else {}\n    conditions.update(kw_conditions)\n    if conditions:\n        for key, val in conditions.items():\n            assert key in field_names, f\"{key} is not a column name in {self.table_name}\"\n            conditions[key] = self.process_filter_value(val)  # make sure it's a value/operation pair\n        query = \" &amp; \".join([self.construct_filter_string(key, val_op_tuple) for key, val_op_tuple in conditions.items()])\n        df = df.query(query)\n    return df\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.open_cursor","title":"<code>open_cursor(commit_changes=False)</code>","text":"<p>Context manager to open a database cursor and manage connections.</p> <p>This context manager provides a convenient way to open a cursor to the database, perform database operations, and manage connections. It also allows you to commit changes if needed.</p> <p>Parameters:</p> Name Type Description Default <code>commit_changes</code> <code>bool</code> <p>Whether to commit changes to the database. Default is False.</p> <code>False</code> <p>Yields:</p> Type Description <code>Cursor</code> <p>A database cursor for executing SQL queries.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while connecting to the database.</p> Example <p>Use the context manager to perform database operations:</p> <p>with self.open_cursor(commit_changes=True) as cursor: ...     cursor.execute(\"SELECT * FROM your_table\")</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>@contextmanager\ndef open_cursor(self, commit_changes: bool = False) -&gt; Generator[pyodbc.Cursor, None, None]:\n    \"\"\"\n    Context manager to open a database cursor and manage connections.\n\n    This context manager provides a convenient way to open a cursor to the database,\n    perform database operations, and manage connections. It also allows you to\n    commit changes if needed.\n\n    Parameters\n    ----------\n    commit_changes : bool, optional\n        Whether to commit changes to the database. Default is False.\n\n    Yields\n    ------\n    pyodbc.Cursor\n        A database cursor for executing SQL queries.\n\n    Raises\n    ------\n    Exception\n        If an error occurs while connecting to the database.\n\n    Example\n    -------\n    Use the context manager to perform database operations:\n\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     cursor.execute(\"SELECT * FROM your_table\")\n\n    \"\"\"\n    try:\n        # Attempt to open a cursor to the database\n        conn = self.connect()\n        cursor = conn.cursor()\n        yield cursor\n    except Exception as ex:\n        print(f\"An exception occurred while trying to connect to {self.db_name}!\")\n        print(ex)\n        raise ex\n    else:\n        # if no exception was raised, commit changes\n        if commit_changes:\n            conn.commit()\n    finally:\n        # Always close the cursor and connection\n        cursor.close()\n        conn.close()\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.process_filter_value","title":"<code>process_filter_value(val)</code>","text":"<p>Ensure filter value has an operation associated with it.</p> <p>Filters are passed to pandas DataFrame queries as {key}{operation}{value}. This method ensures each value is a tuple of (value, operation), defaulting to '==' if no operation is specified.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>any or tuple</code> <p>Filter value. If a tuple, should be (value, operation). If not a tuple, will be converted to (val, '==').</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (value, operation) for use in DataFrame queries.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def process_filter_value(self, val: Union[Any, Tuple[Any, str]]) -&gt; Tuple[Any, str]:\n    \"\"\"\n    Ensure filter value has an operation associated with it.\n\n    Filters are passed to pandas DataFrame queries as {key}{operation}{value}.\n    This method ensures each value is a tuple of (value, operation), defaulting\n    to '==' if no operation is specified.\n\n    Parameters\n    ----------\n    val : any or tuple\n        Filter value. If a tuple, should be (value, operation). If not a tuple,\n        will be converted to (val, '==').\n\n    Returns\n    -------\n    tuple\n        Tuple of (value, operation) for use in DataFrame queries.\n    \"\"\"\n    if not isinstance(val, tuple):\n        val = (val, \"==\")\n    return val\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.process_unique_fields","title":"<code>process_unique_fields(fields)</code>","text":"<p>Process and validate unique field definitions.</p> <p>Converts unique field specifications into a standardized format where each field is a tuple of (field_name, field_type). String fields can be specified as just the name and will default to str type.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>List of field specifications. Each can be: - A string (field name, defaults to str type) - A tuple of (field_name, type) where type is a Python type class</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of tuples, each containing (field_name, field_type).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a field specification is not a string or a valid (string, type) tuple.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def process_unique_fields(self, fields: List[Union[str, Tuple[str, type]]]) -&gt; List[Tuple[str, type]]:\n    \"\"\"\n    Process and validate unique field definitions.\n\n    Converts unique field specifications into a standardized format where each\n    field is a tuple of (field_name, field_type). String fields can be specified\n    as just the name and will default to str type.\n\n    Parameters\n    ----------\n    fields : list\n        List of field specifications. Each can be:\n        - A string (field name, defaults to str type)\n        - A tuple of (field_name, type) where type is a Python type class\n\n    Returns\n    -------\n    list\n        List of tuples, each containing (field_name, field_type).\n\n    Raises\n    ------\n    ValueError\n        If a field specification is not a string or a valid (string, type) tuple.\n    \"\"\"\n    ufields = []\n    for f in fields:\n        if isinstance(f, tuple) and len(f) == 2 and type(f[1]) == type and isinstance(f[0], str):\n            ufields.append(f)\n        elif isinstance(f, str):\n            ufields.append((f, str))\n        else:\n            raise ValueError(f\"unique field {f} must be a string or a string-type tuple\")\n    return ufields\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.save_backup","title":"<code>save_backup(return_out=False)</code>","text":"<p>Save a backup of the database to the backup path specified in metadata.</p> <p>Parameters:</p> Name Type Description Default <code>return_out</code> <code>bool</code> <p>If True, return the output of the robocopy command.</p> <code>False</code> <p>Returns:</p> Type Description <code>CompletedProcess or None</code> <p>CompletedProcess object from the robocopy command (if return_out is True). Returns None if return_out is False.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def save_backup(self, return_out: bool = False) -&gt; Optional[CompletedProcess]:\n    \"\"\"Save a backup of the database to the backup path specified in metadata.\n\n    Parameters\n    ----------\n    return_out : bool, optional\n        If True, return the output of the robocopy command.\n\n    Returns\n    -------\n    CompletedProcess or None\n        CompletedProcess object from the robocopy command (if return_out is True).\n        Returns None if return_out is False.\n    \"\"\"\n    source_path = self.db_path\n    target_path = self.backup_path\n    source_file = self.db_name + self.db_ext\n    robocopy_arguments = f\"robocopy {source_path} {target_path} {source_file}\"\n    outs = run(robocopy_arguments, capture_output=True, text=True)\n    if return_out:\n        return outs\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.show_metadata","title":"<code>show_metadata()</code>","text":"<p>Display metadata associated with the open database.</p> <p>Prints information about the database location, name, table, unique ID field, backup path, and default filtering conditions.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def show_metadata(self) -&gt; None:\n    \"\"\"\n    Display metadata associated with the open database.\n\n    Prints information about the database location, name, table, unique ID field,\n    backup path, and default filtering conditions.\n    \"\"\"\n    print(f\"{self.host_type} database located at {self.db_path}\")\n    print(f\"Database name: {self.db_name}{self.db_ext}, table name: {self.table_name}, with uid: {self.uid}\")\n    if self.backup_path is not None:\n        print(f\"Backup path located at: {self.backup_path}\")\n    else:\n        print(f\"No backup path specified...\")\n    if self.default_conditions:\n        print(f\"Default database filters:\")\n        for key, val in self.default_conditions.items():\n            print(\"  \", self.construct_filter_string(key, self.process_filter_value(val)))\n    else:\n        print(f\"No default filters.\")\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.table_data","title":"<code>table_data()</code>","text":"<p>Retrieve data and field names from the specified table.</p> <p>This method retrieves the field names and table elements from the table specified in the <code>BaseDatabase</code> instance.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two elements: - A list of strings representing the field names of the table. - A list of tuples representing the data rows of the table.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def table_data(self) -&gt; Tuple[List[str], List[Tuple[Any, ...]]]:\n    \"\"\"\n    Retrieve data and field names from the specified table.\n\n    This method retrieves the field names and table elements from the table specified\n    in the `BaseDatabase` instance.\n\n    Returns\n    -------\n    tuple\n        A tuple containing two elements:\n        - A list of strings representing the field names of the table.\n        - A list of tuples representing the data rows of the table.\n    \"\"\"\n    with self.open_cursor() as cursor:\n        field_names = [col.column_name for col in cursor.columns(table=self.table_name)]\n        cursor.execute(f\"SELECT * FROM {self.table_name}\")\n        table_elements = cursor.fetchall()\n\n    return field_names, table_elements\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.BaseDatabase.update_database_field","title":"<code>update_database_field(field, val, **kw_conditions)</code>","text":"<p>Update a database field for all records matching specified conditions.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <code>val</code> <code>any</code> <p>Value to set for the field.</p> required <code>**kw_conditions</code> <code>dict</code> <p>Filtering conditions to identify records to update. See get_table() documentation for filtering syntax. Examples: <code>mouseName='ATL028'</code>, <code>imaging=True</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the specified field is not in the database table.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def update_database_field(self, field: str, val: Any, **kw_conditions: Any) -&gt; None:\n    \"\"\"\n    Update a database field for all records matching specified conditions.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n    val : any\n        Value to set for the field.\n    **kw_conditions : dict, optional\n        Filtering conditions to identify records to update.\n        See get_table() documentation for filtering syntax.\n        Examples: ``mouseName='ATL028'``, ``imaging=True``\n\n    Raises\n    ------\n    AssertionError\n        If the specified field is not in the database table.\n    \"\"\"\n    assert field in self.table_data()[0], f\"Requested field ({field}) is not in table. Use 'self.table_data()[0]' to see available fields.\"\n    df = self.get_table(**kw_conditions)\n    update_statement = self.create_update_many_statement(field)\n    uids = df[self.uid].tolist()  # uids of all sessions requested\n    val_as_list = [val] * len(uids)\n    print(f\"Setting {field}={val} for all requested records...\")\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(update_statement, zip(val_as_list, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase","title":"<code>SessionDatabase</code>","text":"<p>               Bases: <code>BaseDatabase</code></p> <p>Database class for handling VR session data.</p> <p>Specialized database class that extends BaseDatabase with session-specific functionality, including methods for creating session objects, managing registration workflows, and handling quality control processes.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>class SessionDatabase(BaseDatabase):\n    \"\"\"\n    Database class for handling VR session data.\n\n    Specialized database class that extends BaseDatabase with session-specific\n    functionality, including methods for creating session objects, managing\n    registration workflows, and handling quality control processes.\n    \"\"\"\n\n    def iter_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; List[B2Session]:\n        \"\"\"Iterate over sessions matching conditions.\n\n        Parameters\n        ----------\n        session_params : dict, default={}\n            Additional parameters to pass to the session constructor when creating\n            B2Session objects. These are passed through to create_b2session().\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n            Examples: ``mouseName='ATL028'``, ``imaging=True``, ``sessionID=(5, '&gt;')``\n\n        Returns\n        -------\n        sessions : list[B2Session]\n            List of sessions matching the conditions.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        sessions = []\n        for _, row in df.iterrows():\n            sessions.append(create_b2session(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params))\n        return sessions\n\n    # == EVERYTHING BELOW HERE IS THE SAME AS THE ORIGINAL DATABASE CLASS ==\n    # == It should be refactored to use the new vrAnalysis2 classes eventually, but I'm leaving it here for now ==\n    # == It should work as is as long as vrAnalysis stays on the path! ==\n\n    # == vrExperiment related methods ==\n    def session_name(self, row: pd.Series) -&gt; Tuple[str, str, str]:\n        \"\"\"\n        Extract session identifiers from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n\n        Returns\n        -------\n        tuple\n            Tuple of (mouse_name, session_date, session_id) where session_date is\n            formatted as 'YYYY-MM-DD' and session_id is converted to string.\n        \"\"\"\n        mouse_name = row[\"mouseName\"]\n        session_date = row[\"sessionDate\"].strftime(\"%Y-%m-%d\")\n        session_id = str(row[\"sessionID\"])\n        return mouse_name, session_date, session_id\n\n    def make_b2session(self, row: pd.Series) -&gt; B2Session:\n        \"\"\"\n        Create a B2Session object from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n\n        Returns\n        -------\n        B2Session\n            Session object initialized with data from the record.\n        \"\"\"\n        mouse_name, session_date, session_id = self.session_name(row)\n        return create_b2session(mouse_name, session_date, session_id)\n\n    def make_b2registration(self, row: pd.Series, opts: B2RegistrationOpts) -&gt; B2Registration:\n        \"\"\"\n        Create a B2Registration object from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n        opts : B2RegistrationOpts\n            Registration options to use for the session.\n\n        Returns\n        -------\n        B2Registration\n            Registration object initialized with session data and options.\n        \"\"\"\n        mouse_name, session_date, session_id = self.session_name(row)\n        return B2Registration(mouse_name, session_date, session_id, opts)\n\n    # == helper functions for figuring out what needs work ==\n    def needs_registration(self, skip_errors: bool = True, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need registration preprocessing.\n\n        Parameters\n        ----------\n        skip_errors : bool, default=True\n            If True, exclude sessions that had registration errors.\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need registration.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        if skip_errors:\n            df = df[df[\"vrRegistrationError\"] == False]\n        df = df[df[\"vrRegistration\"] == False]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                session = self.make_b2session(row)\n                print(f\"Session needs registration: {session.session_print()}\")\n            return None\n\n    def update_s2p_date_time(self) -&gt; None:\n        \"\"\"\n        Update suite2p creation dates in the database based on file modification times.\n\n        For all sessions where suite2p processing is complete, finds the most recent\n        file modification time in the suite2p output directory and updates the\n        suite2pDate field in the database.\n        \"\"\"\n        df = self.get_table()\n        s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n        uids = s2p_done[self.uid].tolist()\n        s2p_creation_date = []\n        for idx, row in s2p_done.iterrows():\n            session = self.make_b2session(row)  # create vrSession to point to session folder\n            c_latest_mod = 0\n            for p in session.s2p_path.rglob(\"*\"):\n                if not (p.is_dir()):\n                    c_latest_mod = max(p.stat().st_mtime, c_latest_mod)\n            c_date_time = datetime.fromtimestamp(c_latest_mod)\n            s2p_creation_date.append(c_date_time)  # get suite2p path creation date\n\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(self.create_update_many_statement(\"suite2pDate\"), zip(s2p_creation_date, uids))\n\n    def needs_s2p(\n        self, needs_qc: bool = False, return_df: bool = True, print_targets: bool = True, **kw_conditions: Any\n    ) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need suite2p processing or quality control.\n\n        Parameters\n        ----------\n        needs_qc : bool, default=False\n            If False, returns/prints sessions that need suite2p processing.\n            If True, returns/prints sessions that need suite2p quality control.\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        print_targets : bool, default=True\n            If True and return_df=False, prints suite2p target information for sessions needing processing.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        if needs_qc:\n            df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"suite2pQC\"] == False)]\n        else:\n            df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                session = self.make_b2session(row)\n                if needs_qc:\n                    print(f\"Database indicates that suite2p has been run but not QC'd: {session.session_print()}\")\n                else:\n                    print(f\"Database indicates that suite2p has not been run: {session.session_print()}\")\n                    if print_targets:\n                        mouse_name, session_date, session_id = self.session_name(row)\n                        s2p_targets(mouse_name, session_date, session_id)\n                        print(\"\")\n            return None\n\n    def check_s2p(self, with_database_update: bool = False, return_check: bool = False) -&gt; Optional[bool]:\n        \"\"\"\n        Verify suite2p status consistency between database and file system.\n\n        Checks for discrepancies where:\n        - Database says suite2p is done but files don't exist\n        - Files exist but database says suite2p wasn't done\n\n        Parameters\n        ----------\n        with_database_update : bool, default=False\n            If True, automatically corrects database entries when discrepancies are found.\n        return_check : bool, default=False\n            If True, returns a boolean indicating whether any discrepancies were found.\n\n        Returns\n        -------\n        bool or None\n            If return_check is True, returns True if any discrepancies were found,\n            False otherwise. Returns None if return_check is False.\n        \"\"\"\n        df = self.get_table()\n\n        # Check sessions where database says suite2p is done but files don't exist\n        check_s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n        checked_not_done = check_s2p_done.apply(lambda row: not (self.make_b2session(row).s2p_path.exists()), axis=1)\n        not_actually_done = check_s2p_done[checked_not_done]\n\n        # Check sessions where files exist but database says suite2p wasn't done\n        check_s2p_needed = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n        checked_not_needed = check_s2p_needed.apply(lambda row: self.make_b2session(row).s2p_path.exists(), axis=1)\n        not_actually_needed = check_s2p_needed[checked_not_needed]\n\n        # Print database errors to workspace\n        for idx, row in not_actually_done.iterrows():\n            print(f\"Database said suite2p has been ran, but it actually hasn't: {self.make_b2session(row).session_print()}\")\n        for idx, row in not_actually_needed.iterrows():\n            print(f\"Database said suite2p didn't run, but it already did: {self.make_b2session(row).session_print()}\")\n\n        # If with_database_update is True, correct the database\n        if with_database_update:\n            for idx, row in not_actually_done.iterrows():\n                with self.open_cursor(commit_changes=True) as cursor:\n                    cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), False)\n\n            for idx, row in not_actually_needed.iterrows():\n                with self.open_cursor(commit_changes=True) as cursor:\n                    cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), True)\n\n        # If return_check is requested, return True if any records were invalid\n        if return_check:\n            return checked_not_done.any() or checked_not_needed.any()\n\n    # == for communicating with the database about red cell quality control ==\n    def update_red_cell_qc_date_time(self) -&gt; None:\n        \"\"\"\n        Update red cell QC dates in the database based on file modification times.\n\n        For all sessions where red cell QC is complete, finds the most recent\n        modification time of relevant red cell QC files and updates the\n        redCellQCDate field in the database.\n        \"\"\"\n        relevant_one_files = [\n            \"mpciROIs.redCellIdx.npy\",\n            \"mpciROIs.redCellManualAssignment.npy\",\n            \"parametersRed*\",  # wildcard because there are multiple possibilities\n        ]\n\n        df = self.get_table()\n        red_cell_qc_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"redCellQC\"] == True)]\n        uids = red_cell_qc_done[self.uid].tolist()\n        rc_edit_date = []\n        for idx, row in red_cell_qc_done.iterrows():\n            session = self.make_b2session(row)  # create vrSession to point to session folder\n            c_latest_mod = 0\n            for f in relevant_one_files:\n                for file in session.one_path.rglob(f):\n                    c_latest_mod = max(file.stat().st_mtime, c_latest_mod)\n            c_date_time = datetime.fromtimestamp(c_latest_mod)\n            rc_edit_date.append(c_date_time)  # get red cell QC file modification date\n\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(self.create_update_many_statement(\"redCellQCDate\"), zip(rc_edit_date, uids))\n\n    def needs_red_cell_qc(self, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need red cell quality control.\n\n        Parameters\n        ----------\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need red cell QC.\n            Sessions must have imaging, suite2p processing, and registration completed.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"vrRegistration\"] == True) &amp; (df[\"redCellQC\"] == False)]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                print(f\"Database indicates that redCellQC has not been performed for session: {self.make_b2session(row).session_print()}\")\n            return None\n\n    def iter_sessions_need_red_cell_qc(self, **kw_conditions: Any) -&gt; List[B2Session]:\n        \"\"\"\n        Get list of sessions that require red cell quality control.\n\n        Parameters\n        ----------\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        list[B2Session]\n            List of session objects that need red cell QC.\n        \"\"\"\n        df = self.needs_red_cell_qc(return_df=True, **kw_conditions)\n        sessions = []\n        for idx, row in df.iterrows():\n            sessions.append(self.make_b2session(row))\n        return sessions\n\n    def set_red_cell_qc(self, mouse_name: str, date_string: str, session_id: Union[str, int], state: bool = True) -&gt; bool:\n        \"\"\"\n        Set the red cell QC status for a specific session.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Mouse name identifier.\n        date_string : str\n            Session date in 'YYYY-MM-DD' format.\n        session_id : str or int\n            Session ID.\n        state : bool, default=True\n            Red cell QC status to set. If True, also sets the QC date to now.\n\n        Returns\n        -------\n        bool\n            True if update was successful, False otherwise.\n        \"\"\"\n        record = self.get_record(mouse_name, date_string, session_id)\n        if record is None:\n            print(f\"Could not find session {mouse_name}/{date_string}/{session_id} in database.\")\n            return False\n\n        try:\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"redCellQC\", record[self.uid]), state)\n                if state == True:\n                    # If setting red cell QC to true, add the date\n                    cursor.execute(\n                        self.create_update_statement(\"redCellQCDate\", record[self.uid]),\n                        datetime.now(),\n                    )\n                else:\n                    # Otherwise remove the date\n                    cursor.execute(self.create_update_statement(\"redCellQCDate\", record[self.uid]), \"\")\n            return True\n\n        except Exception as ex:\n            print(f\"Failed to update database for session: {mouse_name}/{date_string}/{session_id}\")\n            print(f\"Error: {ex}\")\n            return False\n\n    # == operating vrExperiment pipeline ==\n    def register_record(self, record: pd.Series, raise_exception: bool = False, imaging: Optional[bool] = None) -&gt; Tuple[bool, int]:\n        \"\"\"\n        Perform registration preprocessing for a single session record.\n\n        Creates a B2Registration object and runs preprocessing. Updates the database\n        with success/failure status and error information if applicable.\n\n        Parameters\n        ----------\n        record : pandas.Series\n            Database record containing session information.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting. If None (default), uses the value from the\n            database record. If True or False, overrides the database value.\n\n        Returns\n        -------\n        tuple\n            Tuple of (success, data_size) where:\n            - success: bool indicating if registration succeeded\n            - data_size: int size in bytes of registered oneData (0 if failed)\n\n        Notes\n        -----\n        On failure, clears all oneData files and updates database error fields.\n        On success, updates registration status and date in the database.\n        \"\"\"\n        opts = B2RegistrationOpts()\n        opts.imaging = bool(imaging) if imaging is not None else bool(record[\"imaging\"])\n        opts.facecam = bool(record[\"faceCamera\"])\n        opts.vrBehaviorVersion = record[\"vrBehaviorVersion\"]\n        b2reg = self.make_b2registration(record, opts)\n        try:\n            print(f\"Performing preprocessing for session: {b2reg.session_print()}\")\n            b2reg.do_preprocessing()\n            print(f\"Saving params...\")\n            b2reg.save_session_prms()\n        except Exception as ex:\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), True)\n                cursor.execute(\n                    self.create_update_statement(\"vrRegistrationException\", record[self.uid]),\n                    str(ex),\n                )\n            if raise_exception:\n                raise ex\n            print(f\"The following exception was raised when trying to preprocess session: {b2reg.session_print()}. Clearing all oneData.\")\n            b2reg.clear_one_data(certainty=True)\n            error_print(f\"Last traceback: {traceback.extract_tb(ex.__traceback__, limit=-1)}\")\n            error_print(f\"Exception: {ex}\")\n            # If failed, return (False, 0)\n            out = (False, 0)\n        else:\n            with self.open_cursor(commit_changes=True) as cursor:\n                # Tell the database that vrRegistration was performed and the time of processing\n                cursor.execute(self.create_update_statement(\"vrRegistration\", record[self.uid]), True)\n                cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), False)\n                cursor.execute(self.create_update_statement(\"vrRegistrationException\", record[self.uid]), \"\")\n                cursor.execute(\n                    self.create_update_statement(\"vrRegistrationDate\", record[self.uid]),\n                    datetime.now(),\n                )\n            # If successful, return (True, size of registered oneData)\n            out = (True, sum([one_file.stat().st_size for one_file in b2reg.get_saved_one()]))\n            print(f\"Session {b2reg.session_print()} registered with {readable_bytes(out[1])} oneData.\")\n        finally:\n            del b2reg\n        return out\n\n    def register_single_session(\n        self,\n        mouse_name: str,\n        session_date: str,\n        session_id: Union[str, int],\n        raise_exception: bool = False,\n        imaging: Optional[bool] = None,\n    ) -&gt; Optional[bool]:\n        \"\"\"\n        Register a single session by its identifiers.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Mouse name identifier.\n        session_date : str\n            Session date in 'YYYY-MM-DD' format.\n        session_id : str or int\n            Session ID.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting. If None (default), uses the value from the\n            database record. If True or False, overrides the database value to enable\n            or disable imaging processing during registration.\n\n        Returns\n        -------\n        bool or None\n            True if registration succeeded, False if failed, None if session not found.\n        \"\"\"\n        record = self.get_record(mouse_name, session_date, session_id)\n        if record is None:\n            print(f\"Session {'/'.join([mouse_name, session_date, session_id])} is not in the database\")\n            return\n        out = self.register_record(record, raise_exception=raise_exception, imaging=imaging)\n        return out[0]\n\n    def register_sessions(\n        self, max_data: float = 30e9, skip_errors: bool = True, raise_exception: bool = False, imaging: Optional[bool] = None\n    ) -&gt; None:\n        \"\"\"\n        Register multiple sessions that need registration.\n\n        Processes sessions in batches, stopping when the total data size limit is reached.\n        Provides progress updates including accumulated data size and estimates.\n\n        Parameters\n        ----------\n        max_data : float, default=30e9\n            Maximum total data size (in bytes) to process before stopping.\n            Default is 30 GB.\n        skip_errors : bool, default=True\n            If True, skip sessions that had previous registration errors.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting for all sessions. If None (default), uses the\n            value from each session's database record. If True or False, overrides the\n            database value for all sessions to enable or disable imaging processing.\n\n        Notes\n        -----\n        Prints progress information including:\n        - Accumulated oneData registered\n        - Average data size per session\n        - Estimated remaining data to process\n        \"\"\"\n        count_sessions = 0\n        total_one_data = 0.0\n        df_to_register = self.needs_registration(skip_errors=skip_errors)\n\n        for idx, (_, row) in enumerate(df_to_register.iterrows()):\n            if total_one_data &gt; max_data:\n                print(f\"\\nMax data limit reached. Total processed: {readable_bytes(total_one_data)}. Limit: {readable_bytes(max_data)}\")\n                return\n            print(\"\")\n            out = self.register_record(row, raise_exception=raise_exception, imaging=imaging)\n            if out[0]:\n                count_sessions += 1  # count successful sessions\n                total_one_data += out[1]  # accumulated oneData registered\n                estimate_remaining = len(df_to_register) - idx - 1\n                print(\n                    f\"Accumulated oneData registered: {readable_bytes(total_one_data)}. \"\n                    f\"Averaging: {readable_bytes(total_one_data/count_sessions)} / session. \"\n                    f\"Estimate remaining: {readable_bytes(total_one_data/count_sessions*estimate_remaining)}\"\n                )\n\n    def print_registration_errors(self, **kw_conditions: Any) -&gt; None:\n        \"\"\"\n        Print registration errors for sessions that failed registration.\n\n        Parameters\n        ----------\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        for idx, row in df[df[\"vrRegistrationError\"] == True].iterrows():\n            print(f\"{'/'.join(self.session_name(row))} had error: {row['vrRegistrationException']}\")\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.check_s2p","title":"<code>check_s2p(with_database_update=False, return_check=False)</code>","text":"<p>Verify suite2p status consistency between database and file system.</p> <p>Checks for discrepancies where: - Database says suite2p is done but files don't exist - Files exist but database says suite2p wasn't done</p> <p>Parameters:</p> Name Type Description Default <code>with_database_update</code> <code>bool</code> <p>If True, automatically corrects database entries when discrepancies are found.</p> <code>False</code> <code>return_check</code> <code>bool</code> <p>If True, returns a boolean indicating whether any discrepancies were found.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool or None</code> <p>If return_check is True, returns True if any discrepancies were found, False otherwise. Returns None if return_check is False.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def check_s2p(self, with_database_update: bool = False, return_check: bool = False) -&gt; Optional[bool]:\n    \"\"\"\n    Verify suite2p status consistency between database and file system.\n\n    Checks for discrepancies where:\n    - Database says suite2p is done but files don't exist\n    - Files exist but database says suite2p wasn't done\n\n    Parameters\n    ----------\n    with_database_update : bool, default=False\n        If True, automatically corrects database entries when discrepancies are found.\n    return_check : bool, default=False\n        If True, returns a boolean indicating whether any discrepancies were found.\n\n    Returns\n    -------\n    bool or None\n        If return_check is True, returns True if any discrepancies were found,\n        False otherwise. Returns None if return_check is False.\n    \"\"\"\n    df = self.get_table()\n\n    # Check sessions where database says suite2p is done but files don't exist\n    check_s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n    checked_not_done = check_s2p_done.apply(lambda row: not (self.make_b2session(row).s2p_path.exists()), axis=1)\n    not_actually_done = check_s2p_done[checked_not_done]\n\n    # Check sessions where files exist but database says suite2p wasn't done\n    check_s2p_needed = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n    checked_not_needed = check_s2p_needed.apply(lambda row: self.make_b2session(row).s2p_path.exists(), axis=1)\n    not_actually_needed = check_s2p_needed[checked_not_needed]\n\n    # Print database errors to workspace\n    for idx, row in not_actually_done.iterrows():\n        print(f\"Database said suite2p has been ran, but it actually hasn't: {self.make_b2session(row).session_print()}\")\n    for idx, row in not_actually_needed.iterrows():\n        print(f\"Database said suite2p didn't run, but it already did: {self.make_b2session(row).session_print()}\")\n\n    # If with_database_update is True, correct the database\n    if with_database_update:\n        for idx, row in not_actually_done.iterrows():\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), False)\n\n        for idx, row in not_actually_needed.iterrows():\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), True)\n\n    # If return_check is requested, return True if any records were invalid\n    if return_check:\n        return checked_not_done.any() or checked_not_needed.any()\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.iter_sessions","title":"<code>iter_sessions(session_params={}, **kw_conditions)</code>","text":"<p>Iterate over sessions matching conditions.</p> <p>Parameters:</p> Name Type Description Default <code>session_params</code> <code>dict</code> <p>Additional parameters to pass to the session constructor when creating B2Session objects. These are passed through to create_b2session().</p> <code>{}</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax. Examples: <code>mouseName='ATL028'</code>, <code>imaging=True</code>, <code>sessionID=(5, '&gt;')</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>sessions</code> <code>list[B2Session]</code> <p>List of sessions matching the conditions.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def iter_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; List[B2Session]:\n    \"\"\"Iterate over sessions matching conditions.\n\n    Parameters\n    ----------\n    session_params : dict, default={}\n        Additional parameters to pass to the session constructor when creating\n        B2Session objects. These are passed through to create_b2session().\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n        Examples: ``mouseName='ATL028'``, ``imaging=True``, ``sessionID=(5, '&gt;')``\n\n    Returns\n    -------\n    sessions : list[B2Session]\n        List of sessions matching the conditions.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    sessions = []\n    for _, row in df.iterrows():\n        sessions.append(create_b2session(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params))\n    return sessions\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.iter_sessions_need_red_cell_qc","title":"<code>iter_sessions_need_red_cell_qc(**kw_conditions)</code>","text":"<p>Get list of sessions that require red cell quality control.</p> <p>Parameters:</p> Name Type Description Default <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[B2Session]</code> <p>List of session objects that need red cell QC.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def iter_sessions_need_red_cell_qc(self, **kw_conditions: Any) -&gt; List[B2Session]:\n    \"\"\"\n    Get list of sessions that require red cell quality control.\n\n    Parameters\n    ----------\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    list[B2Session]\n        List of session objects that need red cell QC.\n    \"\"\"\n    df = self.needs_red_cell_qc(return_df=True, **kw_conditions)\n    sessions = []\n    for idx, row in df.iterrows():\n        sessions.append(self.make_b2session(row))\n    return sessions\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.make_b2registration","title":"<code>make_b2registration(row, opts)</code>","text":"<p>Create a B2Registration object from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <code>opts</code> <code>B2RegistrationOpts</code> <p>Registration options to use for the session.</p> required <p>Returns:</p> Type Description <code>B2Registration</code> <p>Registration object initialized with session data and options.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def make_b2registration(self, row: pd.Series, opts: B2RegistrationOpts) -&gt; B2Registration:\n    \"\"\"\n    Create a B2Registration object from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n    opts : B2RegistrationOpts\n        Registration options to use for the session.\n\n    Returns\n    -------\n    B2Registration\n        Registration object initialized with session data and options.\n    \"\"\"\n    mouse_name, session_date, session_id = self.session_name(row)\n    return B2Registration(mouse_name, session_date, session_id, opts)\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.make_b2session","title":"<code>make_b2session(row)</code>","text":"<p>Create a B2Session object from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <p>Returns:</p> Type Description <code>B2Session</code> <p>Session object initialized with data from the record.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def make_b2session(self, row: pd.Series) -&gt; B2Session:\n    \"\"\"\n    Create a B2Session object from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n\n    Returns\n    -------\n    B2Session\n        Session object initialized with data from the record.\n    \"\"\"\n    mouse_name, session_date, session_id = self.session_name(row)\n    return create_b2session(mouse_name, session_date, session_id)\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.needs_red_cell_qc","title":"<code>needs_red_cell_qc(return_df=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need red cell quality control.</p> <p>Parameters:</p> Name Type Description Default <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need red cell QC. Sessions must have imaging, suite2p processing, and registration completed. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def needs_red_cell_qc(self, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need red cell quality control.\n\n    Parameters\n    ----------\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need red cell QC.\n        Sessions must have imaging, suite2p processing, and registration completed.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"vrRegistration\"] == True) &amp; (df[\"redCellQC\"] == False)]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            print(f\"Database indicates that redCellQC has not been performed for session: {self.make_b2session(row).session_print()}\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.needs_registration","title":"<code>needs_registration(skip_errors=True, return_df=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need registration preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>skip_errors</code> <code>bool</code> <p>If True, exclude sessions that had registration errors.</p> <code>True</code> <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need registration. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def needs_registration(self, skip_errors: bool = True, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need registration preprocessing.\n\n    Parameters\n    ----------\n    skip_errors : bool, default=True\n        If True, exclude sessions that had registration errors.\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need registration.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    if skip_errors:\n        df = df[df[\"vrRegistrationError\"] == False]\n    df = df[df[\"vrRegistration\"] == False]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            session = self.make_b2session(row)\n            print(f\"Session needs registration: {session.session_print()}\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.needs_s2p","title":"<code>needs_s2p(needs_qc=False, return_df=True, print_targets=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need suite2p processing or quality control.</p> <p>Parameters:</p> Name Type Description Default <code>needs_qc</code> <code>bool</code> <p>If False, returns/prints sessions that need suite2p processing. If True, returns/prints sessions that need suite2p quality control.</p> <code>False</code> <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>print_targets</code> <code>bool</code> <p>If True and return_df=False, prints suite2p target information for sessions needing processing.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def needs_s2p(\n    self, needs_qc: bool = False, return_df: bool = True, print_targets: bool = True, **kw_conditions: Any\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need suite2p processing or quality control.\n\n    Parameters\n    ----------\n    needs_qc : bool, default=False\n        If False, returns/prints sessions that need suite2p processing.\n        If True, returns/prints sessions that need suite2p quality control.\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    print_targets : bool, default=True\n        If True and return_df=False, prints suite2p target information for sessions needing processing.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    if needs_qc:\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"suite2pQC\"] == False)]\n    else:\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            session = self.make_b2session(row)\n            if needs_qc:\n                print(f\"Database indicates that suite2p has been run but not QC'd: {session.session_print()}\")\n            else:\n                print(f\"Database indicates that suite2p has not been run: {session.session_print()}\")\n                if print_targets:\n                    mouse_name, session_date, session_id = self.session_name(row)\n                    s2p_targets(mouse_name, session_date, session_id)\n                    print(\"\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.print_registration_errors","title":"<code>print_registration_errors(**kw_conditions)</code>","text":"<p>Print registration errors for sessions that failed registration.</p> <p>Parameters:</p> Name Type Description Default <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def print_registration_errors(self, **kw_conditions: Any) -&gt; None:\n    \"\"\"\n    Print registration errors for sessions that failed registration.\n\n    Parameters\n    ----------\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    for idx, row in df[df[\"vrRegistrationError\"] == True].iterrows():\n        print(f\"{'/'.join(self.session_name(row))} had error: {row['vrRegistrationException']}\")\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.register_record","title":"<code>register_record(record, raise_exception=False, imaging=None)</code>","text":"<p>Perform registration preprocessing for a single session record.</p> <p>Creates a B2Registration object and runs preprocessing. Updates the database with success/failure status and error information if applicable.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Series</code> <p>Database record containing session information.</p> required <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting. If None (default), uses the value from the database record. If True or False, overrides the database value.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (success, data_size) where: - success: bool indicating if registration succeeded - data_size: int size in bytes of registered oneData (0 if failed)</p> Notes <p>On failure, clears all oneData files and updates database error fields. On success, updates registration status and date in the database.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def register_record(self, record: pd.Series, raise_exception: bool = False, imaging: Optional[bool] = None) -&gt; Tuple[bool, int]:\n    \"\"\"\n    Perform registration preprocessing for a single session record.\n\n    Creates a B2Registration object and runs preprocessing. Updates the database\n    with success/failure status and error information if applicable.\n\n    Parameters\n    ----------\n    record : pandas.Series\n        Database record containing session information.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting. If None (default), uses the value from the\n        database record. If True or False, overrides the database value.\n\n    Returns\n    -------\n    tuple\n        Tuple of (success, data_size) where:\n        - success: bool indicating if registration succeeded\n        - data_size: int size in bytes of registered oneData (0 if failed)\n\n    Notes\n    -----\n    On failure, clears all oneData files and updates database error fields.\n    On success, updates registration status and date in the database.\n    \"\"\"\n    opts = B2RegistrationOpts()\n    opts.imaging = bool(imaging) if imaging is not None else bool(record[\"imaging\"])\n    opts.facecam = bool(record[\"faceCamera\"])\n    opts.vrBehaviorVersion = record[\"vrBehaviorVersion\"]\n    b2reg = self.make_b2registration(record, opts)\n    try:\n        print(f\"Performing preprocessing for session: {b2reg.session_print()}\")\n        b2reg.do_preprocessing()\n        print(f\"Saving params...\")\n        b2reg.save_session_prms()\n    except Exception as ex:\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), True)\n            cursor.execute(\n                self.create_update_statement(\"vrRegistrationException\", record[self.uid]),\n                str(ex),\n            )\n        if raise_exception:\n            raise ex\n        print(f\"The following exception was raised when trying to preprocess session: {b2reg.session_print()}. Clearing all oneData.\")\n        b2reg.clear_one_data(certainty=True)\n        error_print(f\"Last traceback: {traceback.extract_tb(ex.__traceback__, limit=-1)}\")\n        error_print(f\"Exception: {ex}\")\n        # If failed, return (False, 0)\n        out = (False, 0)\n    else:\n        with self.open_cursor(commit_changes=True) as cursor:\n            # Tell the database that vrRegistration was performed and the time of processing\n            cursor.execute(self.create_update_statement(\"vrRegistration\", record[self.uid]), True)\n            cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), False)\n            cursor.execute(self.create_update_statement(\"vrRegistrationException\", record[self.uid]), \"\")\n            cursor.execute(\n                self.create_update_statement(\"vrRegistrationDate\", record[self.uid]),\n                datetime.now(),\n            )\n        # If successful, return (True, size of registered oneData)\n        out = (True, sum([one_file.stat().st_size for one_file in b2reg.get_saved_one()]))\n        print(f\"Session {b2reg.session_print()} registered with {readable_bytes(out[1])} oneData.\")\n    finally:\n        del b2reg\n    return out\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.register_sessions","title":"<code>register_sessions(max_data=30000000000.0, skip_errors=True, raise_exception=False, imaging=None)</code>","text":"<p>Register multiple sessions that need registration.</p> <p>Processes sessions in batches, stopping when the total data size limit is reached. Provides progress updates including accumulated data size and estimates.</p> <p>Parameters:</p> Name Type Description Default <code>max_data</code> <code>float</code> <p>Maximum total data size (in bytes) to process before stopping. Default is 30 GB.</p> <code>30e9</code> <code>skip_errors</code> <code>bool</code> <p>If True, skip sessions that had previous registration errors.</p> <code>True</code> <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting for all sessions. If None (default), uses the value from each session's database record. If True or False, overrides the database value for all sessions to enable or disable imaging processing.</p> <code>None</code> Notes <p>Prints progress information including: - Accumulated oneData registered - Average data size per session - Estimated remaining data to process</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def register_sessions(\n    self, max_data: float = 30e9, skip_errors: bool = True, raise_exception: bool = False, imaging: Optional[bool] = None\n) -&gt; None:\n    \"\"\"\n    Register multiple sessions that need registration.\n\n    Processes sessions in batches, stopping when the total data size limit is reached.\n    Provides progress updates including accumulated data size and estimates.\n\n    Parameters\n    ----------\n    max_data : float, default=30e9\n        Maximum total data size (in bytes) to process before stopping.\n        Default is 30 GB.\n    skip_errors : bool, default=True\n        If True, skip sessions that had previous registration errors.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting for all sessions. If None (default), uses the\n        value from each session's database record. If True or False, overrides the\n        database value for all sessions to enable or disable imaging processing.\n\n    Notes\n    -----\n    Prints progress information including:\n    - Accumulated oneData registered\n    - Average data size per session\n    - Estimated remaining data to process\n    \"\"\"\n    count_sessions = 0\n    total_one_data = 0.0\n    df_to_register = self.needs_registration(skip_errors=skip_errors)\n\n    for idx, (_, row) in enumerate(df_to_register.iterrows()):\n        if total_one_data &gt; max_data:\n            print(f\"\\nMax data limit reached. Total processed: {readable_bytes(total_one_data)}. Limit: {readable_bytes(max_data)}\")\n            return\n        print(\"\")\n        out = self.register_record(row, raise_exception=raise_exception, imaging=imaging)\n        if out[0]:\n            count_sessions += 1  # count successful sessions\n            total_one_data += out[1]  # accumulated oneData registered\n            estimate_remaining = len(df_to_register) - idx - 1\n            print(\n                f\"Accumulated oneData registered: {readable_bytes(total_one_data)}. \"\n                f\"Averaging: {readable_bytes(total_one_data/count_sessions)} / session. \"\n                f\"Estimate remaining: {readable_bytes(total_one_data/count_sessions*estimate_remaining)}\"\n            )\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.register_single_session","title":"<code>register_single_session(mouse_name, session_date, session_id, raise_exception=False, imaging=None)</code>","text":"<p>Register a single session by its identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Mouse name identifier.</p> required <code>session_date</code> <code>str</code> <p>Session date in 'YYYY-MM-DD' format.</p> required <code>session_id</code> <code>str or int</code> <p>Session ID.</p> required <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting. If None (default), uses the value from the database record. If True or False, overrides the database value to enable or disable imaging processing during registration.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool or None</code> <p>True if registration succeeded, False if failed, None if session not found.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def register_single_session(\n    self,\n    mouse_name: str,\n    session_date: str,\n    session_id: Union[str, int],\n    raise_exception: bool = False,\n    imaging: Optional[bool] = None,\n) -&gt; Optional[bool]:\n    \"\"\"\n    Register a single session by its identifiers.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Mouse name identifier.\n    session_date : str\n        Session date in 'YYYY-MM-DD' format.\n    session_id : str or int\n        Session ID.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting. If None (default), uses the value from the\n        database record. If True or False, overrides the database value to enable\n        or disable imaging processing during registration.\n\n    Returns\n    -------\n    bool or None\n        True if registration succeeded, False if failed, None if session not found.\n    \"\"\"\n    record = self.get_record(mouse_name, session_date, session_id)\n    if record is None:\n        print(f\"Session {'/'.join([mouse_name, session_date, session_id])} is not in the database\")\n        return\n    out = self.register_record(record, raise_exception=raise_exception, imaging=imaging)\n    return out[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.session_name","title":"<code>session_name(row)</code>","text":"<p>Extract session identifiers from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (mouse_name, session_date, session_id) where session_date is formatted as 'YYYY-MM-DD' and session_id is converted to string.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def session_name(self, row: pd.Series) -&gt; Tuple[str, str, str]:\n    \"\"\"\n    Extract session identifiers from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n\n    Returns\n    -------\n    tuple\n        Tuple of (mouse_name, session_date, session_id) where session_date is\n        formatted as 'YYYY-MM-DD' and session_id is converted to string.\n    \"\"\"\n    mouse_name = row[\"mouseName\"]\n    session_date = row[\"sessionDate\"].strftime(\"%Y-%m-%d\")\n    session_id = str(row[\"sessionID\"])\n    return mouse_name, session_date, session_id\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.set_red_cell_qc","title":"<code>set_red_cell_qc(mouse_name, date_string, session_id, state=True)</code>","text":"<p>Set the red cell QC status for a specific session.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Mouse name identifier.</p> required <code>date_string</code> <code>str</code> <p>Session date in 'YYYY-MM-DD' format.</p> required <code>session_id</code> <code>str or int</code> <p>Session ID.</p> required <code>state</code> <code>bool</code> <p>Red cell QC status to set. If True, also sets the QC date to now.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful, False otherwise.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def set_red_cell_qc(self, mouse_name: str, date_string: str, session_id: Union[str, int], state: bool = True) -&gt; bool:\n    \"\"\"\n    Set the red cell QC status for a specific session.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Mouse name identifier.\n    date_string : str\n        Session date in 'YYYY-MM-DD' format.\n    session_id : str or int\n        Session ID.\n    state : bool, default=True\n        Red cell QC status to set. If True, also sets the QC date to now.\n\n    Returns\n    -------\n    bool\n        True if update was successful, False otherwise.\n    \"\"\"\n    record = self.get_record(mouse_name, date_string, session_id)\n    if record is None:\n        print(f\"Could not find session {mouse_name}/{date_string}/{session_id} in database.\")\n        return False\n\n    try:\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(self.create_update_statement(\"redCellQC\", record[self.uid]), state)\n            if state == True:\n                # If setting red cell QC to true, add the date\n                cursor.execute(\n                    self.create_update_statement(\"redCellQCDate\", record[self.uid]),\n                    datetime.now(),\n                )\n            else:\n                # Otherwise remove the date\n                cursor.execute(self.create_update_statement(\"redCellQCDate\", record[self.uid]), \"\")\n        return True\n\n    except Exception as ex:\n        print(f\"Failed to update database for session: {mouse_name}/{date_string}/{session_id}\")\n        print(f\"Error: {ex}\")\n        return False\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.update_red_cell_qc_date_time","title":"<code>update_red_cell_qc_date_time()</code>","text":"<p>Update red cell QC dates in the database based on file modification times.</p> <p>For all sessions where red cell QC is complete, finds the most recent modification time of relevant red cell QC files and updates the redCellQCDate field in the database.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def update_red_cell_qc_date_time(self) -&gt; None:\n    \"\"\"\n    Update red cell QC dates in the database based on file modification times.\n\n    For all sessions where red cell QC is complete, finds the most recent\n    modification time of relevant red cell QC files and updates the\n    redCellQCDate field in the database.\n    \"\"\"\n    relevant_one_files = [\n        \"mpciROIs.redCellIdx.npy\",\n        \"mpciROIs.redCellManualAssignment.npy\",\n        \"parametersRed*\",  # wildcard because there are multiple possibilities\n    ]\n\n    df = self.get_table()\n    red_cell_qc_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"redCellQC\"] == True)]\n    uids = red_cell_qc_done[self.uid].tolist()\n    rc_edit_date = []\n    for idx, row in red_cell_qc_done.iterrows():\n        session = self.make_b2session(row)  # create vrSession to point to session folder\n        c_latest_mod = 0\n        for f in relevant_one_files:\n            for file in session.one_path.rglob(f):\n                c_latest_mod = max(file.stat().st_mtime, c_latest_mod)\n        c_date_time = datetime.fromtimestamp(c_latest_mod)\n        rc_edit_date.append(c_date_time)  # get red cell QC file modification date\n\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(self.create_update_many_statement(\"redCellQCDate\"), zip(rc_edit_date, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.SessionDatabase.update_s2p_date_time","title":"<code>update_s2p_date_time()</code>","text":"<p>Update suite2p creation dates in the database based on file modification times.</p> <p>For all sessions where suite2p processing is complete, finds the most recent file modification time in the suite2p output directory and updates the suite2pDate field in the database.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def update_s2p_date_time(self) -&gt; None:\n    \"\"\"\n    Update suite2p creation dates in the database based on file modification times.\n\n    For all sessions where suite2p processing is complete, finds the most recent\n    file modification time in the suite2p output directory and updates the\n    suite2pDate field in the database.\n    \"\"\"\n    df = self.get_table()\n    s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n    uids = s2p_done[self.uid].tolist()\n    s2p_creation_date = []\n    for idx, row in s2p_done.iterrows():\n        session = self.make_b2session(row)  # create vrSession to point to session folder\n        c_latest_mod = 0\n        for p in session.s2p_path.rglob(\"*\"):\n            if not (p.is_dir()):\n                c_latest_mod = max(p.stat().st_mtime, c_latest_mod)\n        c_date_time = datetime.fromtimestamp(c_latest_mod)\n        s2p_creation_date.append(c_date_time)  # get suite2p path creation date\n\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(self.create_update_many_statement(\"suite2pDate\"), zip(s2p_creation_date, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.get_database","title":"<code>get_database(db_name)</code>","text":"<p>Retrieve an appropriate database object instance.</p> <p>This function retrieves metadata for the specified database and instantiates the appropriate database class (BaseDatabase or a subclass like SessionDatabase).</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database to retrieve.</p> required <p>Returns:</p> Type Description <code>BaseDatabase or SessionDatabase</code> <p>An instance of the appropriate database class as specified in the metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the constructor specified in metadata is not a subclass of BaseDatabase.</p> See Also <p>get_database_metadata : Retrieve metadata for a database.</p> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def get_database(db_name: str) -&gt; Union[\"BaseDatabase\", \"SessionDatabase\"]:\n    \"\"\"\n    Retrieve an appropriate database object instance.\n\n    This function retrieves metadata for the specified database and instantiates\n    the appropriate database class (BaseDatabase or a subclass like SessionDatabase).\n\n    Parameters\n    ----------\n    db_name : str\n        The name of the database to retrieve.\n\n    Returns\n    -------\n    BaseDatabase or SessionDatabase\n        An instance of the appropriate database class as specified in the metadata.\n\n    Raises\n    ------\n    ValueError\n        If the constructor specified in metadata is not a subclass of BaseDatabase.\n\n    See Also\n    --------\n    get_database_metadata : Retrieve metadata for a database.\n    \"\"\"\n    metadata = get_database_metadata(db_name)\n    if \"constructor\" in metadata:\n        if issubclass(metadata[\"constructor\"], BaseDatabase):\n            constructor = metadata[\"constructor\"]  # get class constructor method for this database\n        else:\n            raise ValueError(f\"{metadata['constructor']} must be a subclass of the `BaseDatabase` class!\")\n    else:\n        constructor = BaseDatabase\n    return constructor(db_name)\n</code></pre>"},{"location":"api/database/#vrAnalysis2.database.get_database_metadata","title":"<code>get_database_metadata(db_name)</code>","text":"<p>Retrieve metadata for a specified database.</p> <p>This function retrieves metadata for a specified database from the <code>dbdict</code> dictionary. The <code>dbdict</code> dictionary contains the database paths, names, and primary table name.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database for which to retrieve metadata.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing metadata for the specified database. It requires the following keys:     'db_path': path to the database file     'db_name': name of the database file     'db_ext': extension of the database file     'table_name': name of the table to use     'uid': name of the field defining a unique ID for each row in the table     'backup_path': path to the database backup (None if there isn't one)     'unique_fields': list of names of fields for which there should only be one                     database row per combination of the values in unique_fields                     note: assumes string, but make it a tuple for different types     'default_conditions': dictionary containing key-value pairs of any default                          conditions to filter by when retrieving table data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>db_name</code> is not recognized as a valid database name.</p> Example <p>metadata = get_database_metadata('vrSessions') print(metadata['db_path']) 'C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement' print(metadata['db_name']) 'vrDatabase'</p> Notes <ul> <li>The <code>dbdict</code> dictionary contains metadata for recognized databases.</li> <li>Edit this function to specify the path, database, and primary table on your computer.</li> </ul> Source code in <code>vrAnalysis2/database.py</code> <pre><code>def get_database_metadata(db_name: str) -&gt; dict:\n    \"\"\"\n    Retrieve metadata for a specified database.\n\n    This function retrieves metadata for a specified database from the `dbdict` dictionary.\n    The `dbdict` dictionary contains the database paths, names, and primary table name.\n\n    Parameters\n    ----------\n    db_name : str\n        The name of the database for which to retrieve metadata.\n\n    Returns\n    -------\n    dict\n        A dictionary containing metadata for the specified database.\n        It requires the following keys:\n            'db_path': path to the database file\n            'db_name': name of the database file\n            'db_ext': extension of the database file\n            'table_name': name of the table to use\n            'uid': name of the field defining a unique ID for each row in the table\n            'backup_path': path to the database backup (None if there isn't one)\n            'unique_fields': list of names of fields for which there should only be one\n                            database row per combination of the values in unique_fields\n                            note: assumes string, but make it a tuple for different types\n            'default_conditions': dictionary containing key-value pairs of any default\n                                 conditions to filter by when retrieving table data\n\n    Raises\n    ------\n    ValueError\n        If the provided `db_name` is not recognized as a valid database name.\n\n    Example\n    -------\n    &gt;&gt;&gt; metadata = get_database_metadata('vrSessions')\n    &gt;&gt;&gt; print(metadata['db_path'])\n    'C:\\\\Users\\\\andrew\\\\Documents\\\\localData\\\\vrDatabaseManagement'\n    &gt;&gt;&gt; print(metadata['db_name'])\n    'vrDatabase'\n\n    Notes\n    -----\n    - The `dbdict` dictionary contains metadata for recognized databases.\n    - Edit this function to specify the path, database, and primary table on your computer.\n    \"\"\"\n\n    dbdict = {\n        \"vrSessions\": {\n            \"db_path\": r\"C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"sessiondb\",\n            \"uid\": \"uSessionID\",\n            \"backup_path\": r\"D:\\localData\\vrDatabaseManagement\",\n            \"unique_fields\": [(\"mouseName\", str), (\"sessionDate\", datetime), (\"sessionID\", int)],\n            \"default_conditions\": {\n                \"sessionQC\": True,\n            },\n            \"constructor\": SessionDatabase,\n        },\n        \"vrMice\": {\n            \"db_path\": r\"C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"mousedb\",\n            \"uid\": \"uMouseID\",\n            \"backup_path\": r\"D:\\localData\\vrDatabaseManagement\",\n            \"unique_fields\": [(\"mouseName\", str)],\n            \"default_conditions\": {},\n            \"constructor\": BaseDatabase,\n        },\n    }\n    if db_name not in dbdict.keys():\n        raise ValueError(f\"Did not recognize database={db_name}, valid database names are: {[key for key in dbdict.keys()]}\")\n    return dbdict[db_name]\n</code></pre>"},{"location":"api/processors/","title":"Processors API Reference","text":""},{"location":"api/processors/#vrAnalysis2.processors","title":"<code>processors</code>","text":"<p>Processors module for vrAnalysis2.</p> <p>This module provides data processing pipelines that transform session data into analysis-ready formats.</p>"},{"location":"api/processors/#vrAnalysis2.processors.Maps","title":"<code>Maps</code>  <code>dataclass</code>","text":"<p>Base class for occupancy, speed, and spike maps.</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@dataclass\nclass Maps:\n    \"\"\"Base class for occupancy, speed, and spike maps.\"\"\"\n\n    occmap: np.ndarray | list[np.ndarray]\n    speedmap: np.ndarray | list[np.ndarray]\n    spkmap: np.ndarray | list[np.ndarray]\n    by_environment: bool\n    rois_first: bool\n    environments: list[int] | None = None\n    distcenters: np.ndarray | None = None\n    _averaged: bool = field(default=False, init=False)\n\n    def __post_init__(self):\n        if self.occmap is None or self.speedmap is None or self.spkmap is None:\n            raise ValueError(\"occmap, speedmap, and spkmap must be provided\")\n\n        if self.by_environment:\n            if self.environments is None:\n                raise ValueError(\"environments must be provided if by_environment is True\")\n            if not isinstance(self.occmap, list) or not isinstance(self.speedmap, list) or not isinstance(self.spkmap, list):\n                raise ValueError(\"occmap, speedmap, and spkmap must be lists if by_environment is True\")\n        else:\n            if isinstance(self.occmap, list) or isinstance(self.speedmap, list) or isinstance(self.spkmap, list):\n                raise ValueError(\"occmap, speedmap, and spkmap must be single arrays if by_environment is False\")\n\n        if not self.by_environment:\n            spkmap_shape = self.spkmap.shape[1:] if self.rois_first else self.spkmap.shape[:2]\n            if not (self.occmap.shape == self.speedmap.shape == spkmap_shape):\n                raise ValueError(\"occmap, speedmap, and spkmap must have the same shape\")\n        else:\n            if not (len(self.occmap) == len(self.speedmap) == len(self.spkmap) == len(self.environments)):\n                raise ValueError(\"occmap, speedmap, and spkmap must have the same number of environments\")\n            for i in range(len(self.environments)):\n                spkmap_shape = self.spkmap[i].shape[1:] if self.rois_first else self.spkmap[i].shape[:2]\n                if not (self.occmap[i].shape == self.speedmap[i].shape == spkmap_shape):\n                    raise ValueError(\"occmap, speedmap, and spkmap must have the same shape for each environment\")\n            roi_axis = 0 if self.rois_first else -1\n            rois_per_env = [spkmap.shape[roi_axis] for spkmap in self.spkmap]\n            if not all([rpe == rois_per_env[0] for rpe in rois_per_env]):\n                raise ValueError(\"All environments must have the same number of ROIs\")\n\n    def __repr__(self) -&gt; str:\n        # Get number of positions\n        if self.by_environment:\n            num_positions = self.occmap[0].shape[-1]\n        else:\n            num_positions = self.occmap.shape[-1]\n        # Get number of trials\n        if self._averaged:\n            num_trials = \"averaged\"\n        else:\n            if self.by_environment:\n                num_trials = [occmap.shape[0] for occmap in self.occmap]\n                num_trials = \"{\" + \", \".join([str(nt) for nt in num_trials]) + \"}\"\n            else:\n                num_trials = self.occmap.shape[0]\n        # Get number of ROIs\n        if self.by_environment:\n            num_rois = self.spkmap[0].shape[0] if self.rois_first else self.spkmap[0].shape[1]\n        else:\n            num_rois = self.spkmap.shape[0] if self.rois_first else self.spkmap.shape[1]\n        environments = f\", environments={{{', '.join([str(env) for env in self.environments])}}}\" if self.by_environment else \"\"\n        return f\"Maps(num_trials={num_trials}, num_positions={num_positions}, num_rois={num_rois}{environments}, rois_first={self.rois_first})\"\n\n    @classmethod\n    def create_raw_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n        return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=False)\n\n    @classmethod\n    def create_processed_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n        return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=True)\n\n    @classmethod\n    def create_environment_maps(\n        cls, occmap: list[np.ndarray], speedmap: list[np.ndarray], spkmap: list[np.ndarray], environments: list[int], distcenters: np.ndarray = None\n    ) -&gt; \"Maps\":\n        return cls(\n            occmap=occmap,\n            speedmap=speedmap,\n            spkmap=spkmap,\n            distcenters=distcenters,\n            environments=environments,\n            by_environment=True,\n            rois_first=True,\n        )\n\n    @classmethod\n    def map_types(self) -&gt; List[str]:\n        return [\"occmap\", \"speedmap\", \"spkmap\"]\n\n    def __getitem__(self, key: str) -&gt; np.ndarray:\n        return getattr(self, key)\n\n    def __setitem__(self, key: str, value: np.ndarray):\n        setattr(self, key, value)\n\n    def _get_position_axis(self, mapname: str) -&gt; int:\n        \"\"\"The only time the position axis isn't the last one is for spkmap when rois_first is False\"\"\"\n        average_offset = -1 if self._averaged else 0\n        if mapname == \"spkmap\" and not self.rois_first:\n            return -2 + average_offset\n        else:\n            return -1\n\n    def filter_positions(self, idx_positions: np.ndarray) -&gt; None:\n        if self.distcenters is not None:\n            self.distcenters = self.distcenters[idx_positions]\n        for mapname in self.map_types():\n            axis = self._get_position_axis(mapname)\n            if self.by_environment:\n                self[mapname] = [np.take(x, idx_positions, axis=axis) for x in self[mapname]]\n            else:\n                self[mapname] = np.take(self[mapname], idx_positions, axis=axis)\n\n    def filter_rois(self, idx_rois: np.ndarray) -&gt; None:\n        axis = 0 if self.rois_first else -1\n        if self.by_environment:\n            self.spkmap = [np.take(x, idx_rois, axis=axis) for x in self.spkmap]\n        else:\n            self.spkmap = np.take(self.spkmap, idx_rois, axis=axis)\n\n    def filter_environments(self, environments: list[int]) -&gt; None:\n        if self.by_environment:\n            idx_to_requested_env = [i for i, env in enumerate(self.environments) if env in environments]\n            self.occmap = [self.occmap[i] for i in idx_to_requested_env]\n            self.speedmap = [self.speedmap[i] for i in idx_to_requested_env]\n            self.spkmap = [self.spkmap[i] for i in idx_to_requested_env]\n            self.environments = [self.environments[i] for i in idx_to_requested_env]\n        else:\n            raise ValueError(\"Cannot filter environments when maps aren't separated by environment!\")\n\n    def pop_nan_positions(self) -&gt; None:\n        \"\"\"Remove positions with nans from the maps\"\"\"\n        if self.by_environment:\n            idx_valid_positions = np.where(~np.any(np.stack([np.any(np.isnan(occmap), axis=0) for occmap in self.occmap], axis=0), axis=0))[0]\n        else:\n            idx_valid_positions = np.where(~np.any(np.isnan(self.occmap), axis=0))[0]\n        self.filter_positions(idx_valid_positions)\n\n    def smooth_maps(self, positions: np.ndarray, kernel_width: float) -&gt; None:\n        \"\"\"Smooth the maps using a Gaussian kernel\"\"\"\n        kernel = get_gauss_kernel(positions, kernel_width)\n\n        # Replace nans with 0s\n        if self.by_environment:\n            idxnan = [np.isnan(occmap) for occmap in self.occmap]\n        else:\n            idxnan = np.isnan(self.occmap)\n\n        if self.rois_first:\n            # Move the rois axis to the last axis\n            if self.by_environment:\n                self.spkmap = [np.moveaxis(map, 0, -1) for map in self.spkmap]\n            else:\n                self.spkmap = np.moveaxis(self.spkmap, 0, -1)\n\n        for mapname in self.map_types():\n            if self.by_environment:\n                for ienv, inanenv in enumerate(idxnan):\n                    self[mapname][ienv][inanenv] = 0\n            else:\n                self[mapname][idxnan] = 0\n\n        for mapname in self.map_types():\n            # Since we moved ROIs to the last axis position will be axis=1 for all map types\n            if self.by_environment:\n                self[mapname] = [convolve_toeplitz(map, kernel, axis=1) for map in self[mapname]]\n            else:\n                self[mapname] = convolve_toeplitz(self[mapname], kernel, axis=1)\n\n        # Put nans back in place\n        for mapname in self.map_types():\n            if self.by_environment:\n                for ienv, inanenv in enumerate(idxnan):\n                    self[mapname][ienv][inanenv] = np.nan\n            else:\n                self[mapname][idxnan] = np.nan\n\n        # Move the rois axis back to the first axis\n        if self.rois_first:\n            if self.by_environment:\n                self.spkmap = [np.moveaxis(map, -1, 0) for map in self.spkmap]\n            else:\n                self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n\n    def average_trials(self, keepdims: bool = False) -&gt; None:\n        \"\"\"Average the trials within each environment\"\"\"\n        if self._averaged:\n            return\n        for mapname in self.map_types():\n            axis = 1 if mapname == \"spkmap\" and self.rois_first else 0\n            if self.by_environment:\n                self[mapname] = [ss.mean(map, axis=axis, keepdims=keepdims) for map in self[mapname]]\n            else:\n                self[mapname] = ss.mean(self[mapname], axis=axis, keepdims=keepdims)\n        self._averaged = True\n\n    def nbytes(self) -&gt; int:\n        num_bytes = 0\n        for name in self.map_types():\n            if self.by_environment:\n                num_bytes += sum(x.nbytes for x in getattr(self, name))\n            else:\n                num_bytes += getattr(self, name).nbytes\n        return num_bytes\n\n    def raw_to_processed(self, positions: np.ndarray, smooth_width: float | None = None) -&gt; \"Maps\":\n        \"\"\"Convert raw maps to processed maps\"\"\"\n        if smooth_width is not None:\n            self.smooth_maps(positions, smooth_width)\n\n        self.speedmap = correct_map(self.occmap, self.speedmap)\n        self.spkmap = correct_map(self.occmap, self.spkmap)\n\n        # Change spkmap to be ROIs first\n        self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n        self.rois_first = True\n\n        return self\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.Maps.average_trials","title":"<code>average_trials(keepdims=False)</code>","text":"<p>Average the trials within each environment</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def average_trials(self, keepdims: bool = False) -&gt; None:\n    \"\"\"Average the trials within each environment\"\"\"\n    if self._averaged:\n        return\n    for mapname in self.map_types():\n        axis = 1 if mapname == \"spkmap\" and self.rois_first else 0\n        if self.by_environment:\n            self[mapname] = [ss.mean(map, axis=axis, keepdims=keepdims) for map in self[mapname]]\n        else:\n            self[mapname] = ss.mean(self[mapname], axis=axis, keepdims=keepdims)\n    self._averaged = True\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.Maps.pop_nan_positions","title":"<code>pop_nan_positions()</code>","text":"<p>Remove positions with nans from the maps</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def pop_nan_positions(self) -&gt; None:\n    \"\"\"Remove positions with nans from the maps\"\"\"\n    if self.by_environment:\n        idx_valid_positions = np.where(~np.any(np.stack([np.any(np.isnan(occmap), axis=0) for occmap in self.occmap], axis=0), axis=0))[0]\n    else:\n        idx_valid_positions = np.where(~np.any(np.isnan(self.occmap), axis=0))[0]\n    self.filter_positions(idx_valid_positions)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.Maps.raw_to_processed","title":"<code>raw_to_processed(positions, smooth_width=None)</code>","text":"<p>Convert raw maps to processed maps</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def raw_to_processed(self, positions: np.ndarray, smooth_width: float | None = None) -&gt; \"Maps\":\n    \"\"\"Convert raw maps to processed maps\"\"\"\n    if smooth_width is not None:\n        self.smooth_maps(positions, smooth_width)\n\n    self.speedmap = correct_map(self.occmap, self.speedmap)\n    self.spkmap = correct_map(self.occmap, self.spkmap)\n\n    # Change spkmap to be ROIs first\n    self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n    self.rois_first = True\n\n    return self\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.Maps.smooth_maps","title":"<code>smooth_maps(positions, kernel_width)</code>","text":"<p>Smooth the maps using a Gaussian kernel</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def smooth_maps(self, positions: np.ndarray, kernel_width: float) -&gt; None:\n    \"\"\"Smooth the maps using a Gaussian kernel\"\"\"\n    kernel = get_gauss_kernel(positions, kernel_width)\n\n    # Replace nans with 0s\n    if self.by_environment:\n        idxnan = [np.isnan(occmap) for occmap in self.occmap]\n    else:\n        idxnan = np.isnan(self.occmap)\n\n    if self.rois_first:\n        # Move the rois axis to the last axis\n        if self.by_environment:\n            self.spkmap = [np.moveaxis(map, 0, -1) for map in self.spkmap]\n        else:\n            self.spkmap = np.moveaxis(self.spkmap, 0, -1)\n\n    for mapname in self.map_types():\n        if self.by_environment:\n            for ienv, inanenv in enumerate(idxnan):\n                self[mapname][ienv][inanenv] = 0\n        else:\n            self[mapname][idxnan] = 0\n\n    for mapname in self.map_types():\n        # Since we moved ROIs to the last axis position will be axis=1 for all map types\n        if self.by_environment:\n            self[mapname] = [convolve_toeplitz(map, kernel, axis=1) for map in self[mapname]]\n        else:\n            self[mapname] = convolve_toeplitz(self[mapname], kernel, axis=1)\n\n    # Put nans back in place\n    for mapname in self.map_types():\n        if self.by_environment:\n            for ienv, inanenv in enumerate(idxnan):\n                self[mapname][ienv][inanenv] = np.nan\n        else:\n            self[mapname][idxnan] = np.nan\n\n    # Move the rois axis back to the first axis\n    if self.rois_first:\n        if self.by_environment:\n            self.spkmap = [np.moveaxis(map, -1, 0) for map in self.spkmap]\n        else:\n            self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapParams","title":"<code>SpkmapParams</code>  <code>dataclass</code>","text":"<p>Parameters for spike map processing.</p> <p>Contains configuration settings that control how spike maps are processed, including distance steps, speed thresholds, and standardization options.</p> <p>Parameters:</p> Name Type Description Default <code>dist_step</code> <code>float</code> <p>Step size for distance calculations in spatial units</p> <code>1</code> <code>speed_threshold</code> <code>float</code> <p>Minimum speed threshold for valid movement periods</p> <code>1.0</code> <code>speed_max_allowed</code> <code>float</code> <p>Maximum speed allowed for valid movement periods (default is no maximum, can be useful when behavioral computer allows jumps in position which are usually due to hardware issues</p> <code>np.inf</code> <code>full_trial_flexibility</code> <code>float | None</code> <p>Flexibility parameter for trial alignment. If None, no flexibility</p> <code>None</code> <code>standardize_spks</code> <code>bool</code> <p>Whether to standardize spike counts by dividing by the standard deviation</p> <code>True</code> <code>smooth_width</code> <code>float | None</code> <p>Width of the Gaussian smoothing kernel to apply to the maps (width in spatial units)</p> <code>1</code> <code>reliability_method</code> <code>str</code> <p>Method to use for calculating reliability</p> <code>\"leave_one_out\"</code> <code>autosave</code> <code>bool</code> <p>Whether to save the cache automatically</p> <code>True</code> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@dataclass\nclass SpkmapParams:\n    \"\"\"Parameters for spike map processing.\n\n    Contains configuration settings that control how spike maps are processed,\n    including distance steps, speed thresholds, and standardization options.\n\n    Parameters\n    ----------\n    dist_step : float, default=1\n        Step size for distance calculations in spatial units\n    speed_threshold : float, default=1.0\n        Minimum speed threshold for valid movement periods\n    speed_max_allowed : float, default=np.inf\n        Maximum speed allowed for valid movement periods (default is no maximum,\n        can be useful when behavioral computer allows jumps in position which\n        are usually due to hardware issues\n    full_trial_flexibility : float | None, default=None\n        Flexibility parameter for trial alignment. If None, no flexibility\n    standardize_spks : bool, default=True\n        Whether to standardize spike counts by dividing by the standard deviation\n    smooth_width : float | None, default=1\n        Width of the Gaussian smoothing kernel to apply to the maps (width in spatial units)\n    reliability_method : str, default=\"leave_one_out\"\n        Method to use for calculating reliability\n    autosave : bool, default=True\n        Whether to save the cache automatically\n    \"\"\"\n\n    dist_step: float = 1.0\n    speed_threshold: float = 1.0\n    speed_max_allowed: float = np.inf\n    full_trial_flexibility: Union[float, None] = 3.0\n    standardize_spks: bool = True\n    smooth_width: Union[float, None] = 1.0\n    reliability_method: str = \"leave_one_out\"\n    autosave: bool = False\n\n    def __repr__(self) -&gt; str:\n        class_fields = fields(self)\n        lines = []\n        for field in class_fields:\n            field_name = field.name\n            field_value = getattr(self, field_name)\n            lines.append(f\"{field_name}={repr(field_value)}\")\n\n        class_name = self.__class__.__name__\n        joined_lines = \",\\n    \".join(lines)\n        return f\"{class_name}(\\n    {joined_lines}\\n)\"\n\n    @classmethod\n    def from_dict(cls, params_dict: dict) -&gt; \"SpkmapParams\":\n        \"\"\"Create a SpkmapParams instance from a dictionary, using defaults for missing values\"\"\"\n        return cls(**{k: params_dict[k] for k in params_dict})\n\n    @classmethod\n    def from_path(cls, path: Path) -&gt; \"SpkmapParams\":\n        \"\"\"Create a SpkmapParams instance from a json file\"\"\"\n        with open(path, \"r\") as f:\n            return cls.from_dict(json.load(f))\n\n    def compare(self, other: \"SpkmapParams\", filter_keys: Optional[List[str]] = None) -&gt; bool:\n        \"\"\"Compare two SpkmapParams instances\"\"\"\n        if filter_keys is None:\n            return self == other\n        else:\n            return all(getattr(self, key) == getattr(other, key) for key in filter_keys)\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save the parameters to a json file\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(self), f, sort_keys=True)\n\n    def __post_init__(self):\n        if self.dist_step &lt;= 0:\n            raise ValueError(\"dist_step must be positive\")\n        if self.speed_threshold &lt;= 0:\n            raise ValueError(\"speed_threshold must be positive\")\n        if self.full_trial_flexibility is not None and self.full_trial_flexibility &lt; 0:\n            raise ValueError(\"If used, full_trial_flexibility must be nonnegative (can also be None)\")\n        if self.smooth_width is not None and self.smooth_width &lt;= 0:\n            raise ValueError(\"smooth_width must be positive (can also be None)\")\n        # Convert floats to floats when not None\n        self.dist_step = float(self.dist_step)\n        self.speed_threshold = float(self.speed_threshold)\n        self.speed_max_allowed = float(self.speed_max_allowed)\n        self.full_trial_flexibility = float(self.full_trial_flexibility) if self.full_trial_flexibility is not None else None\n        self.smooth_width = float(self.smooth_width) if self.smooth_width is not None else None\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapParams.compare","title":"<code>compare(other, filter_keys=None)</code>","text":"<p>Compare two SpkmapParams instances</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def compare(self, other: \"SpkmapParams\", filter_keys: Optional[List[str]] = None) -&gt; bool:\n    \"\"\"Compare two SpkmapParams instances\"\"\"\n    if filter_keys is None:\n        return self == other\n    else:\n        return all(getattr(self, key) == getattr(other, key) for key in filter_keys)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapParams.from_dict","title":"<code>from_dict(params_dict)</code>  <code>classmethod</code>","text":"<p>Create a SpkmapParams instance from a dictionary, using defaults for missing values</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@classmethod\ndef from_dict(cls, params_dict: dict) -&gt; \"SpkmapParams\":\n    \"\"\"Create a SpkmapParams instance from a dictionary, using defaults for missing values\"\"\"\n    return cls(**{k: params_dict[k] for k in params_dict})\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapParams.from_path","title":"<code>from_path(path)</code>  <code>classmethod</code>","text":"<p>Create a SpkmapParams instance from a json file</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path) -&gt; \"SpkmapParams\":\n    \"\"\"Create a SpkmapParams instance from a json file\"\"\"\n    with open(path, \"r\") as f:\n        return cls.from_dict(json.load(f))\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapParams.save","title":"<code>save(path)</code>","text":"<p>Save the parameters to a json file</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save the parameters to a json file\"\"\"\n    with open(path, \"w\") as f:\n        json.dump(asdict(self), f, sort_keys=True)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor","title":"<code>SpkmapProcessor</code>  <code>dataclass</code>","text":"<p>Class for processing and caching spike maps from session data</p> <p>NOTES ON ENGINEERING: I want the variables required for processing spkmaps to be properties (@property) that have hidden attributes for caching. Therefore, we can use the property method to get the attribute and each property method can do whatever processing is needed for that attribute. (Uh, duh). Time to get modern. lol.</p> <p>Right now I've almost got the register_spkmaps method working again (not tested yet) but now is when the dataclass refactoring comes in. 1. Make it possible to separate the occmap from the spkmap loading.    - do so by making the preliminary variables properties with caching 2. Consider how to implement smoothing then correctMap functionality -- it should    be possible to do this in a way that allows me to iteratively try different    parameterizations without having to go through the whole pipeline again. 3. Consider how / when to implement reliability measures. In PCSS, they're done all    right there with get_spkmaps. But it's probably not always necessary and can    actually take a bit of time? It would also be nice to save reliability scores for    the neurons... but then we'd also need an independent params saving system for them. 4. Re: the point above, I wonder if the one.data loading system is ideal or if I should    use a more explicit and dedicated SpkmapProcessor saving / loading system.</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@dataclass\nclass SpkmapProcessor:\n    \"\"\"Class for processing and caching spike maps from session data\n\n    NOTES ON ENGINEERING:\n    I want the variables required for processing spkmaps to be properties (@property)\n    that have hidden attributes for caching. Therefore, we can use the property method\n    to get the attribute and each property method can do whatever processing is needed\n    for that attribute. (Uh, duh). Time to get modern. lol.\n\n    Right now I've almost got the register_spkmaps method working again (not tested yet)\n    but now is when the dataclass refactoring comes in.\n    1. Make it possible to separate the occmap from the spkmap loading.\n       - do so by making the preliminary variables properties with caching\n    2. Consider how to implement smoothing then correctMap functionality -- it should\n       be possible to do this in a way that allows me to iteratively try different\n       parameterizations without having to go through the whole pipeline again.\n    3. Consider how / when to implement reliability measures. In PCSS, they're done all\n       right there with get_spkmaps. But it's probably not always necessary and can\n       actually take a bit of time? It would also be nice to save reliability scores for\n       the neurons... but then we'd also need an independent params saving system for them.\n    4. Re: the point above, I wonder if the one.data loading system is ideal or if I should\n       use a more explicit and dedicated SpkmapProcessor saving / loading system.\n    \"\"\"\n\n    session: Union[SessionData, B2Session, SessionToSpkmapProtocol]\n    params: SpkmapParams = field(default_factory=SpkmapParams, repr=False)\n    data_cache: dict = field(default_factory=dict, repr=False, init=False)\n\n    def __post_init__(self):\n        # Check if the session provided is compatible with SpkmapProcessing\n        if not isinstance(self.session, SessionData):\n            raise ValueError(f\"session must be a SessionData instance, not {type(self.session)}\")\n        # (Don't check if it's a SessionToSpkmapProtocol because hasattr() will call properties which loads data...)\n\n        # We need to handle the case where params is a dictionary of partial updates to the default params\n        self.params = helpers.resolve_dataclass(self.params, SpkmapParams)\n\n    def cached_dependencies(self, data_type: str) -&gt; List[str]:\n        \"\"\"Get the dependencies for a given data type\"\"\"\n        if data_type == \"raw_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\"]\n        elif data_type == \"processed_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\"]\n        elif data_type == \"env_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\", \"full_trial_flexibility\"]\n        elif data_type == \"reliability\":\n            return [\n                \"dist_step\",\n                \"speed_threshold\",\n                \"speed_max_allowed\",\n                \"standardize_spks\",\n                \"smooth_width\",\n                \"full_trial_flexibility\",\n                \"reliability_method\",\n            ]\n        # Otherwise just return all params\n        return list(self.params.__dict__.keys())\n\n    def show_cache(self, data_type: Optional[str] = None) -&gt; str:\n        \"\"\"Helper function that scrapes the cache directory and shows cached files\n\n        Parameters\n        ----------\n        data_type: Optional[str] = None\n            Indicate a data type to filter which parts of the cache to show\n\n        Returns\n        -------\n        str\n            Formatted string showing cache information including data_type, size, parameters, and date\n        \"\"\"\n        import os\n        from datetime import datetime\n\n        # Get the base cache directory\n        base_cache_dir = self.cache_directory()\n\n        if not base_cache_dir.exists():\n            return f\"No cache directory found at: {base_cache_dir}\"\n\n        # Collect information about all cache files\n        cache_info = []\n\n        # Define the data types to check\n        if data_type is not None:\n            data_types_to_check = [data_type]\n        else:\n            data_types_to_check = [\"raw_maps\", \"processed_maps\", \"env_maps\", \"reliability\"]\n\n        for dt in data_types_to_check:\n            cache_dir = self.cache_directory(dt)\n            if not cache_dir.exists():\n                continue\n\n            # Find all parameter files (they define what caches exist)\n            param_files = list(cache_dir.glob(\"params_*.npz\"))\n\n            for param_file in param_files:\n                # Extract the hash from the filename\n                params_hash = param_file.stem.replace(\"params_\", \"\")\n\n                # Load the parameters\n                try:\n                    cached_params = dict(np.load(param_file))\n                    param_str = \", \".join([f\"{k}={v}\" for k, v in cached_params.items()])\n                except Exception as e:\n                    param_str = f\"Error loading params: {e}\"\n\n                # Get file modification time\n                mod_time = datetime.fromtimestamp(param_file.stat().st_mtime)\n                date_str = mod_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n                # Calculate total size of all related cache files\n                total_size = param_file.stat().st_size\n\n                if dt in [\"raw_maps\", \"processed_maps\"]:\n                    # For maps, look for data files for each map type\n                    for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                        data_file = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                        if data_file.exists():\n                            total_size += data_file.stat().st_size\n\n                elif dt == \"env_maps\":\n                    # For env_maps, look for environment file and individual environment data files\n                    env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                    if env_file.exists():\n                        total_size += env_file.stat().st_size\n                        # Load environments to find all data files\n                        try:\n                            environments = np.load(env_file)\n                            for env in environments:\n                                for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                                    data_file = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                                    if data_file.exists():\n                                        total_size += data_file.stat().st_size\n                        except Exception:\n                            pass  # Continue even if we can't load environments\n\n                elif dt == \"reliability\":\n                    # For reliability, look for environments and reliability data files\n                    env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                    rel_file = cache_dir / f\"data_reliability_{params_hash}.npy\"\n                    if env_file.exists():\n                        total_size += env_file.stat().st_size\n                    if rel_file.exists():\n                        total_size += rel_file.stat().st_size\n\n                # Convert size to human readable format\n                size_str = self._format_file_size(total_size)\n\n                cache_info.append(\n                    {\n                        \"data_type\": dt,\n                        \"size\": size_str,\n                        \"parameters\": param_str,\n                        \"date\": date_str,\n                        \"hash\": params_hash[:8],  # Show first 8 chars of hash\n                    }\n                )\n\n        if not cache_info:\n            return \"No cache files found.\"\n\n        # Format the output as a table\n        output_lines = []\n        output_lines.append(\"Cache Files Summary\")\n        output_lines.append(\"=\" * 80)\n        output_lines.append(f\"{'Data Type':&lt;15} {'Size':&lt;10} {'Date':&lt;20} {'Hash':&lt;10} {'Parameters'}\")\n        output_lines.append(\"-\" * 80)\n\n        for info in cache_info:\n            output_lines.append(f\"{info['data_type']:&lt;15} {info['size']:&lt;10} {info['date']:&lt;20} \" f\"{info['hash']:&lt;10} {info['parameters']}\")\n\n        output_lines.append(\"-\" * 80)\n        output_lines.append(f\"Total cache entries: {len(cache_info)}\")\n\n        result = \"\\n\".join(output_lines)\n        print(result)\n\n    def _format_file_size(self, size_bytes: int) -&gt; str:\n        \"\"\"Convert bytes to human readable format\"\"\"\n        if size_bytes == 0:\n            return \"0 B\"\n\n        size_names = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n        import math\n\n        i = int(math.floor(math.log(size_bytes, 1024)))\n        p = math.pow(1024, i)\n        s = round(size_bytes / p, 2)\n        return f\"{s} {size_names[i]}\"\n\n    def cache_directory(self, data_type: Optional[str] = None) -&gt; Path:\n        \"\"\"Get the cache directory for a given data type and spks_type\"\"\"\n        if data_type is None:\n            return self.session.data_path / \"spkmaps\"\n        else:\n            folder_name = f\"{data_type}_{self.session.spks_type}\"\n            return self.session.data_path / \"spkmaps\" / folder_name\n\n    def dependent_params(self, data_type: str) -&gt; dict:\n        \"\"\"Get the dependent parameters for a given data type\"\"\"\n        return {k: getattr(self.params, k) for k in self.cached_dependencies(data_type)}\n\n    def _params_hash(self, data_type: str) -&gt; str:\n        \"\"\"Get the hash of the dependent parameters for a given data type\"\"\"\n        return hashlib.sha256(json.dumps(self.dependent_params(data_type), sort_keys=True).encode()).hexdigest()\n\n    def save_cache(self, data_type: str, data: Union[Maps, Reliability]):\n        \"\"\"Save the cached params and data for a given data type\"\"\"\n        cache_dir = self.cache_directory(data_type)\n        params_hash = self._params_hash(data_type)\n        cache_param_path = cache_dir / f\"params_{params_hash}.npz\"\n        if not cache_dir.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n        np.savez(cache_param_path, **self.dependent_params(data_type))\n        if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n            for mapname in Maps.map_types():\n                cache_data_path = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                np.save(cache_data_path, getattr(data, mapname))\n        elif data_type == \"env_maps\":\n            environments = data.environments\n            np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n            for ienv, env in enumerate(environments):\n                for mapname in Maps.map_types():\n                    cache_data_path = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                    np.save(cache_data_path, getattr(data, mapname)[ienv])\n        elif data_type == \"reliability\":\n            values = data.values\n            environments = data.environments\n            # don't need data.method because it's in params...\n            np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n            np.save(cache_dir / f\"data_reliability_{params_hash}.npy\", values)\n        else:\n            raise ValueError(f\"Unknown data type: {data_type}\")\n\n    def load_from_cache(self, data_type: str) -&gt; Tuple[Union[Maps, Reliability], bool]:\n        \"\"\"Get the cached params and data for a given data type\"\"\"\n        cache_dir = self.cache_directory(data_type)\n        if cache_dir.exists():\n            # If the directory exists, check if there are any cached params that match the expected hash\n            params_hash = self._params_hash(data_type)\n            cached_params_path = cache_dir / f\"params_{params_hash}.npz\"\n            if cached_params_path.exists():\n                cached_params = dict(np.load(cached_params_path))\n                # Check if the cached params match the dependent params\n                if self.check_params_match(cached_params):\n                    return self._load_from_cache(data_type, params_hash, params=cached_params), True\n        return None, False\n\n    def check_params_match(self, cached_params: dict) -&gt; bool:\n        \"\"\"Check if the cached params and the current params are the same.\n\n        Parameters\n        ----------\n        cached_params : dict\n            The cached params to check against the current params\n\n        Returns\n        -------\n        bool\n            True if the cached params are nonempty and match the current params, False otherwise\n        \"\"\"\n        return cached_params and all(cached_params[k] == getattr(self.params, k) for k in cached_params)\n\n    def _load_from_cache(self, data_type: str, params_hash: str, params: Optional[Dict[str, Any]] | None = None) -&gt; Union[Maps, Reliability]:\n        \"\"\"Load the cached data for a given data type\"\"\"\n        cache_dir = self.cache_directory(data_type)\n        if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n            cached_data = {}\n            for name in Maps.map_types():\n                cached_data[name] = np.load(cache_dir / f\"data_{name}_{params_hash}.npy\", mmap_mode=\"r\")\n            if data_type == \"raw_maps\":\n                return Maps.create_raw_maps(**cached_data)\n            elif data_type == \"processed_maps\":\n                return Maps.create_processed_maps(**cached_data)\n        elif data_type == \"env_maps\":\n            environments = np.load(cache_dir / f\"data_environments_{params_hash}.npy\")\n            cached_data = dict(environments=environments)\n            for name in Maps.map_types():\n                cached_data[name] = []\n                for env in environments:\n                    cached_data[name].append(np.load(cache_dir / f\"data_{name}_{env}_{params_hash}.npy\", mmap_mode=\"r\"))\n            return Maps.create_environment_maps(**cached_data)\n        elif data_type == \"reliability\":\n            environments = np.load(cache_dir / f\"data_environments_{params_hash}.npy\")\n            values = np.load(cache_dir / f\"data_reliability_{params_hash}.npy\")\n            method = params[\"reliability_method\"]\n            return Reliability(values, environments, method)\n        else:\n            raise ValueError(f\"Unknown data type: {data_type}\")\n\n    @manage_one_cache\n    def _filter_environments(\n        self,\n        envnum: Union[int, Iterable[int], None] = None,\n        clear_one_cache: bool = True,\n    ) -&gt; np.ndarray[bool]:\n        \"\"\"Filter the session data to only include trials from certain environments\n\n        NOTE:\n        This assumes that the trials are in order. We might want to use the third output of session.positions to\n        get the \"real\" trial numbers which aren't always contiguous and 0 indexed.\n\n        If envnum is not provided, will return all trials.\n        \"\"\"\n        if envnum is None:\n            envnum = self.session.environments\n        envnum = helpers.check_iterable(envnum)\n        return np.isin(self.session.trial_environment, envnum)\n\n    @property\n    def dist_edges(self) -&gt; np.ndarray[float]:\n        \"\"\"Distance edges for the position bins\"\"\"\n        if not hasattr(self, \"_env_length\"):\n            env_length = self.session.env_length\n            if hasattr(env_length, \"__len__\"):\n                if np.unique(env_length).size != 1:\n                    msg = \"SpkmapProcessor (currently) requires all trials to have the same env length!\"\n                    raise ValueError(msg)\n                env_length = env_length[0]\n            self._env_length = env_length\n\n        num_positions = int(self._env_length / self.params.dist_step)\n        return np.linspace(0, self._env_length, num_positions + 1)\n\n    @property\n    def dist_centers(self) -&gt; np.ndarray[float]:\n        \"\"\"Distance centers for the position bins\"\"\"\n        return helpers.edge2center(self.dist_edges)\n\n    @manage_one_cache\n    def _idx_required_position_bins(self, clear_one_cache: bool = True) -&gt; np.ndarray:\n        \"\"\"Get the indices of the position bins that are required for a full trial\n\n        Parameters\n        ----------\n        clear_one_cache : bool, default=False\n            Whether to clear the onefile cache after getting the indices\n\n        Returns\n        -------\n        np.ndarray\n            The indices of the position bins that are required for a trial to be considered full\n        \"\"\"\n        num_position_bins = len(self.dist_centers)\n        if self.params.full_trial_flexibility is None:\n            idx_to_required_bins = np.arange(num_position_bins)\n        else:\n            start_idx = np.where(self.dist_edges &gt;= self.params.full_trial_flexibility)[0][0]\n            end_idx = np.where(self.dist_edges &lt;= self.dist_edges[-1] - self.params.full_trial_flexibility)[0][-1]\n            idx_to_required_bins = np.arange(start_idx, end_idx)\n        return idx_to_required_bins\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"raw_maps\", disable=False)\n    def get_raw_maps(\n        self,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get maps (occupancy, speed, spkmap) from session data by processing with provided parameters.\n\n        Parameters\n        ----------\n        force_recompute : bool, default=False\n            Whether to force the recomputation of the maps even if they exist in the cache.\n        clear_one_cache : bool, default=False\n            Whether to clear the onefile cache after getting the maps (only clears the onecache for this method)\n        params : SpkmapParams, dict, or None, default=None\n            Parameters for the maps. If None, the parameters will be taken from the SpkmapProcessor instance.\n            If a dictionary, it will be used to update the parameters.\n            Will always be temporary -- so the original parameters will be restored after the method is finished.\n        \"\"\"\n        dist_edges = self.dist_edges\n        dist_centers = self.dist_centers\n        num_positions = len(dist_centers)\n\n        # Get behavioral timestamps and positions\n        timestamps, positions, trial_numbers, idx_behave_to_frame = self.session.positions\n\n        # compute behavioral speed on each sample\n        within_trial_sample = np.append(np.diff(trial_numbers) == 0, True)\n        sample_duration = np.append(np.diff(timestamps), 0)\n        speeds = np.append(np.diff(positions) / sample_duration[:-1], 0)\n        # do this after division so no /0 errors\n        sample_duration = sample_duration * within_trial_sample\n        # speed 0 in last sample for each trial (it's undefined)\n        speeds = speeds * within_trial_sample\n        # Convert positions to position bins\n        position_bin = np.digitize(positions, dist_edges) - 1\n\n        # get imaging information\n        frame_time_stamps = self.session.timestamps\n        sampling_period = np.median(np.diff(frame_time_stamps))\n        dist_cutoff = sampling_period / 2\n        delay_position_to_imaging = frame_time_stamps[idx_behave_to_frame] - timestamps\n\n        # get spiking information\n        spks = self.session.spks\n        num_rois = self.session.get_value(\"numROIs\")\n\n        # Do standardization\n        if self.params.standardize_spks:\n            spks = median_zscore(spks, median_subtract=not self.session.zero_baseline_spks)\n\n        # Get high resolution occupancy and speed maps\n        dtype = np.float32\n        occmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        speedmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        spkmap = np.zeros((self.session.num_trials, num_positions, num_rois), dtype=dtype)\n        extra_counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n\n        # Get maps -- doing this independently for each map allows for more\n        # flexibility in which data to load (basically the occmap &amp; speedmap\n        # are instantaneous, but the spkmap is a bit slower)\n        get_summation_map(\n            sample_duration,\n            trial_numbers,\n            position_bin,\n            occmap,\n            counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=False,\n            use_sample_to_value_idx=False,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n        get_summation_map(\n            speeds,\n            trial_numbers,\n            position_bin,\n            speedmap,\n            counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=True,\n            use_sample_to_value_idx=False,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n        get_summation_map(\n            spks,\n            trial_numbers,\n            position_bin,\n            spkmap,\n            extra_counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=True,\n            use_sample_to_value_idx=True,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n\n        # Figure out the valid range (outside of this range, set the maps to nan, because their values are not meaningful)\n        position_bin_per_trial = [position_bin[trial_numbers == tnum] for tnum in range(self.session.num_trials)]\n\n        # offsetting by 1 because there is a bug in the vrControl software where the first sample is always set\n        # to the minimum position (which is 0), but if there is a built-up buffer in the rotary encoder, the position\n        # will jump at the second sample. In general this will always work unless the mice have a truly ridiculous\n        # speed at the beginning of the trial...\n        first_valid_bin = [np.min(bpb[1:] if len(bpb) &gt; 1 else bpb) for bpb in position_bin_per_trial]\n        last_valid_bin = [np.max(bpb) for bpb in position_bin_per_trial]\n\n        # set bins to nan when mouse didn't visit them\n        occmap = replace_missing_data(occmap, first_valid_bin, last_valid_bin)\n        speedmap = replace_missing_data(speedmap, first_valid_bin, last_valid_bin)\n        spkmap = replace_missing_data(spkmap, first_valid_bin, last_valid_bin)\n\n        return Maps.create_raw_maps(occmap, speedmap, spkmap)\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"processed_maps\", disable=False)\n    def get_processed_maps(\n        self,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Process the maps\"\"\"\n        # Get the raw maps first (don't need to specify params because they're already set by this method)\n        maps = self.get_raw_maps(\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n        )\n\n        # Process the maps (smooth, divide by occupancy, and change to ROIs first)\n        return maps.raw_to_processed(self.dist_centers, self.params.smooth_width)\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"env_maps\", disable=False)\n    def get_env_maps(\n        self,\n        use_session_filters: bool = True,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get the map for a given environment number\"\"\"\n        # Make sure it's an iterable -- the output will always be a list\n        envnum = helpers.check_iterable(self.session.environments)\n\n        # Get the indices of the trials to each environment\n        idx_each_environment = [self._filter_environments(env) for env in envnum]\n\n        # Then get the indices of the position bins that are required for a full trial\n        idx_required_position_bins = self._idx_required_position_bins(clear_one_cache)\n\n        # Get the processed maps (don't need to specify params because they're already set by the decorator)\n        maps = self.get_processed_maps(\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n        )\n\n        # Add the list of environments to the maps\n        maps.environments = envnum\n\n        # Make a list of the maps we are processing\n        maps_to_process = Maps.map_types()\n\n        # Filter the maps to only include the ROIs we want\n        if use_session_filters:\n            idx_rois = np.where(self.session.idx_rois)[0]\n        else:\n            idx_rois = np.arange(self.session.get_value(\"numROIs\"), dtype=int)\n\n        # Filter the maps to only include the full trials\n        full_trials = np.where(np.all(~np.isnan(maps.occmap[:, idx_required_position_bins]), axis=1))[0]\n\n        # Implement trial &amp; ROI filtering here\n        for mapname in maps_to_process:\n            if mapname == \"spkmap\":\n                maps[mapname] = np.take(np.take(maps[mapname], idx_rois, axis=0), full_trials, axis=1)\n            else:\n                maps[mapname] = np.take(maps[mapname], full_trials, axis=0)\n\n        # Filter the trial indices to only include full trials\n        idx_each_environment = [np.where(np.take(idx, full_trials, axis=0))[0] for idx in idx_each_environment]\n\n        # Then group each one by environment\n        # -&gt; this is now (trials_in_env, position_bins, ...(roi if spkmap)...)\n        maps.by_environment = True\n        for mapname in maps_to_process:\n            if mapname == \"spkmap\":\n                maps[mapname] = [np.take(maps[mapname], idx, axis=1) for idx in idx_each_environment]\n            else:\n                maps[mapname] = [np.take(maps[mapname], idx, axis=0) for idx in idx_each_environment]\n\n        return maps\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"reliability\", disable=False)\n    def get_reliability(\n        self,\n        use_session_filters: bool = True,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ):\n        \"\"\"Get the reliability of the maps\"\"\"\n        envnum = helpers.check_iterable(self.session.environments)\n\n        # A list of the requested environments (all if not specified)\n        maps = self.get_env_maps(\n            use_session_filters=use_session_filters,\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n            params={\"autosave\": False},  # Prevent saving in the case of a recompute\n        )\n\n        # All reliability measures require no NaNs\n        maps.pop_nan_positions()\n\n        if self.params.reliability_method == \"leave_one_out\":\n            rel_values = [helpers.reliability_loo(spkmap) for spkmap in maps.spkmap]\n        elif self.params.reliability_method == \"correlation\" or self.params.reliability_method == \"mse\":\n            rel_mse, rel_cor = helpers.named_transpose([helpers.measureReliability(spkmap) for spkmap in maps.spkmap])\n            rel_values = rel_mse if self.params.reliability_method == \"mse\" else rel_cor\n        else:\n            raise ValueError(f\"Method {self.params.reliability_method} not supported\")\n\n        return Reliability(\n            np.stack(rel_values),\n            environments=envnum,\n            method=self.params.reliability_method,\n        )\n\n    # ------------------- convert between imaging and behavioral time -------------------\n    @with_temp_params\n    @manage_one_cache\n    def get_frame_behavior(self, clear_one_cache: bool = True, params: Union[SpkmapParams, Dict[str, Any], None] = None):\n        \"\"\"\n        get position and environment data for each frame in imaging data\n        nan if no position data is available for that frame (e.g. if the closest\n        behavioral sample is further away in time than the sampling period)\n        \"\"\"\n        timestamps = self.session.loadone(\"positionTracking.times\")\n        position = self.session.loadone(\"positionTracking.position\")\n        idx_behave_to_frame = self.session.loadone(\"positionTracking.mpci\")\n        trial_start_index = self.session.loadone(\"trials.positionTracking\")\n        num_samples = len(position)\n        trial_numbers = np.arange(len(trial_start_index))\n        trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n        trial_numbers = np.repeat(trial_numbers, trial_lengths)\n        trial_environment = self.session.loadone(\"trials.environmentIndex\")\n        trial_environment = np.repeat(trial_environment, trial_lengths)\n\n        within_trial = np.append(np.diff(trial_numbers) == 0, True)\n        sample_duration = np.append(np.diff(timestamps), 0)\n        speed = np.append(np.diff(position) / sample_duration[:-1], 0)\n        sample_duration = sample_duration * within_trial\n        speed = speed * within_trial\n\n        frame_timestamps = self.session.loadone(\"mpci.times\")\n        difference_timestamps = np.abs(timestamps - frame_timestamps[idx_behave_to_frame])\n        sampling_period = np.median(np.diff(frame_timestamps))\n        dist_cutoff = sampling_period / 2\n\n        frame_position = np.zeros_like(frame_timestamps)\n        count = np.zeros_like(frame_timestamps)\n        helpers.get_average_frame_position(position, idx_behave_to_frame, difference_timestamps, dist_cutoff, frame_position, count)\n        frame_position[count &gt; 0] /= count[count &gt; 0]\n        frame_position[count == 0] = np.nan\n        frame_speed = np.diff(frame_position) / np.diff(frame_timestamps)\n        frame_speed = np.append(frame_speed, 0)\n\n        # Let the last frame of each trial have a speed equal to previous frame\n        idx_first_nan = np.where(np.diff(1.0 * np.isnan(frame_position)) == 1.0)[0]\n        frame_speed[idx_first_nan] = frame_speed[idx_first_nan - 1]\n\n        idx_frame_to_behave, dist_frame_to_behave = helpers.nearestpoint(frame_timestamps, timestamps)\n        idx_get_position = dist_frame_to_behave &lt; dist_cutoff\n\n        frame_environment = np.full(len(frame_timestamps), np.nan)\n        frame_environment[idx_get_position] = trial_environment[idx_frame_to_behave[idx_get_position]]\n        frame_environment[count == 0] = np.nan\n\n        frame_trial = np.full(len(frame_timestamps), np.nan)\n        frame_trial[idx_get_position] = trial_numbers[idx_frame_to_behave[idx_get_position]]\n        frame_trial[count == 0] = np.nan\n\n        return frame_position, frame_speed, frame_environment, frame_trial\n\n    @with_temp_params\n    @manage_one_cache\n    def get_placefield_prediction(\n        self,\n        use_session_filters: bool = True,\n        spks_type: Union[str, None] = None,\n        use_speed_threshold: bool = True,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ):\n        \"\"\"\n        get placefield prediction of session spks data from spkmaps\n        \"\"\"\n        if spks_type is not None:\n            _spks_type = self.session.spks_type\n            self.session.params.spks_type = spks_type\n\n        frame_position, frame_speed, frame_environment, _ = self.get_frame_behavior(clear_one_cache, params)\n        idx_valid = ~np.isnan(frame_position)\n        if use_speed_threshold:\n            idx_valid = idx_valid &amp; (frame_speed &gt; self.params.speed_threshold)\n\n        # Convert frame position to bins indices\n        frame_position_index = np.searchsorted(self.dist_edges, frame_position, side=\"right\") - 1\n\n        # Get the place field for each neuron\n        env_maps = self.get_env_maps(use_session_filters=use_session_filters)\n        env_maps.average_trials()\n\n        # Convert frame environment to indices\n        env_to_idx = {env: i for i, env in enumerate(env_maps.environments)}\n        frame_environment_index = np.array([env_to_idx[env] if not np.isnan(env) else -1000 for env in frame_environment], dtype=int)\n\n        # Get the original spks data\n        spks = self.session.spks\n        if use_session_filters:\n            spks = spks[:, self.session.idx_rois]\n\n        # Use a numba speed up to get the placefield prediction (single pass simple algorithm)\n        placefield_prediction = np.full(spks.shape, np.nan)\n        placefield_prediction = _placefield_prediction_numba(\n            placefield_prediction,\n            env_maps.spkmap,\n            frame_environment_index,\n            frame_position_index,\n            idx_valid,\n        )\n\n        # This will add samples for which a place field was not estimable (at the edges of the environment)\n        idx_valid = np.all(~np.isnan(placefield_prediction), axis=1)\n\n        # Reset spks_type\n        if spks_type is not None:\n            self.session.params.spks_type = _spks_type\n\n        # Include extra details in a dictionary for forward compatibility\n        extras = dict(\n            frame_position_index=frame_position_index,\n            frame_environment_index=frame_environment_index,\n            idx_valid=idx_valid,\n        )\n\n        return placefield_prediction, extras\n\n    def get_traversals(\n        self,\n        idx_roi: int,\n        idx_env: int,\n        width: int = 10,\n        placefield_threshold: float = 5.0,  # in cm (or whatever units the frame_position is in)\n        fill_nan: bool = False,\n        spks: np.ndarray = None,\n        spks_prediction: np.ndarray = None,\n    ):\n        frame_position, _, frame_environment, frame_trial = self.get_frame_behavior()\n        if spks_prediction is None:\n            spks_prediction = self.get_placefield_prediction(use_session_filters=True)[0]\n        if spks is None:\n            spks = self.session.spks[:, self.session.idx_rois]\n\n        if spks.shape != spks_prediction.shape:\n            raise ValueError(\"spks and spks_prediction must have the same shape\")\n\n        env_maps = self.get_env_maps()\n        pos_peak = self.dist_centers[np.nanargmax(np.nanmean(env_maps.spkmap[idx_env][idx_roi], axis=0))]\n        envnum = env_maps.environments[idx_env]\n\n        env_trials = np.unique(frame_trial[frame_environment == envnum])\n\n        num_trials = len(env_trials)\n        idx_traversal = -1 * np.ones(num_trials, dtype=int)\n        for itrial, trialnum in enumerate(env_trials):\n            idx_trial = frame_trial == trialnum\n            idx_closest_pos = np.nanargmin(np.abs(frame_position - pos_peak) + 10000 * ~idx_trial)\n\n            # Only include the trial if the closest position is within placefield threshold of the peak\n            if np.abs(frame_position[idx_closest_pos] - pos_peak) &lt; placefield_threshold:\n                idx_traversal[itrial] = idx_closest_pos\n\n        # Filter out trials that don't have a traversal\n        idx_traversal = idx_traversal[idx_traversal != -1]\n\n        # Get traversals through place field in requested environment\n        traversals = np.zeros((len(idx_traversal), width * 2 + 1))\n        pred_travs = np.zeros((len(idx_traversal), width * 2 + 1))\n        for ii, it in enumerate(idx_traversal):\n            istart = it - width\n            iend = it + width + 1\n            istartoffset = max(0, -istart)\n            iendoffset = max(0, iend - spks.shape[0])\n            traversals[ii, istartoffset : width * 2 + 1 - iendoffset] = spks[istart + istartoffset : iend - iendoffset, idx_roi]\n            pred_travs[ii, istartoffset : width * 2 + 1 - iendoffset] = spks_prediction[istart + istartoffset : iend - iendoffset, idx_roi]\n\n        if fill_nan:\n            traversals[np.isnan(traversals)] = 0.0\n            pred_travs[np.isnan(pred_travs)] = 0.0\n\n        return traversals, pred_travs\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.dist_centers","title":"<code>dist_centers</code>  <code>property</code>","text":"<p>Distance centers for the position bins</p>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.dist_edges","title":"<code>dist_edges</code>  <code>property</code>","text":"<p>Distance edges for the position bins</p>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.cache_directory","title":"<code>cache_directory(data_type=None)</code>","text":"<p>Get the cache directory for a given data type and spks_type</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def cache_directory(self, data_type: Optional[str] = None) -&gt; Path:\n    \"\"\"Get the cache directory for a given data type and spks_type\"\"\"\n    if data_type is None:\n        return self.session.data_path / \"spkmaps\"\n    else:\n        folder_name = f\"{data_type}_{self.session.spks_type}\"\n        return self.session.data_path / \"spkmaps\" / folder_name\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.cached_dependencies","title":"<code>cached_dependencies(data_type)</code>","text":"<p>Get the dependencies for a given data type</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def cached_dependencies(self, data_type: str) -&gt; List[str]:\n    \"\"\"Get the dependencies for a given data type\"\"\"\n    if data_type == \"raw_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\"]\n    elif data_type == \"processed_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\"]\n    elif data_type == \"env_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\", \"full_trial_flexibility\"]\n    elif data_type == \"reliability\":\n        return [\n            \"dist_step\",\n            \"speed_threshold\",\n            \"speed_max_allowed\",\n            \"standardize_spks\",\n            \"smooth_width\",\n            \"full_trial_flexibility\",\n            \"reliability_method\",\n        ]\n    # Otherwise just return all params\n    return list(self.params.__dict__.keys())\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.check_params_match","title":"<code>check_params_match(cached_params)</code>","text":"<p>Check if the cached params and the current params are the same.</p> <p>Parameters:</p> Name Type Description Default <code>cached_params</code> <code>dict</code> <p>The cached params to check against the current params</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the cached params are nonempty and match the current params, False otherwise</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def check_params_match(self, cached_params: dict) -&gt; bool:\n    \"\"\"Check if the cached params and the current params are the same.\n\n    Parameters\n    ----------\n    cached_params : dict\n        The cached params to check against the current params\n\n    Returns\n    -------\n    bool\n        True if the cached params are nonempty and match the current params, False otherwise\n    \"\"\"\n    return cached_params and all(cached_params[k] == getattr(self.params, k) for k in cached_params)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.dependent_params","title":"<code>dependent_params(data_type)</code>","text":"<p>Get the dependent parameters for a given data type</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def dependent_params(self, data_type: str) -&gt; dict:\n    \"\"\"Get the dependent parameters for a given data type\"\"\"\n    return {k: getattr(self.params, k) for k in self.cached_dependencies(data_type)}\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.get_env_maps","title":"<code>get_env_maps(use_session_filters=True, force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get the map for a given environment number</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"env_maps\", disable=False)\ndef get_env_maps(\n    self,\n    use_session_filters: bool = True,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get the map for a given environment number\"\"\"\n    # Make sure it's an iterable -- the output will always be a list\n    envnum = helpers.check_iterable(self.session.environments)\n\n    # Get the indices of the trials to each environment\n    idx_each_environment = [self._filter_environments(env) for env in envnum]\n\n    # Then get the indices of the position bins that are required for a full trial\n    idx_required_position_bins = self._idx_required_position_bins(clear_one_cache)\n\n    # Get the processed maps (don't need to specify params because they're already set by the decorator)\n    maps = self.get_processed_maps(\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n    )\n\n    # Add the list of environments to the maps\n    maps.environments = envnum\n\n    # Make a list of the maps we are processing\n    maps_to_process = Maps.map_types()\n\n    # Filter the maps to only include the ROIs we want\n    if use_session_filters:\n        idx_rois = np.where(self.session.idx_rois)[0]\n    else:\n        idx_rois = np.arange(self.session.get_value(\"numROIs\"), dtype=int)\n\n    # Filter the maps to only include the full trials\n    full_trials = np.where(np.all(~np.isnan(maps.occmap[:, idx_required_position_bins]), axis=1))[0]\n\n    # Implement trial &amp; ROI filtering here\n    for mapname in maps_to_process:\n        if mapname == \"spkmap\":\n            maps[mapname] = np.take(np.take(maps[mapname], idx_rois, axis=0), full_trials, axis=1)\n        else:\n            maps[mapname] = np.take(maps[mapname], full_trials, axis=0)\n\n    # Filter the trial indices to only include full trials\n    idx_each_environment = [np.where(np.take(idx, full_trials, axis=0))[0] for idx in idx_each_environment]\n\n    # Then group each one by environment\n    # -&gt; this is now (trials_in_env, position_bins, ...(roi if spkmap)...)\n    maps.by_environment = True\n    for mapname in maps_to_process:\n        if mapname == \"spkmap\":\n            maps[mapname] = [np.take(maps[mapname], idx, axis=1) for idx in idx_each_environment]\n        else:\n            maps[mapname] = [np.take(maps[mapname], idx, axis=0) for idx in idx_each_environment]\n\n    return maps\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.get_frame_behavior","title":"<code>get_frame_behavior(clear_one_cache=True, params=None)</code>","text":"<p>get position and environment data for each frame in imaging data nan if no position data is available for that frame (e.g. if the closest behavioral sample is further away in time than the sampling period)</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\ndef get_frame_behavior(self, clear_one_cache: bool = True, params: Union[SpkmapParams, Dict[str, Any], None] = None):\n    \"\"\"\n    get position and environment data for each frame in imaging data\n    nan if no position data is available for that frame (e.g. if the closest\n    behavioral sample is further away in time than the sampling period)\n    \"\"\"\n    timestamps = self.session.loadone(\"positionTracking.times\")\n    position = self.session.loadone(\"positionTracking.position\")\n    idx_behave_to_frame = self.session.loadone(\"positionTracking.mpci\")\n    trial_start_index = self.session.loadone(\"trials.positionTracking\")\n    num_samples = len(position)\n    trial_numbers = np.arange(len(trial_start_index))\n    trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n    trial_numbers = np.repeat(trial_numbers, trial_lengths)\n    trial_environment = self.session.loadone(\"trials.environmentIndex\")\n    trial_environment = np.repeat(trial_environment, trial_lengths)\n\n    within_trial = np.append(np.diff(trial_numbers) == 0, True)\n    sample_duration = np.append(np.diff(timestamps), 0)\n    speed = np.append(np.diff(position) / sample_duration[:-1], 0)\n    sample_duration = sample_duration * within_trial\n    speed = speed * within_trial\n\n    frame_timestamps = self.session.loadone(\"mpci.times\")\n    difference_timestamps = np.abs(timestamps - frame_timestamps[idx_behave_to_frame])\n    sampling_period = np.median(np.diff(frame_timestamps))\n    dist_cutoff = sampling_period / 2\n\n    frame_position = np.zeros_like(frame_timestamps)\n    count = np.zeros_like(frame_timestamps)\n    helpers.get_average_frame_position(position, idx_behave_to_frame, difference_timestamps, dist_cutoff, frame_position, count)\n    frame_position[count &gt; 0] /= count[count &gt; 0]\n    frame_position[count == 0] = np.nan\n    frame_speed = np.diff(frame_position) / np.diff(frame_timestamps)\n    frame_speed = np.append(frame_speed, 0)\n\n    # Let the last frame of each trial have a speed equal to previous frame\n    idx_first_nan = np.where(np.diff(1.0 * np.isnan(frame_position)) == 1.0)[0]\n    frame_speed[idx_first_nan] = frame_speed[idx_first_nan - 1]\n\n    idx_frame_to_behave, dist_frame_to_behave = helpers.nearestpoint(frame_timestamps, timestamps)\n    idx_get_position = dist_frame_to_behave &lt; dist_cutoff\n\n    frame_environment = np.full(len(frame_timestamps), np.nan)\n    frame_environment[idx_get_position] = trial_environment[idx_frame_to_behave[idx_get_position]]\n    frame_environment[count == 0] = np.nan\n\n    frame_trial = np.full(len(frame_timestamps), np.nan)\n    frame_trial[idx_get_position] = trial_numbers[idx_frame_to_behave[idx_get_position]]\n    frame_trial[count == 0] = np.nan\n\n    return frame_position, frame_speed, frame_environment, frame_trial\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.get_placefield_prediction","title":"<code>get_placefield_prediction(use_session_filters=True, spks_type=None, use_speed_threshold=True, clear_one_cache=True, params=None)</code>","text":"<p>get placefield prediction of session spks data from spkmaps</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\ndef get_placefield_prediction(\n    self,\n    use_session_filters: bool = True,\n    spks_type: Union[str, None] = None,\n    use_speed_threshold: bool = True,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n):\n    \"\"\"\n    get placefield prediction of session spks data from spkmaps\n    \"\"\"\n    if spks_type is not None:\n        _spks_type = self.session.spks_type\n        self.session.params.spks_type = spks_type\n\n    frame_position, frame_speed, frame_environment, _ = self.get_frame_behavior(clear_one_cache, params)\n    idx_valid = ~np.isnan(frame_position)\n    if use_speed_threshold:\n        idx_valid = idx_valid &amp; (frame_speed &gt; self.params.speed_threshold)\n\n    # Convert frame position to bins indices\n    frame_position_index = np.searchsorted(self.dist_edges, frame_position, side=\"right\") - 1\n\n    # Get the place field for each neuron\n    env_maps = self.get_env_maps(use_session_filters=use_session_filters)\n    env_maps.average_trials()\n\n    # Convert frame environment to indices\n    env_to_idx = {env: i for i, env in enumerate(env_maps.environments)}\n    frame_environment_index = np.array([env_to_idx[env] if not np.isnan(env) else -1000 for env in frame_environment], dtype=int)\n\n    # Get the original spks data\n    spks = self.session.spks\n    if use_session_filters:\n        spks = spks[:, self.session.idx_rois]\n\n    # Use a numba speed up to get the placefield prediction (single pass simple algorithm)\n    placefield_prediction = np.full(spks.shape, np.nan)\n    placefield_prediction = _placefield_prediction_numba(\n        placefield_prediction,\n        env_maps.spkmap,\n        frame_environment_index,\n        frame_position_index,\n        idx_valid,\n    )\n\n    # This will add samples for which a place field was not estimable (at the edges of the environment)\n    idx_valid = np.all(~np.isnan(placefield_prediction), axis=1)\n\n    # Reset spks_type\n    if spks_type is not None:\n        self.session.params.spks_type = _spks_type\n\n    # Include extra details in a dictionary for forward compatibility\n    extras = dict(\n        frame_position_index=frame_position_index,\n        frame_environment_index=frame_environment_index,\n        idx_valid=idx_valid,\n    )\n\n    return placefield_prediction, extras\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.get_processed_maps","title":"<code>get_processed_maps(force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Process the maps</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"processed_maps\", disable=False)\ndef get_processed_maps(\n    self,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Process the maps\"\"\"\n    # Get the raw maps first (don't need to specify params because they're already set by this method)\n    maps = self.get_raw_maps(\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n    )\n\n    # Process the maps (smooth, divide by occupancy, and change to ROIs first)\n    return maps.raw_to_processed(self.dist_centers, self.params.smooth_width)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.get_raw_maps","title":"<code>get_raw_maps(force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get maps (occupancy, speed, spkmap) from session data by processing with provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>force_recompute</code> <code>bool</code> <p>Whether to force the recomputation of the maps even if they exist in the cache.</p> <code>False</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after getting the maps (only clears the onecache for this method)</p> <code>False</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for the maps. If None, the parameters will be taken from the SpkmapProcessor instance. If a dictionary, it will be used to update the parameters. Will always be temporary -- so the original parameters will be restored after the method is finished.</p> <code>None</code> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"raw_maps\", disable=False)\ndef get_raw_maps(\n    self,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get maps (occupancy, speed, spkmap) from session data by processing with provided parameters.\n\n    Parameters\n    ----------\n    force_recompute : bool, default=False\n        Whether to force the recomputation of the maps even if they exist in the cache.\n    clear_one_cache : bool, default=False\n        Whether to clear the onefile cache after getting the maps (only clears the onecache for this method)\n    params : SpkmapParams, dict, or None, default=None\n        Parameters for the maps. If None, the parameters will be taken from the SpkmapProcessor instance.\n        If a dictionary, it will be used to update the parameters.\n        Will always be temporary -- so the original parameters will be restored after the method is finished.\n    \"\"\"\n    dist_edges = self.dist_edges\n    dist_centers = self.dist_centers\n    num_positions = len(dist_centers)\n\n    # Get behavioral timestamps and positions\n    timestamps, positions, trial_numbers, idx_behave_to_frame = self.session.positions\n\n    # compute behavioral speed on each sample\n    within_trial_sample = np.append(np.diff(trial_numbers) == 0, True)\n    sample_duration = np.append(np.diff(timestamps), 0)\n    speeds = np.append(np.diff(positions) / sample_duration[:-1], 0)\n    # do this after division so no /0 errors\n    sample_duration = sample_duration * within_trial_sample\n    # speed 0 in last sample for each trial (it's undefined)\n    speeds = speeds * within_trial_sample\n    # Convert positions to position bins\n    position_bin = np.digitize(positions, dist_edges) - 1\n\n    # get imaging information\n    frame_time_stamps = self.session.timestamps\n    sampling_period = np.median(np.diff(frame_time_stamps))\n    dist_cutoff = sampling_period / 2\n    delay_position_to_imaging = frame_time_stamps[idx_behave_to_frame] - timestamps\n\n    # get spiking information\n    spks = self.session.spks\n    num_rois = self.session.get_value(\"numROIs\")\n\n    # Do standardization\n    if self.params.standardize_spks:\n        spks = median_zscore(spks, median_subtract=not self.session.zero_baseline_spks)\n\n    # Get high resolution occupancy and speed maps\n    dtype = np.float32\n    occmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    speedmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    spkmap = np.zeros((self.session.num_trials, num_positions, num_rois), dtype=dtype)\n    extra_counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n\n    # Get maps -- doing this independently for each map allows for more\n    # flexibility in which data to load (basically the occmap &amp; speedmap\n    # are instantaneous, but the spkmap is a bit slower)\n    get_summation_map(\n        sample_duration,\n        trial_numbers,\n        position_bin,\n        occmap,\n        counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=False,\n        use_sample_to_value_idx=False,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n    get_summation_map(\n        speeds,\n        trial_numbers,\n        position_bin,\n        speedmap,\n        counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=True,\n        use_sample_to_value_idx=False,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n    get_summation_map(\n        spks,\n        trial_numbers,\n        position_bin,\n        spkmap,\n        extra_counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=True,\n        use_sample_to_value_idx=True,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n\n    # Figure out the valid range (outside of this range, set the maps to nan, because their values are not meaningful)\n    position_bin_per_trial = [position_bin[trial_numbers == tnum] for tnum in range(self.session.num_trials)]\n\n    # offsetting by 1 because there is a bug in the vrControl software where the first sample is always set\n    # to the minimum position (which is 0), but if there is a built-up buffer in the rotary encoder, the position\n    # will jump at the second sample. In general this will always work unless the mice have a truly ridiculous\n    # speed at the beginning of the trial...\n    first_valid_bin = [np.min(bpb[1:] if len(bpb) &gt; 1 else bpb) for bpb in position_bin_per_trial]\n    last_valid_bin = [np.max(bpb) for bpb in position_bin_per_trial]\n\n    # set bins to nan when mouse didn't visit them\n    occmap = replace_missing_data(occmap, first_valid_bin, last_valid_bin)\n    speedmap = replace_missing_data(speedmap, first_valid_bin, last_valid_bin)\n    spkmap = replace_missing_data(spkmap, first_valid_bin, last_valid_bin)\n\n    return Maps.create_raw_maps(occmap, speedmap, spkmap)\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.get_reliability","title":"<code>get_reliability(use_session_filters=True, force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get the reliability of the maps</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"reliability\", disable=False)\ndef get_reliability(\n    self,\n    use_session_filters: bool = True,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n):\n    \"\"\"Get the reliability of the maps\"\"\"\n    envnum = helpers.check_iterable(self.session.environments)\n\n    # A list of the requested environments (all if not specified)\n    maps = self.get_env_maps(\n        use_session_filters=use_session_filters,\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n        params={\"autosave\": False},  # Prevent saving in the case of a recompute\n    )\n\n    # All reliability measures require no NaNs\n    maps.pop_nan_positions()\n\n    if self.params.reliability_method == \"leave_one_out\":\n        rel_values = [helpers.reliability_loo(spkmap) for spkmap in maps.spkmap]\n    elif self.params.reliability_method == \"correlation\" or self.params.reliability_method == \"mse\":\n        rel_mse, rel_cor = helpers.named_transpose([helpers.measureReliability(spkmap) for spkmap in maps.spkmap])\n        rel_values = rel_mse if self.params.reliability_method == \"mse\" else rel_cor\n    else:\n        raise ValueError(f\"Method {self.params.reliability_method} not supported\")\n\n    return Reliability(\n        np.stack(rel_values),\n        environments=envnum,\n        method=self.params.reliability_method,\n    )\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.load_from_cache","title":"<code>load_from_cache(data_type)</code>","text":"<p>Get the cached params and data for a given data type</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def load_from_cache(self, data_type: str) -&gt; Tuple[Union[Maps, Reliability], bool]:\n    \"\"\"Get the cached params and data for a given data type\"\"\"\n    cache_dir = self.cache_directory(data_type)\n    if cache_dir.exists():\n        # If the directory exists, check if there are any cached params that match the expected hash\n        params_hash = self._params_hash(data_type)\n        cached_params_path = cache_dir / f\"params_{params_hash}.npz\"\n        if cached_params_path.exists():\n            cached_params = dict(np.load(cached_params_path))\n            # Check if the cached params match the dependent params\n            if self.check_params_match(cached_params):\n                return self._load_from_cache(data_type, params_hash, params=cached_params), True\n    return None, False\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.save_cache","title":"<code>save_cache(data_type, data)</code>","text":"<p>Save the cached params and data for a given data type</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def save_cache(self, data_type: str, data: Union[Maps, Reliability]):\n    \"\"\"Save the cached params and data for a given data type\"\"\"\n    cache_dir = self.cache_directory(data_type)\n    params_hash = self._params_hash(data_type)\n    cache_param_path = cache_dir / f\"params_{params_hash}.npz\"\n    if not cache_dir.exists():\n        cache_dir.mkdir(parents=True, exist_ok=True)\n    np.savez(cache_param_path, **self.dependent_params(data_type))\n    if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n        for mapname in Maps.map_types():\n            cache_data_path = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n            np.save(cache_data_path, getattr(data, mapname))\n    elif data_type == \"env_maps\":\n        environments = data.environments\n        np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n        for ienv, env in enumerate(environments):\n            for mapname in Maps.map_types():\n                cache_data_path = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                np.save(cache_data_path, getattr(data, mapname)[ienv])\n    elif data_type == \"reliability\":\n        values = data.values\n        environments = data.environments\n        # don't need data.method because it's in params...\n        np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n        np.save(cache_dir / f\"data_reliability_{params_hash}.npy\", values)\n    else:\n        raise ValueError(f\"Unknown data type: {data_type}\")\n</code></pre>"},{"location":"api/processors/#vrAnalysis2.processors.SpkmapProcessor.show_cache","title":"<code>show_cache(data_type=None)</code>","text":"<p>Helper function that scrapes the cache directory and shows cached files</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Optional[str]</code> <p>Indicate a data type to filter which parts of the cache to show</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string showing cache information including data_type, size, parameters, and date</p> Source code in <code>vrAnalysis2/processors/spkmaps.py</code> <pre><code>def show_cache(self, data_type: Optional[str] = None) -&gt; str:\n    \"\"\"Helper function that scrapes the cache directory and shows cached files\n\n    Parameters\n    ----------\n    data_type: Optional[str] = None\n        Indicate a data type to filter which parts of the cache to show\n\n    Returns\n    -------\n    str\n        Formatted string showing cache information including data_type, size, parameters, and date\n    \"\"\"\n    import os\n    from datetime import datetime\n\n    # Get the base cache directory\n    base_cache_dir = self.cache_directory()\n\n    if not base_cache_dir.exists():\n        return f\"No cache directory found at: {base_cache_dir}\"\n\n    # Collect information about all cache files\n    cache_info = []\n\n    # Define the data types to check\n    if data_type is not None:\n        data_types_to_check = [data_type]\n    else:\n        data_types_to_check = [\"raw_maps\", \"processed_maps\", \"env_maps\", \"reliability\"]\n\n    for dt in data_types_to_check:\n        cache_dir = self.cache_directory(dt)\n        if not cache_dir.exists():\n            continue\n\n        # Find all parameter files (they define what caches exist)\n        param_files = list(cache_dir.glob(\"params_*.npz\"))\n\n        for param_file in param_files:\n            # Extract the hash from the filename\n            params_hash = param_file.stem.replace(\"params_\", \"\")\n\n            # Load the parameters\n            try:\n                cached_params = dict(np.load(param_file))\n                param_str = \", \".join([f\"{k}={v}\" for k, v in cached_params.items()])\n            except Exception as e:\n                param_str = f\"Error loading params: {e}\"\n\n            # Get file modification time\n            mod_time = datetime.fromtimestamp(param_file.stat().st_mtime)\n            date_str = mod_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            # Calculate total size of all related cache files\n            total_size = param_file.stat().st_size\n\n            if dt in [\"raw_maps\", \"processed_maps\"]:\n                # For maps, look for data files for each map type\n                for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                    data_file = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                    if data_file.exists():\n                        total_size += data_file.stat().st_size\n\n            elif dt == \"env_maps\":\n                # For env_maps, look for environment file and individual environment data files\n                env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                if env_file.exists():\n                    total_size += env_file.stat().st_size\n                    # Load environments to find all data files\n                    try:\n                        environments = np.load(env_file)\n                        for env in environments:\n                            for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                                data_file = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                                if data_file.exists():\n                                    total_size += data_file.stat().st_size\n                    except Exception:\n                        pass  # Continue even if we can't load environments\n\n            elif dt == \"reliability\":\n                # For reliability, look for environments and reliability data files\n                env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                rel_file = cache_dir / f\"data_reliability_{params_hash}.npy\"\n                if env_file.exists():\n                    total_size += env_file.stat().st_size\n                if rel_file.exists():\n                    total_size += rel_file.stat().st_size\n\n            # Convert size to human readable format\n            size_str = self._format_file_size(total_size)\n\n            cache_info.append(\n                {\n                    \"data_type\": dt,\n                    \"size\": size_str,\n                    \"parameters\": param_str,\n                    \"date\": date_str,\n                    \"hash\": params_hash[:8],  # Show first 8 chars of hash\n                }\n            )\n\n    if not cache_info:\n        return \"No cache files found.\"\n\n    # Format the output as a table\n    output_lines = []\n    output_lines.append(\"Cache Files Summary\")\n    output_lines.append(\"=\" * 80)\n    output_lines.append(f\"{'Data Type':&lt;15} {'Size':&lt;10} {'Date':&lt;20} {'Hash':&lt;10} {'Parameters'}\")\n    output_lines.append(\"-\" * 80)\n\n    for info in cache_info:\n        output_lines.append(f\"{info['data_type']:&lt;15} {info['size']:&lt;10} {info['date']:&lt;20} \" f\"{info['hash']:&lt;10} {info['parameters']}\")\n\n    output_lines.append(\"-\" * 80)\n    output_lines.append(f\"Total cache entries: {len(cache_info)}\")\n\n    result = \"\\n\".join(output_lines)\n    print(result)\n</code></pre>"},{"location":"api/registration/","title":"Registration API Reference","text":""},{"location":"api/registration/#vrAnalysis2.registration","title":"<code>registration</code>","text":""},{"location":"api/sessions/","title":"Sessions API Reference","text":""},{"location":"api/sessions/#vrAnalysis2.sessions","title":"<code>sessions</code>","text":""},{"location":"api/vrAnalysis2/","title":"vrAnalysis2 API Reference","text":""},{"location":"api/vrAnalysis2/#vrAnalysis2","title":"<code>vrAnalysis2</code>","text":""},{"location":"examples/registration/","title":"Registration Workflow Example","text":"<p>This example demonstrates a complete registration workflow for processing VR session data.</p>"},{"location":"examples/registration/#basic-registration","title":"Basic Registration","text":"<pre><code>from vrAnalysis2.registration import B2Registration\nfrom vrAnalysis2.sessions.b2session import B2RegistrationOpts\n\n# Create registration options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,\n    redCellProcessing=True,\n    neuropilCoefficient=0.7,\n    tau=1.5,\n    fs=6\n)\n\n# Create and run registration\nregistration = B2Registration(\n    mouse_name=\"mouse001\",\n    date_string=\"2024-01-15\",\n    session_id=\"001\",\n    opts=opts\n)\n\nregistration.register()\n</code></pre>"},{"location":"examples/registration/#batch-registration","title":"Batch Registration","text":"<p>Register multiple sessions from the database:</p> <pre><code>from vrAnalysis2.database import get_database\nfrom vrAnalysis2.sessions.b2session import B2RegistrationOpts\n\n# Get database\ndb = get_database(\"vrSessions\")\n\n# Find sessions needing registration\nneeds_reg = db.needs_registration(mouseName=\"mouse001\")\n\n# Create options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,\n    redCellProcessing=True\n)\n\n# Register each session\nfor _, row in needs_reg.iterrows():\n    try:\n        registration = db.make_b2registration(row, opts)\n        registration.register()\n\n        # Update database\n        db.update_database_field(\"vrRegistration\", True, uSessionID=row[\"uSessionID\"])\n        print(f\"Successfully registered: {registration.session_print()}\")\n    except Exception as e:\n        print(f\"Error registering {row['uSessionID']}: {e}\")\n        db.update_database_field(\"vrRegistrationError\", True, uSessionID=row[\"uSessionID\"])\n</code></pre>"},{"location":"examples/registration/#custom-registration-options","title":"Custom Registration Options","text":"<p>Use different options for different sessions:</p> <pre><code># Standard registration\nstandard_opts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    oasis=True,\n    tau=1.5\n)\n\n# High-resolution registration\nhighres_opts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    oasis=True,\n    tau=0.8,  # Higher temporal resolution\n    fs=10     # Higher sampling rate\n)\n\n# Apply based on session criteria\nif session_date &gt;= \"2024-01-01\":\n    opts = highres_opts\nelse:\n    opts = standard_opts\n</code></pre>"},{"location":"examples/session_analysis/","title":"Session Analysis Example","text":"<p>This example demonstrates how to perform analysis on a single session.</p>"},{"location":"examples/session_analysis/#loading-and-processing-data","title":"Loading and Processing Data","text":"<pre><code>from vrAnalysis2.sessions import create_b2session\nfrom vrAnalysis2.processors.spkmaps import SpkmapProcessor\n\n# Create session\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n\n# Load data\nsession.load_data()\n\n# Generate spike maps\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(\n    bin_size=5.0,\n    by_environment=True,\n    min_occupancy=0.1\n)\n\n# Access maps\nfor env_idx, env in enumerate(maps.environments):\n    occ_map = maps.occmap[env_idx]\n    spk_map = maps.spkmap[env_idx]\n\n    # Perform analysis on maps\n    # (analysis code here)\n</code></pre>"},{"location":"examples/session_analysis/#analyzing-place-cells","title":"Analyzing Place Cells","text":"<pre><code>from vrAnalysis2.syd.placecell_reliability import analyze_reliability\n\n# Analyze place cell reliability\nreliability = analyze_reliability(session)\n\n# Access results\nreliable_cells = reliability[\"reliable_cells\"]\nreliability_scores = reliability[\"scores\"]\n</code></pre>"},{"location":"examples/session_analysis/#visualizing-results","title":"Visualizing Results","text":"<pre><code>from vrAnalysis2.helpers.plotting import plot_spike_map\n\n# Plot spike maps for reliable cells\nfor roi_idx in reliable_cells:\n    plot_spike_map(\n        maps.spkmap[roi_idx],\n        maps.occmap,\n        title=f\"ROI {roi_idx}\"\n    )\n</code></pre>"},{"location":"modules/analysis/","title":"Analysis","text":"<p>The <code>vrAnalysis2.analysis</code> module provides tools for analyzing VR session data, including place cell analysis, reliability metrics, and plasticity measurements.</p>"},{"location":"modules/analysis/#analysis-modules","title":"Analysis Modules","text":""},{"location":"modules/analysis/#place-cell-reliability","title":"Place Cell Reliability","text":"<p>Analyze place cell reliability across sessions:</p> <pre><code>from vrAnalysis2.syd.placecell_reliability import analyze_reliability\n\n# Analyze reliability for a session\nreliability = analyze_reliability(session)\n</code></pre>"},{"location":"modules/analysis/#changing-place-fields","title":"Changing Place Fields","text":"<p>Track how place fields change across sessions:</p> <pre><code>from vrAnalysis2.syd.changing_placefields import analyze_changes\n\n# Analyze place field changes\nchanges = analyze_changes(session1, session2)\n</code></pre>"},{"location":"modules/analysis/#tracked-plasticity","title":"Tracked Plasticity","text":"<p>Analyze plasticity in tracked cells:</p> <pre><code>from vrAnalysis2.analysis.tracked_plasticity import analyze_plasticity\n\n# Analyze plasticity for tracked pairs\nplasticity = analyze_plasticity(tracked_pairs)\n</code></pre>"},{"location":"modules/analysis/#common-analysis-workflows","title":"Common Analysis Workflows","text":""},{"location":"modules/analysis/#single-session-analysis","title":"Single Session Analysis","text":"<pre><code>from vrAnalysis2.sessions import create_b2session\nfrom vrAnalysis2.processors.spkmaps import SpikeMapProcessor\n\n# Load session\nsession = create_b2session(\"mouse001\", \"2024-01-15\", \"001\")\nsession.load_data()\n\n# Generate spike maps\nprocessor = SpikeMapProcessor(session)\nmaps = processor.process(bin_size=5.0)\n\n# Perform analysis\n# (analysis code here)\n</code></pre>"},{"location":"modules/analysis/#multi-session-analysis","title":"Multi-Session Analysis","text":"<pre><code>from vrAnalysis2.multisession import MultiSession\nfrom vrAnalysis2.database import get_database\n\n# Get sessions from database\ndb = get_database(\"vrSessions\")\nsessions_data = db.get_table(mouseName=\"mouse001\")\n\n# Create multi-session object\nmulti = MultiSession(sessions_data)\n\n# Perform cross-session analysis\n# (analysis code here)\n</code></pre>"},{"location":"modules/analysis/#see-also","title":"See Also","text":"<ul> <li>Multi-session Analysis for cross-session workflows</li> <li>Tracking Module for cell tracking</li> <li>Processors Module for data processing</li> </ul>"},{"location":"modules/database/","title":"Database Management","text":"<p>The <code>vrAnalysis2.database</code> module provides classes and functions for managing VR session data in a database. It's designed to work with Microsoft Access databases (<code>.accdb</code> files) by default, but can be adapted to other SQL databases.</p>"},{"location":"modules/database/#core-classes","title":"Core Classes","text":""},{"location":"modules/database/#basedatabase","title":"BaseDatabase","text":"<p>The base class for database operations. Provides core functionality for connecting to databases, querying records, and updating data.</p> <p>Key Methods:</p> <ul> <li><code>get_table()</code>: Query records from the database table</li> <li><code>get_record()</code>: Retrieve a single record by unique identifiers</li> <li><code>update_database_field()</code>: Update field values for matching records</li> <li><code>add_record()</code>: Add new records to the database</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis2.database import get_database\n\n# Get a database instance\ndb = get_database(\"vrMice\")\n\n# Query records\nmice = db.get_table()\n\n# Get a specific record\nmouse = db.get_record(\"mouse001\")\n\n# Update a field\ndb.update_database_field(\"someField\", \"newValue\", mouseName=\"mouse001\")\n</code></pre>"},{"location":"modules/database/#sessiondatabase","title":"SessionDatabase","text":"<p>Specialized database class for managing VR sessions. Extends <code>BaseDatabase</code> with session-specific functionality.</p> <p>Key Methods:</p> <ul> <li><code>iter_sessions()</code>: Create B2Session objects from database records</li> <li><code>make_b2session()</code>: Create a B2Session from a database row</li> <li><code>make_b2registration()</code>: Create a B2Registration object for preprocessing</li> <li><code>needs_registration()</code>: Find sessions that need registration</li> <li><code>needs_s2p()</code>: Find sessions that need suite2p processing or QC</li> <li><code>check_s2p()</code>: Verify suite2p status consistency</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis2.database import get_database\n\n# Get session database\ndb = get_database(\"vrSessions\")\n\n# Find sessions needing registration\nneeds_reg = db.needs_registration(mouseName=\"mouse001\")\n\n# Create session objects\nsessions = db.iter_sessions(mouseName=\"mouse001\", sessionQC=True)\n\n# Check suite2p status\ndb.check_s2p(with_database_update=True)\n</code></pre>"},{"location":"modules/database/#configuration","title":"Configuration","text":"<p>Database configuration is managed through the <code>get_database_metadata()</code> function. You'll need to edit this function to match your database setup:</p> <pre><code>def get_database_metadata(db_name: str) -&gt; dict:\n    dbdict = {\n        \"vrSessions\": {\n            \"db_path\": r\"C:\\path\\to\\your\\database\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"sessiondb\",\n            \"uid\": \"uSessionID\",\n            \"backup_path\": r\"D:\\backup\\path\",\n            \"unique_fields\": [(\"mouseName\", str), (\"sessionDate\", datetime), (\"sessionID\", int)],\n            \"default_conditions\": {\"sessionQC\": True},\n            \"constructor\": SessionDatabase,\n        },\n        # ... other databases\n    }\n    return dbdict[db_name]\n</code></pre>"},{"location":"modules/database/#querying-data","title":"Querying Data","text":"<p>The <code>get_table()</code> method supports flexible querying:</p> <pre><code># Simple equality\ndf = db.get_table(mouseName=\"mouse001\")\n\n# Comparison operators\ndf = db.get_table(sessionID=(5, \"&gt;\"))  # sessionID &gt; 5\n\n# Multiple conditions (AND logic)\ndf = db.get_table(mouseName=\"mouse001\", imaging=True)\n\n# Disable default conditions\ndf = db.get_table(use_default=False, mouseName=\"mouse001\")\n</code></pre>"},{"location":"modules/database/#adding-records","title":"Adding Records","text":"<p>Use the GUI to add new records:</p> <pre><code>from vrAnalysis2.uilib.add_entry_gui import add_entry_gui\n\n# Open GUI for adding entries\nadd_entry_gui(\"vrSessions\")\n</code></pre> <p>Or programmatically:</p> <pre><code># Create insert statement\ncolumns = [\"mouseName\", \"sessionDate\", \"sessionID\", ...]\nvalues = [\"mouse001\", datetime(2024, 1, 15), 1, ...]\ninsert_stmt = f\"INSERT INTO {db.table_name} ({', '.join(columns)}) VALUES ({', '.join(['?'] * len(columns))})\"\n\n# Add record\ndb.add_record(insert_stmt, columns, values)\n</code></pre>"},{"location":"modules/database/#database-backup","title":"Database Backup","text":"<p>Automatically backup your database:</p> <pre><code>db.save_backup()\n</code></pre>"},{"location":"modules/database/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide for basic usage</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/helpers/","title":"Helpers","text":"<p>The <code>vrAnalysis2.helpers</code> module provides utility functions for common operations.</p>"},{"location":"modules/helpers/#helper-modules","title":"Helper Modules","text":""},{"location":"modules/helpers/#plotting","title":"Plotting","text":"<p>Utilities for creating plots:</p> <pre><code>from vrAnalysis2.helpers.plotting import plot_spike_map, plot_traces\n\n# Plot spike map\nplot_spike_map(spike_map, occupancy_map)\n\n# Plot calcium traces\nplot_traces(traces, time_axis)\n</code></pre>"},{"location":"modules/helpers/#signals","title":"Signals","text":"<p>Signal processing utilities:</p> <pre><code>from vrAnalysis2.helpers.signals import smooth, normalize\n\n# Smooth signal\nsmoothed = smooth(signal, window_size=5)\n\n# Normalize signal\nnormalized = normalize(signal)\n</code></pre>"},{"location":"modules/helpers/#indexing","title":"Indexing","text":"<p>Indexing utilities:</p> <pre><code>from vrAnalysis2.helpers.indexing import get_plane_indices\n\n# Get indices for specific plane\nindices = get_plane_indices(session, plane=0)\n</code></pre>"},{"location":"modules/helpers/#vr-support","title":"VR Support","text":"<p>VR-specific utilities:</p> <pre><code>from vrAnalysis2.helpers.vrsupport import get_environment_transitions\n\n# Get environment transition times\ntransitions = get_environment_transitions(behavior)\n</code></pre>"},{"location":"modules/helpers/#see-also","title":"See Also","text":"<ul> <li>Individual helper modules for specific functionality</li> <li>API Reference for complete function listings</li> </ul>"},{"location":"modules/multisession/","title":"Multi-Session Analysis","text":"<p>The <code>vrAnalysis2.multisession</code> module provides tools for analyzing data across multiple sessions, enabling population-level and longitudinal analyses.</p>"},{"location":"modules/multisession/#multisession-class","title":"MultiSession Class","text":"<p>The <code>MultiSession</code> class manages multiple sessions and provides cross-session analysis capabilities.</p> <pre><code>from vrAnalysis2.multisession import MultiSession\nfrom vrAnalysis2.database import get_database\n\n# Get sessions from database\ndb = get_database(\"vrSessions\")\nsessions_data = db.get_table(mouseName=\"mouse001\")\n\n# Create multi-session object\nmulti = MultiSession(sessions_data)\n\n# Access sessions\nsessions = multi.sessions\n</code></pre>"},{"location":"modules/multisession/#cross-session-analysis","title":"Cross-Session Analysis","text":"<p>Analyze data across sessions:</p> <pre><code># Analyze place field stability\nstability = multi.analyze_place_field_stability()\n\n# Analyze population activity\npopulation_activity = multi.analyze_population()\n\n# Compare sessions\ncomparison = multi.compare_sessions(session1_idx=0, session2_idx=1)\n</code></pre>"},{"location":"modules/multisession/#see-also","title":"See Also","text":"<ul> <li>Tracking Module for cell tracking</li> <li>Analysis Module for analysis tools</li> </ul>"},{"location":"modules/processors/","title":"Processors","text":"<p>The <code>vrAnalysis2.processors</code> module provides data processing pipelines that transform session data into analysis-ready formats.</p>"},{"location":"modules/processors/#spike-map-processor","title":"Spike Map Processor","text":"<p>The <code>SpikeMapProcessor</code> creates spatial representations of neural activity.</p>"},{"location":"modules/processors/#maps-class","title":"Maps Class","text":"<p>The <code>Maps</code> dataclass contains occupancy, speed, and spike maps:</p> <ul> <li><code>occmap</code>: Occupancy map (time spent in each spatial bin)</li> <li><code>speedmap</code>: Speed map (average speed in each spatial bin)</li> <li><code>spkmap</code>: Spike map (spike count in each spatial bin)</li> <li><code>by_environment</code>: Whether maps are separated by environment</li> <li><code>rois_first</code>: Whether ROI dimension comes first in spkmap</li> </ul>"},{"location":"modules/processors/#processing-spike-maps","title":"Processing Spike Maps","text":"<pre><code>from vrAnalysis2.processors.spkmaps import SpkmapProcessor\n\n# Create processor\nprocessor = SpkmapProcessor(session)\n\n# Process maps\nmaps = processor.process(\n    bin_size=5.0,  # 5 cm bins\n    by_environment=True,\n    rois_first=True,\n    min_occupancy=0.1  # Minimum occupancy threshold\n)\n\n# Access maps\nif maps.by_environment:\n    # Maps are lists, one per environment\n    for env_idx, env in enumerate(maps.environments):\n        occ = maps.occmap[env_idx]\n        spk = maps.spkmap[env_idx]\n        speed = maps.speedmap[env_idx]\nelse:\n    # Maps are single arrays\n    occ = maps.occmap\n    spk = maps.spkmap\n    speed = maps.speedmap\n</code></pre>"},{"location":"modules/processors/#map-averaging","title":"Map Averaging","text":"<p>Average maps across ROIs or sessions:</p> <pre><code># Average across ROIs\naveraged = maps.average_rois()\n\n# Average across environments\naveraged = maps.average_environments()\n</code></pre>"},{"location":"modules/processors/#map-visualization","title":"Map Visualization","text":"<p>Maps can be visualized using helper functions:</p> <pre><code>from vrAnalysis2.helpers.plotting import plot_spike_map\n\n# Plot spike map for a single ROI\nplot_spike_map(\n    maps.spkmap[roi_idx],\n    maps.occmap,\n    title=f\"ROI {roi_idx}\"\n)\n</code></pre>"},{"location":"modules/processors/#processing-options","title":"Processing Options","text":""},{"location":"modules/processors/#bin-size","title":"Bin Size","text":"<p>Control spatial resolution:</p> <pre><code># Fine bins (2 cm)\nfine_maps = processor.process(bin_size=2.0)\n\n# Coarse bins (10 cm)\ncoarse_maps = processor.process(bin_size=10.0)\n</code></pre>"},{"location":"modules/processors/#environment-separation","title":"Environment Separation","text":"<p>Separate maps by environment:</p> <pre><code># Separate by environment\nby_env = processor.process(by_environment=True)\n\n# Combined across environments\ncombined = processor.process(by_environment=False)\n</code></pre>"},{"location":"modules/processors/#occupancy-filtering","title":"Occupancy Filtering","text":"<p>Filter out low-occupancy bins:</p> <pre><code># Only include bins with &gt; 0.1 seconds occupancy\nfiltered = processor.process(min_occupancy=0.1)\n</code></pre>"},{"location":"modules/processors/#caching","title":"Caching","text":"<p>Processors cache results to speed up repeated operations:</p> <pre><code># First call processes data\nmaps1 = processor.process(bin_size=5.0)\n\n# Second call uses cache\nmaps2 = processor.process(bin_size=5.0)  # Fast!\n</code></pre>"},{"location":"modules/processors/#custom-processors","title":"Custom Processors","text":"<p>You can create custom processors by extending the base processor class:</p> <pre><code>from vrAnalysis2.processors.spkmaps import SpkmapProcessor\n\nclass CustomProcessor(SpikeMapProcessor):\n    def process_custom(self, **kwargs):\n        # Custom processing logic\n        maps = self.process(**kwargs)\n        # Additional processing\n        return processed_maps\n</code></pre>"},{"location":"modules/processors/#see-also","title":"See Also","text":"<ul> <li>Sessions Module for session data</li> <li>Analysis Module for analysis workflows</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/registration/","title":"Registration","text":"<p>The <code>vrAnalysis2.registration</code> module handles preprocessing and registration of experimental data. Registration aligns behavioral and imaging data, runs deconvolution, and prepares data for analysis.</p>"},{"location":"modules/registration/#core-classes","title":"Core Classes","text":""},{"location":"modules/registration/#b2registration","title":"B2Registration","text":"<p>Extends <code>B2Session</code> with registration workflows. Handles the complete preprocessing pipeline.</p> <p>Key Methods:</p> <ul> <li><code>register()</code>: Run the complete registration workflow</li> <li><code>register_behavior()</code>: Process and register behavioral data</li> <li><code>register_imaging()</code>: Process and register imaging data</li> <li><code>register_oasis()</code>: Run OASIS deconvolution</li> <li><code>register_redcell()</code>: Process red cell annotations</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis2.registration import B2Registration\nfrom vrAnalysis2.sessions.b2session import B2RegistrationOpts\n\n# Create registration options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,\n    redCellProcessing=True,\n    neuropilCoefficient=0.7,\n    tau=1.5,\n    fs=6\n)\n\n# Create registration object\nregistration = B2Registration(\n    mouse_name=\"mouse001\",\n    date_string=\"2024-01-15\",\n    session_id=\"001\",\n    opts=opts\n)\n\n# Run registration\nregistration.register()\n</code></pre>"},{"location":"modules/registration/#b2registrationopts","title":"B2RegistrationOpts","text":"<p>Dataclass for configuring registration options.</p> <p>Parameters:</p> <ul> <li><code>vrBehaviorVersion</code>: Version of vrControl software used (1 or 2)</li> <li><code>facecam</code>: Whether to process facecam data</li> <li><code>imaging</code>: Whether to process imaging data</li> <li><code>oasis</code>: Whether to run OASIS deconvolution</li> <li><code>moveRawData</code>: Whether to move raw data files</li> <li><code>redCellProcessing</code>: Whether to process red cell annotations</li> <li><code>clearOne</code>: Whether to clear One files</li> <li><code>neuropilCoefficient</code>: Coefficient for neuropil subtraction (default: 0.7)</li> <li><code>tau</code>: OASIS tau parameter (default: 1.5)</li> <li><code>fs</code>: Sampling frequency in Hz (default: 6)</li> </ul>"},{"location":"modules/registration/#registration-workflow","title":"Registration Workflow","text":"<p>The registration process includes:</p> <ol> <li>Behavior Registration: Load and process behavioral data from Timeline files</li> <li>Imaging Registration: Load suite2p outputs and process imaging data</li> <li>OASIS Deconvolution: Deconvolve calcium traces to get spike trains</li> <li>Red Cell Processing: Process red cell classifier results</li> <li>Data Alignment: Align behavioral and imaging data in time</li> </ol>"},{"location":"modules/registration/#behavior-processing","title":"Behavior Processing","text":"<p>Behavior processing handles different versions of vrControl:</p> <pre><code>from vrAnalysis2.registration.behavior import register_behavior\n\n# Process behavior for version 1\nregistration = register_behavior(registration, behavior_type=1)\n\n# Process behavior for version 2 (CR hippocampus)\nregistration = register_behavior(registration, behavior_type=2)\n</code></pre> <p>Different behavior versions may require different processing: - Position tracking - Reward delivery - Lick detection - Environment transitions</p>"},{"location":"modules/registration/#oasis-deconvolution","title":"OASIS Deconvolution","text":"<p>OASIS deconvolution converts calcium traces to spike trains:</p> <pre><code>from vrAnalysis2.registration.oasis import oasis_deconvolution\n\n# Run OASIS\ndeconvolved = oasis_deconvolution(\n    traces,\n    tau=1.5,\n    fs=6\n)\n</code></pre>"},{"location":"modules/registration/#red-cell-processing","title":"Red Cell Processing","text":"<p>Process red cell classifier results:</p> <pre><code>from vrAnalysis2.registration.redcell import RedCellProcessing\n\n# Create processor\nprocessor = RedCellProcessing(session)\n\n# Process red cells\nprocessor.process()\n</code></pre>"},{"location":"modules/registration/#database-integration","title":"Database Integration","text":"<p>Registration status is tracked in the database:</p> <pre><code>from vrAnalysis2.database import get_database\n\n# Get database\ndb = get_database(\"vrSessions\")\n\n# Find sessions needing registration\nneeds_reg = db.needs_registration(mouseName=\"mouse001\")\n\n# Run registration for each\nfor _, row in needs_reg.iterrows():\n    registration = db.make_b2registration(row, opts)\n    registration.register()\n\n    # Update database\n    db.update_database_field(\"vrRegistration\", True, uSessionID=row[\"uSessionID\"])\n</code></pre>"},{"location":"modules/registration/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide for basic usage</li> <li>Sessions Module for session management</li> <li>Examples for detailed workflows</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/sessions/","title":"Sessions","text":"<p>The <code>vrAnalysis2.sessions</code> module provides classes for loading and managing VR session data. Sessions are the core objects that represent individual experimental runs.</p>"},{"location":"modules/sessions/#core-classes","title":"Core Classes","text":""},{"location":"modules/sessions/#b2session","title":"B2Session","text":"<p>The main session class that loads and provides access to experimental data.</p> <p>Key Attributes:</p> <ul> <li><code>mouse_name</code>: Mouse identifier</li> <li><code>date</code>: Session date</li> <li><code>session_id</code>: Session identifier</li> <li><code>params</code>: Session parameters (B2SessionParams)</li> </ul> <p>Key Methods:</p> <ul> <li><code>load_data()</code>: Load behavioral and imaging data</li> <li><code>get_spks()</code>: Get spike data</li> <li><code>get_behavior()</code>: Get behavioral data</li> <li><code>session_print()</code>: Print session information</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis2.sessions import create_b2session\n\n# Create session with default parameters\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n\n# Load data\nsession.load_data()\n\n# Access spike data\nspks = session.get_spks()\n\n# Access behavioral data\nbehavior = session.get_behavior()\n</code></pre>"},{"location":"modules/sessions/#b2sessionparams","title":"B2SessionParams","text":"<p>Dataclass for configuring how session data is loaded.</p> <p>Parameters:</p> <ul> <li><code>spks_type</code>: Type of spike data to load (e.g., \"significant\")</li> <li><code>keep_planes</code>: List of plane indices to keep (None = all)</li> <li><code>good_labels</code>: List of ROI labels to keep (e.g., [\"c\", \"d\"])</li> <li><code>fraction_filled_threshold</code>: Threshold for ROI fraction filled</li> <li><code>footprint_size_threshold</code>: Threshold for ROI footprint size</li> <li><code>exclude_silent_rois</code>: Whether to exclude ROIs with no activity</li> <li><code>neuropil_coefficient</code>: Neuropil subtraction coefficient</li> <li><code>exclude_redundant_rois</code>: Whether to exclude redundant ROIs</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis2.sessions import create_b2session, B2SessionParams\n\n# Create custom parameters\nparams = B2SessionParams(\n    spks_type=\"significant\",\n    keep_planes=[0, 1, 2],\n    good_labels=[\"c\", \"d\"],\n    exclude_silent_rois=True\n)\n\n# Create session with custom parameters\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\",\n    params=params\n)\n</code></pre>"},{"location":"modules/sessions/#session-data-structure","title":"Session Data Structure","text":"<p>Sessions provide access to:</p>"},{"location":"modules/sessions/#behavioral-data","title":"Behavioral Data","text":"<ul> <li>Position tracking</li> <li>Velocity</li> <li>Rewards</li> <li>Licks</li> <li>Environment information</li> </ul>"},{"location":"modules/sessions/#imaging-data","title":"Imaging Data","text":"<ul> <li>Calcium traces (F, Fneu, etc.)</li> <li>Spike data (deconvolved or raw)</li> <li>ROI information</li> <li>Neuropil signals</li> <li>Suite2p statistics</li> </ul>"},{"location":"modules/sessions/#metadata","title":"Metadata","text":"<ul> <li>Session information</li> <li>Processing status</li> <li>Quality control flags</li> </ul>"},{"location":"modules/sessions/#loading-data","title":"Loading Data","text":"<p>Data is loaded lazily by default. Access data properties to trigger loading:</p> <pre><code># Data is loaded when accessed\nspks = session.spks  # Loads spike data\nbehavior = session.behavior  # Loads behavioral data\n</code></pre>"},{"location":"modules/sessions/#session-parameters","title":"Session Parameters","text":"<p>Session parameters control data filtering and processing:</p> <pre><code># Update parameters\nsession.params.update(\n    good_labels=[\"c\"],\n    exclude_silent_rois=True\n)\n\n# Reload data with new parameters\nsession.load_data()\n</code></pre>"},{"location":"modules/sessions/#file-paths","title":"File Paths","text":"<p>Sessions automatically construct paths to data files:</p> <pre><code># Suite2p path\ns2p_path = session.s2p_path\n\n# Timeline path\ntimeline_path = session.timeline_path\n\n# Session root\nsession_root = session.session_root\n</code></pre>"},{"location":"modules/sessions/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide for basic usage</li> <li>Registration Module for preprocessing workflows</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/tracking/","title":"Tracking","text":"<p>The <code>vrAnalysis2.tracking</code> module provides functionality for tracking cells across multiple sessions, enabling longitudinal analysis of individual neurons.</p>"},{"location":"modules/tracking/#cell-tracking","title":"Cell Tracking","text":"<p>Track the same cells across sessions:</p> <pre><code>from vrAnalysis2.tracking import TrackedPair\nfrom vrAnalysis2.sessions import create_b2session\n\n# Load two sessions\nsession1 = create_b2session(\"mouse001\", \"2024-01-15\", \"001\")\nsession2 = create_b2session(\"mouse001\", \"2024-01-16\", \"001\")\n\n# Create tracked pair\ntracked = TrackedPair(session1, session2)\n\n# Get matched pairs\nmatched_pairs = tracked.get_matched_pairs()\n\n# Access matched ROIs\nfor roi1_idx, roi2_idx in matched_pairs:\n    print(f\"ROI {roi1_idx} in session 1 matches ROI {roi2_idx} in session 2\")\n</code></pre>"},{"location":"modules/tracking/#tracking-methods","title":"Tracking Methods","text":"<p>Tracking uses spatial correlation and other metrics to match ROIs:</p> <pre><code># Get tracking metrics\nmetrics = tracked.get_tracking_metrics()\n\n# Access correlation scores\ncorrelations = metrics[\"correlation\"]\n\n# Access distance scores\ndistances = metrics[\"distance\"]\n</code></pre>"},{"location":"modules/tracking/#multi-session-tracking","title":"Multi-Session Tracking","text":"<p>Track cells across multiple sessions:</p> <pre><code>from vrAnalysis2.multisession import MultiSession\n\n# Create multi-session object\nmulti = MultiSession(sessions_data)\n\n# Track cells across all sessions\ntracked_cells = multi.track_cells()\n\n# Access tracking chains\nfor chain in tracked_cells:\n    # chain contains ROI indices for each session\n    print(f\"Cell tracked across {len(chain)} sessions\")\n</code></pre>"},{"location":"modules/tracking/#see-also","title":"See Also","text":"<ul> <li>Multi-session Analysis for cross-session workflows</li> <li>Analysis Module for tracked cell analysis</li> </ul>"}]}