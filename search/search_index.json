{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"vrAnalysis Documentation","text":"<p>Welcome to the documentation for vrAnalysis, a comprehensive Python package for processing and analyzing virtual reality (VR) behavioral and imaging experiments.</p>"},{"location":"#overview","title":"Overview","text":"<p>vrAnalysis is designed to work with: - Behavioral data from vrControl experiments - Imaging data processed with suite2p - Database management using Microsoft Access databases (or other SQL databases with minimal modifications) - Session tracking following the Alyx directory structure</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Database Management: Track and manage VR session data with an easy-to-use database interface</li> <li>Session Registration: Automated preprocessing of behavioral and imaging data</li> <li>Data Processing: Generate spike maps, occupancy maps, and other spatial representations</li> <li>Cell Tracking: Track cells across sessions for longitudinal analysis</li> <li>Multi-session Analysis: Analyze data across groups of sessions</li> <li>Quality Control: Built-in tools for session quality control and annotation</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Installation Guide - Get started with vrAnalysis</li> <li>Quickstart Tutorial - Learn the basics</li> <li>Module Documentation - Detailed module descriptions</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#package-structure","title":"Package Structure","text":"<p>vrAnalysis is organized into several main modules:</p> <ul> <li><code>database</code>: Database management and session tracking</li> <li><code>sessions</code>: Session data loading and management</li> <li><code>registration</code>: Data preprocessing and registration workflows</li> <li><code>processors</code>: Data processing pipelines (e.g., spike maps)</li> <li><code>analysis</code>: Analysis tools and utilities</li> <li><code>tracking</code>: Cell tracking across sessions</li> <li><code>multisession</code>: Multi-session analysis capabilities</li> <li><code>helpers</code>: Utility functions and helpers</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you encounter issues or have questions:</p> <ol> <li>Check the Quickstart Guide for common workflows</li> <li>Review the Module Documentation for specific features</li> <li>Consult the API Reference for detailed function signatures</li> </ol>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to vrAnalysis!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and clone the repository</li> <li>Create a conda environment:</li> </ol> <pre><code>conda env create -f environment.yml\nconda activate vrAnalysis\n</code></pre> <ol> <li>Install in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints where possible</li> <li>Write docstrings in NumPy format</li> <li>Keep line length to 150 characters (per <code>pyproject.toml</code>)</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Add docstrings to all public functions and classes</li> <li>Use NumPy-style docstrings</li> <li>Update relevant documentation pages when adding features</li> <li>Include examples in docstrings where helpful</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure existing tests pass</li> <li>Test with real data when possible</li> </ul>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ul> <li>Create a new branch for your changes</li> <li>Write clear commit messages</li> <li>Update documentation as needed</li> <li>Ensure all tests pass before submitting</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, please open an issue on GitHub.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide will help you install vrAnalysis and its dependencies.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher (note tested, might be even higher because of typing)</li> <li>Conda or Mamba (recommended for managing dependencies, use Mamba for faster installation!)</li> <li>Git (for cloning the repository)</li> </ul>"},{"location":"installation/#installation_1","title":"Installation","text":"<p>This method allows you to edit the code and is recommended if you plan to contribute or modify the package.</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/landoskape/vrAnalysis\ncd vrAnalysis\n</code></pre> <ol> <li>Create a conda environment from the provided <code>environment.yml</code>:</li> </ol> <pre><code>conda env create -f environment.yml\n# Or use mamba for faster installation:\nmamba env create -f environment.yml\n</code></pre> <ol> <li>Activate the environment:</li> </ol> <pre><code>conda activate vrAnalysis  # or whatever name is specified in environment.yml\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>After installation, you have to configure paths for your data:</p> <ol> <li>Data Directory: Set your local data path in <code>vrAnalysis/files.py</code>:</li> </ol> <pre><code>def local_data_path() -&gt; Path:\n    return Path(\"C:/path/to/your/data\")\n</code></pre> <ol> <li>Database Paths: Configure database paths in <code>vrAnalysis/database.py</code> using the <code>get_database_metadata()</code> function.</li> </ol>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that vrAnalysis is installed correctly:</p> <pre><code>import vrAnalysis\nfrom vrAnalysis.sessions import B2Session, create_b2session\nfrom vrAnalysis.database import get_database_metadata\n\nprint(\"vrAnalysis installed successfully!\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Database Connection Errors: Ensure you have the appropriate database drivers installed (e.g., Microsoft Access ODBC drivers for <code>.accdb</code> files).</p> </li> <li> <p>Import Errors: Make sure you've activated the correct conda environment and installed all dependencies.</p> </li> <li> <p>Path Issues: Verify that your data paths are correctly configured and accessible.</p> </li> </ol>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation, check out the Quickstart Guide to begin using vrAnalysis.</p>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#architecture","title":"Architecture","text":"<p>vrAnalysis is organized around a few core concepts:</p>"},{"location":"overview/#sessions","title":"Sessions","text":"<p>A session represents a single experimental run. Each session contains: - Behavioral data (position, velocity, rewards, etc.) - Imaging data (calcium traces, ROIs, etc.) - Metadata (mouse name, date, session ID, etc.)</p> <p>Sessions are represented by the <code>B2Session</code> class, which provides methods to load and access data.</p>"},{"location":"overview/#database","title":"Database","text":"<p>The database tracks all sessions and their metadata. It uses Microsoft Access (<code>.accdb</code>) files by default, but can be adapted to other SQL databases.</p> <p>The <code>SessionDatabase</code> class provides methods to: - Query sessions based on criteria - Add new sessions - Update session metadata - Track registration status and quality control flags</p>"},{"location":"overview/#registration","title":"Registration","text":"<p>Registration is the process of preprocessing raw experimental data. This includes: - Loading behavioral data from Timeline files - Processing imaging data from suite2p outputs - Running OASIS deconvolution - Processing red cell annotations - Aligning data in time</p> <p>Registration creates standardized data structures that can be used for analysis.</p>"},{"location":"overview/#processors","title":"Processors","text":"<p>Processors transform session data into analysis-ready formats. For example: - <code>SpkmapProcessor</code>: Creates spatial maps of neural activity - Other processors generate various representations of the data</p>"},{"location":"overview/#tracking","title":"Tracking","text":"<p>Tracking identifies the same cells across multiple sessions. This enables longitudinal analysis of how individual cells change over time.</p>"},{"location":"overview/#data-flow","title":"Data Flow","text":"<pre><code>Raw Data (Timeline, suite2p)\n    \u2193\nRegistration (B2Registration)\n    \u2193\nSession Data (B2Session)\n    \u2193\nProcessing (Processors)\n    \u2193\nAnalysis (Analysis Tools)\n</code></pre>"},{"location":"overview/#directory-structure","title":"Directory Structure","text":"<p>vrAnalysis expects data organized in an Alyx-style structure:</p> <pre><code>localData/\n\u251c\u2500\u2500 mouse001/\n\u2502   \u251c\u2500\u2500 2024-01-15/\n\u2502   \u2502   \u251c\u2500\u2500 001/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 suite2p/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 timeline/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 002/\n\u2502   \u2514\u2500\u2500 2024-01-16/\n\u2514\u2500\u2500 mouse002/\n</code></pre>"},{"location":"overview/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Session-Centric: All operations revolve around session objects</li> <li>Database-Driven: Session metadata is tracked in a database</li> <li>Lazy Loading: Data is loaded on-demand to minimize memory usage</li> <li>Caching: Intermediate results are cached to speed up repeated operations</li> <li>Modularity: Each component can be used independently</li> </ol>"},{"location":"overview/#common-workflows","title":"Common Workflows","text":""},{"location":"overview/#1-daily-registration-workflow","title":"1. Daily Registration Workflow","text":"<ol> <li>Add new sessions to database using GUI</li> <li>Run registration for new sessions</li> <li>Review and QC registered sessions</li> <li>Update database with QC status</li> </ol>"},{"location":"overview/#2-analysis-workflow","title":"2. Analysis Workflow","text":"<ol> <li>Query database for sessions of interest</li> <li>Load sessions into <code>B2Session</code> objects</li> <li>Process data using processors</li> <li>Perform analysis</li> <li>Visualize and save results</li> </ol>"},{"location":"overview/#3-longitudinal-analysis-workflow","title":"3. Longitudinal Analysis Workflow","text":"<ol> <li>Query database for sessions from same mouse</li> <li>Load sessions and track cells across sessions</li> <li>Analyze changes in tracked cells</li> <li>Compare across experimental conditions</li> </ol>"},{"location":"overview/#integration-with-other-tools","title":"Integration with Other Tools","text":"<p>vrAnalysis integrates with: - suite2p: For calcium imaging data processing - vrControl: For behavioral data collection - Timeline (Rigbox): For experimental event tracking - ROICaT: For ROI classification (via <code>roicat_support</code>) - OASIS: For calcium trace deconvolution</p>"},{"location":"overview/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Quickstart Guide for hands-on examples</li> <li>Explore Module Documentation for detailed information</li> <li>Check the API Reference for complete function signatures</li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will walk you through the basic workflows in vrAnalysis.</p>"},{"location":"quickstart/#creating-a-session","title":"Creating a Session","text":"<p>The core object in vrAnalysis is the <code>B2Session</code>, which represents a single experimental session.</p>"},{"location":"quickstart/#basic-session-creation","title":"Basic Session Creation","text":"<pre><code>from vrAnalysis.sessions import create_b2session\n\n# Create a session with default parameters\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n</code></pre>"},{"location":"quickstart/#custom-session-parameters","title":"Custom Session Parameters","text":"<p>You can customize how data is loaded using <code>B2SessionParams</code>:</p> <pre><code>from vrAnalysis.sessions import create_b2session\n\n# Create a session with custom parameters\nparams = {\n    \"spks_type\": \"significant\",  # Use significant transients\n    \"keep_planes\": [0, 1, 2],    # Only load specific planes\n    \"good_labels\": [\"c\", \"d\"],   # Keep only \"c\" and \"d\" classified ROIs\n    \"exclude_silent_rois\": True   # Exclude ROIs with no activity\n}\n\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\",\n    params=params\n)\n</code></pre>"},{"location":"quickstart/#working-with-the-database","title":"Working with the Database","text":""},{"location":"quickstart/#querying-sessions","title":"Querying Sessions","text":"<pre><code>from vrAnalysis.database import get_database\n\n# Get database metadata\ndb = get_database(\"vrSessions\")\n\n# Query sessions\nsessions_df = db.get_table(mouseName=\"mouse001\", sessionQC=True)\n\n# Access session data\nfor _, session_row in sessions_df.iterrows():\n    print(f\"Session: {session_row['sessionDate']} - {session_row['sessionID']}\")\n\n# Or create session objects directly\nsessions = db.iter_sessions(mouseName=\"mouse001\", sessionQC=True)\nfor session in sessions:\n    print(f\"Session: {session.session_print()}\")\n</code></pre>"},{"location":"quickstart/#adding-sessions-to-database","title":"Adding Sessions to Database","text":"<p>Use the GUI to add new sessions:</p> <pre><code>from vrAnalysis.uilib.add_entry_gui import add_entry_gui\n\n# Open the database entry GUI\nadd_entry_gui(\"vrSessions\")\n</code></pre>"},{"location":"quickstart/#registration-workflow","title":"Registration Workflow","text":"<p>Registration is the process of preprocessing and aligning behavioral and imaging data.</p> <pre><code>from vrAnalysis.registration import B2Registration\nfrom vrAnalysis.sessions.b2session import B2RegistrationOpts\n\n# Create registration options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,  # Run OASIS deconvolution\n    redCellProcessing=True,\n    neuropilCoefficient=0.7,\n    tau=1.5,\n    fs=6\n)\n\n# Create registration object\nregistration = B2Registration(\n    mouse_name=\"mouse001\",\n    date_string=\"2024-01-15\",\n    session_id=\"001\",\n    opts=opts\n)\n\n# Run registration\nregistration.register()\n</code></pre>"},{"location":"quickstart/#processing-spike-maps","title":"Processing Spike Maps","text":"<p>Generate spatial representations of neural activity:</p> <pre><code>from vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Create processor\nprocessor = SpkmapProcessor(session)\n\n# Generate maps\nmaps = processor.process(\n    bin_size=5.0,  # 5 cm bins\n    by_environment=True,  # Separate by environment\n    rois_first=True\n)\n\n# Access maps\noccupancy_map = maps.occmap\nspike_map = maps.spkmap\nspeed_map = maps.speedmap\n</code></pre>"},{"location":"quickstart/#tracking-cells-across-sessions","title":"Tracking Cells Across Sessions","text":"<p>Track the same cells across multiple sessions:</p> <pre><code>from vrAnalysis.tracking import Tracker\n\n# Create tracker for a mouse (tracks all sessions for that mouse)\ntracker = Tracker(\"mouse001\")\n\n# Get tracked ROIs across sessions\nidx_tracked, extras = tracker.get_tracked_idx(\n    idx_ses=[0, 1],  # Track between first two sessions\n    use_session_filters=True\n)\n\n# idx_tracked is a (num_sessions, num_tracked_rois) array\n# Each column represents a tracked ROI across sessions\nfor roi_idx in range(idx_tracked.shape[1]):\n    session1_roi = idx_tracked[0, roi_idx]\n    session2_roi = idx_tracked[1, roi_idx]\n    print(f\"ROI {session1_roi} in session 1 matches ROI {session2_roi} in session 2\")\n</code></pre>"},{"location":"quickstart/#multi-session-analysis","title":"Multi-Session Analysis","text":"<p>Analyze data across multiple sessions:</p> <pre><code>from vrAnalysis.multisession import MultiSessionSpkmaps\nfrom vrAnalysis.tracking import Tracker\n\n# Create tracker for a mouse\ntracker = Tracker(\"mouse001\")\n\n# Create multi-session object for spike map analysis\nmulti = MultiSessionSpkmaps(tracker)\n\n# Perform analysis across sessions\n# (specific analysis methods depend on your needs)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Database Management</li> <li>Explore Session Configuration</li> <li>Understand Registration Workflows</li> <li>Check out Analysis Tools</li> </ul>"},{"location":"registration/","title":"Registration","text":"<p>Registration is the process of preprocessing and preparing experimental data from VR sessions for analysis. This includes processing behavioral data from Timeline and vrControl, aligning imaging data from suite2p, running deconvolution algorithms, and creating a unified data structure for analysis.</p>"},{"location":"registration/#overview","title":"Overview","text":"<p>The registration process transforms raw experimental data into a standardized format that can be used for analysis. It handles both behavioral and imaging data. It handles:</p> <ul> <li>Timeline Data: Raw hardware signals from Rigbox (rotary encoder, photodiode, lick detector, reward commands)</li> <li>Behavioral Data: VR environment data from vrControl (position tracking, trial information, rewards, licks)</li> <li>Imaging Data: Calcium imaging data from suite2p (ROI fluorescence traces, deconvolved spikes)</li> <li>Data Alignment: Synchronizing behavioral and imaging data in time</li> </ul> <p>The registration process produces \"onedata\" files - standardized NumPy arrays stored in the session's <code>onedata/</code> directory that can be easily loaded for analysis.</p>"},{"location":"registration/#prerequisites","title":"Prerequisites","text":"<p>Before registering a session, several prerequisites must be met:</p>"},{"location":"registration/#1-database-setup","title":"1. Database Setup","text":"<p>You must have a SQL database (preferably Microsoft Access) set up with a table containing session information. The table should have the following columns (not all are required):</p> <p>Required Columns:</p> <ul> <li><code>uSessionID</code> (int): Unique session identifier</li> <li><code>mouseName</code> (str): Mouse identifier</li> <li><code>sessionDate</code> (datetime): Session date</li> <li><code>sessionID</code> (int): Session ID number</li> <li><code>vrRegistration</code> (bool): Registration status flag</li> <li><code>imaging</code> (bool): Whether session has imaging data</li> <li><code>behavior</code> (bool): Whether session has behavioral data</li> <li><code>faceCamera</code> (bool): Whether session has face camera data</li> <li><code>vrBehaviorVersion</code> (int): Version of vrControl software used (1 or 2)</li> </ul> <p>Optional Columns:</p> <ul> <li><code>sessionQC</code> (bool): Quality control flag</li> <li><code>experimentType</code> (str)</li> <li><code>experimentID</code> (int)</li> <li><code>variableGain</code> (bool)</li> <li><code>vrEnvironments</code> (int)</li> <li><code>headPlateRotation</code> (float)</li> <li><code>numPlanes</code> (int)</li> <li><code>planeSeparation</code> (float)</li> <li><code>pockelsPercentage</code> (float)</li> <li><code>objectiveRotation</code> (float)</li> <li><code>suite2p</code> (bool)</li> <li><code>suite2pQC</code> (bool)</li> <li><code>redCellQC</code> (bool)</li> <li><code>scratchJustification</code> (str)</li> <li><code>logtime</code> (datetime)</li> <li><code>sessionNotes</code> (str)</li> <li><code>suite2pDate</code> (datetime)</li> <li><code>vrRegistrationDate</code> (datetime)</li> <li><code>vrRegistrationError</code> (bool)</li> <li><code>vrRegistrationException</code> (str)</li> <li><code>redCellQCDate</code> (datetime)</li> <li><code>dontTrack</code> (bool)</li> </ul>"},{"location":"registration/#2-adding-sessions-to-database","title":"2. Adding Sessions to Database","text":"<p>The best way to add a new session entry to the database is using the GUI. The GUI will automatically build a form based on the database schema, so this will actually work for any database object you provide. </p> <pre><code>  from vrAnalysis.database import get_database\n  from vrAnalysis.uilib.add_entry_gui import NewEntryGUI\n\n  # Get database\n  db = get_database(\"vrSessions\")\n\n  # Optionally create a session object to auto-populate fields\n  from vrAnalysis.sessions import create_b2session\n  session = create_b2session(\"mouse001\", \"2024-01-15\", \"001\")\n\n  # any other parameters you want to use to preload the GUI form.\n  other_params = {} \n\n  # Open GUI to add entry\n  gui = NewEntryGUI(db, ses=session, **other_params)\n</code></pre> <p>The GUI will automatically populate fields from the session object if provided, and validate all inputs before submission.</p>"},{"location":"registration/#3-suite2p-processing-for-imaging-sessions","title":"3. Suite2p Processing (for imaging sessions)","text":"<p>If the session has imaging data, suite2p must be run before registration. The suite2p output directory should be located at:</p> <pre><code>{session_path}/suite2p/\n</code></pre> <p>Where <code>session_path</code> follows the conventional structure: <code>{local_data_path}/{mouse_name}/{date}/{session_id}/</code></p> <p>The suite2p directory should contain subdirectories for each imaging plane (e.g., <code>plane0/</code>, <code>plane1/</code>, etc.), each containing the standard suite2p output files (<code>.npy</code> files like <code>F.npy</code>, <code>Fneu.npy</code>, <code>spks.npy</code>, <code>stat.npy</code>, <code>ops.npy</code>, <code>iscell.npy</code>).</p>"},{"location":"registration/#4-required-input-files","title":"4. Required Input Files","text":"<p>The session directory must contain the following files. Note that they are determined by other software - including both vrControl and Rigbox.</p> <p>Timeline Data: - <code>{date}_{session_id}_{mouse_name}_Timeline.mat</code>: Contains raw hardware signals from Rigbox</p> <p>Behavioral Data: - <code>{date}_{session_id}_{mouse_name}_VRBehavior_trial.mat</code>: Contains VR environment data from vrControl</p> <p>These files are typically generated during the experiment by Rigbox and vrControl software.</p>"},{"location":"registration/#data-structure","title":"Data Structure","text":""},{"location":"registration/#timeline-data","title":"Timeline Data","text":"<p>The Timeline.mat file contains hardware signals recorded by Rigbox:</p> <ul> <li>timestamps: Raw DAQ timestamps for all signals</li> <li>rotaryEncoder: Rotary encoder position counter (circular, needs conversion to linear position)</li> <li>photoDiode: Photodiode signal for detecting visual stimulus flips</li> <li>lickDetector: Edge counter signal for detecting licks</li> <li>rewardCommand: Voltage signal indicating reward delivery</li> <li>neuralFrames: Frame counter from ScanImage indicating imaging volume acquisition</li> <li>mpepUDPTimes/mpepUDPEvents: Trial start/end messages from vrControl</li> </ul>"},{"location":"registration/#behavioral-data","title":"Behavioral Data","text":"<p>The VRBehavior_trial.mat file contains VR environment data. The structure varies by <code>vrBehaviorVersion</code>:</p> <p>Version 1 (standard_behavior):</p> <ul> <li><code>expInfo</code>: Experiment information (room length, movement gain per trial)</li> <li><code>trialInfo</code>: Sparse matrices containing trial-by-trial data:</li> <li><code>time</code>: Timestamps for each behavioral sample</li> <li><code>roomPosition</code>: Virtual position in VR environment</li> <li><code>rewardPosition</code>: Reward zone position</li> <li><code>rewardTolerance</code>: Reward zone half-width</li> <li><code>rewardAvailable</code>: Whether reward was available</li> <li><code>rewardDeliveryFrame</code>: Frame when reward was delivered</li> <li><code>activeLicking</code>: Whether active licking was required</li> <li><code>activeStopping</code>: Whether active stopping was required</li> <li><code>lick</code>: Lick counts per frame</li> <li><code>vrEnvIdx</code>: Environment index (optional)</li> </ul> <p>Version 2 (cr_hippocannula_behavior):</p> <ul> <li>Similar structure but with different field names (<code>TRIAL</code> instead of <code>trialInfo</code>, <code>EXP</code> instead of <code>expInfo</code>)</li> </ul> <p>Both versions also contain <code>rigInfo</code> with hardware configuration:</p> <ul> <li><code>rotEncPos</code>: Rotary encoder position (\"left\" or \"right\")</li> <li><code>rotEncSign</code>: Sign of rotary encoder (-1 or 1)</li> <li><code>wheelToVR</code>: Conversion factor from wheel counts to VR units</li> <li><code>wheelRadius</code>: Physical wheel radius in cm</li> <li><code>rotaryRange</code>: Bit range of rotary encoder (typically 32)</li> </ul>"},{"location":"registration/#imaging-data","title":"Imaging Data","text":"<p>Suite2p outputs are organized by plane:</p> <pre><code>suite2p/\n  plane0/\n    F.npy          # Fluorescence traces (nROIs x nFrames)\n    Fneu.npy       # Neuropil fluorescence (nROIs x nFrames)\n    spks.npy       # Deconvolved spikes (nROIs x nFrames)\n    stat.npy        # ROI statistics (list of dicts)\n    ops.npy         # Suite2p options (dict)\n    iscell.npy      # Cell classifier output (nROIs x 2)\n    redcell.npy     # Red cell classifier output (optional, nROIs x 2)\n</code></pre>"},{"location":"registration/#using-the-database-pipeline","title":"Using the Database Pipeline","text":"<p>The recommended way to register sessions is through the database pipeline, which handles error tracking and status updates automatically. The database object for sessions includes some supporting methods that make it easy to identify sessions that need registration and register them all at once.</p>"},{"location":"registration/#finding-sessions-needing-registration","title":"Finding Sessions Needing Registration","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Get DataFrame of sessions needing registration\n  needs_reg = db.needs_registration(return_df=True)\n\n  # Filter by mouse\n  needs_reg = db.needs_registration(mouseName=\"mouse001\", return_df=True)\n\n  # Print sessions (alternative to DataFrame)\n  db.needs_registration(return_df=False, mouseName=\"mouse001\")\n</code></pre>"},{"location":"registration/#registering-a-single-session","title":"Registering a Single Session","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Register by identifiers\n  success = db.register_single_session(\n      mouse_name=\"mouse001\",\n      session_date=\"2024-01-15\",\n      session_id=\"001\",\n  )\n\n  if success:\n      print(\"Registration successful!\")\n  else:\n      print(\"Registration failed - check database for error details\")\n</code></pre>"},{"location":"registration/#registering-multiple-sessions","title":"Registering Multiple Sessions","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Register all sessions needing registration\n  # Stops when total data size exceeds max_data (default 30 GB)\n  db.register_sessions(\n      max_data=30e9,        # Maximum total data to process (bytes)\n      skip_errors=True,     # Skip sessions with previous errors\n      raise_exception=False, # Don't raise exceptions on failure\n  )\n</code></pre> <p>The <code>register_sessions</code> method provides progress updates including: - Accumulated onedata registered - Average data size per session - Estimated remaining data to process</p>"},{"location":"registration/#error-handling","title":"Error Handling","text":"<p>When registration fails, the database is automatically updated:</p> <ul> <li><code>vrRegistrationError</code> set to <code>True</code></li> <li><code>vrRegistrationException</code> contains the error message</li> <li>All onedata files are cleared</li> <li>Registration status remains <code>False</code></li> </ul> <p>You can check for errors:</p> <pre><code>  # Print all registration errors\n  db.print_registration_errors()\n\n  # Get sessions with errors\n  errors = db.get_table(vrRegistrationError=True)\n</code></pre> <p>If you try to register sessions without setting the <code>skip_errors</code> parameter to <code>False</code>, the bulk registration pipeline will simply skip sessions that had previous errors. You probably don't want this unless you want to abandon the session! So use the <code>needs_registration</code> method above and the <code>kwarg</code> <code>vrRegistrationError=True</code> to identify sessions that had errors. You'll need to debug them yourself.</p>"},{"location":"registration/#manual-registration","title":"Manual Registration","text":"<p>You can also register sessions directly without using the database:</p> <pre><code>  from vrAnalysis.registration import B2Registration\n  from vrAnalysis.sessions.b2session import B2RegistrationOpts\n\n  # Create registration options\n  opts = B2RegistrationOpts(\n      vrBehaviorVersion=1,      # Version of vrControl (1 or 2)\n      facecam=False,             # Process face camera data\n      imaging=True,              # Process imaging data\n      oasis=True,                # Run OASIS deconvolution\n      redCellProcessing=True,   # Process red cell features\n      clearOne=True,            # Clear existing onedata before registration\n      neuropilCoefficient=0.7,  # Neuropil subtraction coefficient\n      tau=1.5,                  # OASIS tau parameter (seconds)\n      fs=6                      # Sampling frequency (Hz)\n  )\n\n  # Create registration object\n  registration = B2Registration(\n      mouse_name=\"mouse001\",\n      date_string=\"2024-01-15\",\n      session_id=\"001\",\n      opts=opts\n  )\n\n  # Run registration\n  registration.register()\n\n  # Save session parameters (automatically called by register())\n  # registration.save_session_prms()\n</code></pre>"},{"location":"registration/#registration-options","title":"Registration Options","text":"<p>The <code>B2RegistrationOpts</code> dataclass controls registration behavior:</p> <p>Behavior Options: - <code>vrBehaviorVersion</code> (int): Version of vrControl software (1=standard, 2=CR hippocampus, software no longer exists!)     - Note: this system will allow you to use this pipeline with new behavior software, just add a new version number and a new behavior processing function in the <code>vrAnalysis.registration.behavior</code> module.</p> <p>Data Processing Flags:</p> <ul> <li><code>imaging</code> (bool): Process imaging data (requires suite2p directory)</li> <li><code>facecam</code> (bool): Process face camera data (not yet implemented)</li> <li><code>redCellProcessing</code> (bool): Compute red cell features</li> </ul> <p>Deconvolution Options:</p> <ul> <li><code>oasis</code> (bool): Run OASIS deconvolution (recomputes spikes from corrected fluorescence)</li> <li><code>tau</code> (float): OASIS decay time constant in seconds (default: 1.5)</li> <li><code>fs</code> (int): Sampling frequency in Hz (default: 6)</li> <li><code>neuropilCoefficient</code> (float): Coefficient for neuropil subtraction (default: 0.7)</li> </ul> <p>Data Management:</p> <ul> <li><code>clearOne</code> (bool): Clear existing onedata before registration (default: True)</li> </ul>"},{"location":"registration/#output-data-structure","title":"Output Data Structure","text":"<p>After registration, the session's <code>onedata/</code> directory contains standardized NumPy arrays:</p>"},{"location":"registration/#timeline-data_1","title":"Timeline Data","text":"<ul> <li><code>wheelPosition.times.npy</code>: Timeline timestamps</li> <li><code>wheelPosition.position.npy</code>: Wheel position in cm</li> <li><code>licks.times.npy</code>: Lick event timestamps</li> <li><code>rewards.times.npy</code>: Reward delivery timestamps</li> <li><code>trials.startTimes.npy</code>: Trial start timestamps</li> </ul>"},{"location":"registration/#behavioral-data_1","title":"Behavioral Data","text":"<ul> <li><code>positionTracking.times.npy</code>: Behavioral timestamps</li> <li><code>positionTracking.position.npy</code>: Virtual position in VR environment</li> <li><code>positionTracking.mpci.npy</code>: Mapping from behavioral samples to imaging frames</li> <li><code>trials.positionTracking.npy</code>: Start frame index for each trial</li> <li><code>trials.environmentIndex.npy</code>: Environment ID for each trial</li> <li><code>trials.roomlength.npy</code>: Room length for each trial</li> <li><code>trials.movementGain.npy</code>: Movement gain for each trial</li> <li><code>trials.rewardPosition.npy</code>: Reward zone position</li> <li><code>trials.rewardZoneHalfwidth.npy</code>: Reward zone half-width</li> <li><code>trials.rewardAvailability.npy</code>: Whether reward was available</li> <li><code>trials.rewardPositionTracking.npy</code>: Reward delivery frame index</li> <li><code>trials.activeLicking.npy</code>: Active licking requirement</li> <li><code>trials.activeStopping.npy</code>: Active stopping requirement</li> <li><code>licksTracking.positionTracking.npy</code>: Lick event indices in behavioral samples</li> </ul>"},{"location":"registration/#imaging-data_1","title":"Imaging Data","text":"<ul> <li><code>mpci.times.npy</code>: Imaging frame timestamps</li> <li><code>mpci.roiActivityF.npy</code>: Fluorescence traces (or LoadingRecipe reference)</li> <li><code>mpci.roiNeuropilActivityF.npy</code>: Neuropil fluorescence (or LoadingRecipe reference)</li> <li><code>mpci.roiActivityDeconvolved.npy</code>: Suite2p deconvolved spikes (or LoadingRecipe reference)</li> <li><code>mpci.roiActivityDeconvolvedOasis.npy</code>: OASIS deconvolved spikes (if computed)</li> <li><code>mpciROIs.isCell.npy</code>: Cell classifier output</li> <li><code>mpciROIs.stackPosition.npy</code>: ROI positions (nROIs x 3: x, y, plane)</li> <li><code>mpciROIs.redS2P.npy</code>: Red cell classifier output (if available)</li> </ul>"},{"location":"registration/#red-cell-data-if-processed","title":"Red Cell Data (if processed)","text":"<ul> <li><code>mpciROIs.redDotProduct.npy</code>: Dot product feature</li> <li><code>mpciROIs.redPearson.npy</code>: Pearson correlation feature</li> <li><code>mpciROIs.redPhaseCorrelation.npy</code>: Phase correlation feature</li> <li><code>mpciROIs.redCellIdx.npy</code>: Boolean array for red cell identification</li> <li><code>mpciROIs.redCellManualAssignments.npy</code>: Manual assignment array</li> <li><code>parametersRedDotProduct.keyValuePairs.npy</code>: Dot product parameters</li> <li><code>parametersRedPearson.keyValuePairs.npy</code>: Pearson parameters</li> <li><code>parametersRedPhaseCorrelation.keyValuePairs.npy</code>: Phase correlation parameters</li> </ul>"},{"location":"registration/#session-metadata","title":"Session Metadata","text":"<ul> <li><code>vrExperimentOptions.json</code>: Registration options used</li> <li><code>vrExperimentPreprocessing.json</code>: List of preprocessing steps completed</li> <li><code>vrExperimentValues.json</code>: Session metadata values (numTrials, numROIs, etc.)</li> </ul>"},{"location":"registration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"registration/#common-issues","title":"Common Issues","text":"<p>\"Session directory does not exist\"</p> <ul> <li>Ensure the session directory follows the expected structure: <code>{local_data_path}/{mouse_name}/{date}/{session_id}/</code></li> <li>Check that <code>local_data_path()</code> is configured correctly</li> </ul> <p>\"suite2p directory does not exist\"</p> <ul> <li>Run suite2p processing before registration</li> <li>Ensure suite2p output is in <code>{session_path}/suite2p/</code></li> </ul> <p>\"Missing required suite2p files\"</p> <ul> <li>Verify all planes have the required <code>.npy</code> files (F, Fneu, spks, stat, ops, iscell)</li> <li>Check for typos in file names</li> </ul> <p>\"Frame count mismatch\"</p> <ul> <li>The registration process attempts to handle minor mismatches automatically</li> <li>Large mismatches may indicate a problem with the Timeline neuralFrames signal</li> <li>Check that ScanImage TTL signals were properly recorded</li> </ul> <p>\"Behavior type not supported\"</p> <ul> <li>Ensure <code>vrBehaviorVersion</code> matches the version of vrControl used</li> <li>Currently supported: 1 (standard), 2 (CR hippocampus)</li> </ul> <p>\"First flips in trial are not all down\"</p> <ul> <li>This assertion only applies to sessions after 2022-08-30</li> <li>May indicate a problem with photodiode signal or trial preparation</li> </ul>"},{"location":"registration/#checking-registration-status","title":"Checking Registration Status","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Check if session is registered\n  record = db.get_record(\"mouse001\", \"2024-01-15\", \"001\")\n  if record is not None:\n      print(f\"Registration status: {record['vrRegistration']}\")\n      print(f\"Registration date: {record['vrRegistrationDate']}\")\n      if record['vrRegistrationError']:\n          print(f\"Error: {record['vrRegistrationException']}\")\n</code></pre>"},{"location":"registration/#see-also","title":"See Also","text":"<ul> <li>Database Module for database management</li> <li>Sessions Module for session data loading</li> <li>Registration Module for API reference</li> <li>Registration Examples for detailed workflows</li> </ul>"},{"location":"api/analysis/","title":"Analysis API Reference","text":"<p>The analysis functionality is distributed across several modules:</p> <ul> <li><code>vrAnalysis.syd</code>: Analysis tools for place cells, reliability, and plasticity</li> <li><code>vrAnalysis.analysis.same_cell_candidates</code>: Tools for identifying same cell candidates</li> <li><code>vrAnalysis.analysis.tracked_plasticity</code>: Analysis of tracked cell plasticity</li> </ul> <p>Note: The <code>vrAnalysis.analysis</code> package is not directly importable as a module. Refer to individual submodules for specific functionality.</p>"},{"location":"api/database/","title":"Database API Reference","text":""},{"location":"api/database/#vrAnalysis.database","title":"<code>database</code>","text":"<p>Database management module for VR analysis sessions.</p> <p>This module provides classes and functions for interacting with Microsoft Access databases used to track VR session data. It includes base database functionality and specialized session database management with support for registration workflows, suite2p processing tracking, and quality control operations.</p>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase","title":"<code>BaseDatabase</code>","text":"Source code in <code>vrAnalysis/database.py</code> <pre><code>class BaseDatabase:\n    def __init__(self, db_name: str):\n        \"\"\"\n        Initialize a new database instance.\n\n        This constructor initializes a new instance of the BaseDatabase class. It sets the default\n        values for the table name, database name, and database path. It is built to work with\n        the Microsoft Access application; however, a few small changes can make it compatible with\n        other SQL-based database systems.\n\n        Parameters\n        ----------\n        db_name : str, required\n            The name of the database to access.\n\n        Example\n        -------\n        &gt;&gt;&gt; db = BaseDatabase('vrSessions')\n        &gt;&gt;&gt; print(vrdb.table_name)\n        'sessiondb'\n        &gt;&gt;&gt; print(vrdb.db_name)\n        'vrDatabase'\n\n        Notes\n        -----\n        - This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.\n        - If you are using this on a new system, then you should edit your path, database name, and default table in that function.\n        \"\"\"\n\n        metadata = get_database_metadata(db_name)\n        self.db_path = metadata[\"db_path\"]\n        self.db_name = metadata[\"db_name\"]\n        self.db_ext = metadata[\"db_ext\"]\n        self.table_name = metadata[\"table_name\"]\n        self.uid = metadata[\"uid\"]\n        self.backup_path = metadata[\"backup_path\"]\n        self.host_type = host_types[self.db_ext]\n        self.unique_fields = self.process_unique_fields(metadata[\"unique_fields\"])\n        self.default_conditions = metadata[\"default_conditions\"]\n\n    def process_unique_fields(self, fields: List[Union[str, Tuple[str, type]]]) -&gt; List[Tuple[str, type]]:\n        \"\"\"\n        Process and validate unique field definitions.\n\n        Converts unique field specifications into a standardized format where each\n        field is a tuple of (field_name, field_type). String fields can be specified\n        as just the name and will default to str type.\n\n        Parameters\n        ----------\n        fields : list\n            List of field specifications. Each can be:\n            - A string (field name, defaults to str type)\n            - A tuple of (field_name, type) where type is a Python type class\n\n        Returns\n        -------\n        list\n            List of tuples, each containing (field_name, field_type).\n\n        Raises\n        ------\n        ValueError\n            If a field specification is not a string or a valid (string, type) tuple.\n        \"\"\"\n        ufields = []\n        for f in fields:\n            if isinstance(f, tuple) and len(f) == 2 and type(f[1]) == type and isinstance(f[0], str):\n                ufields.append(f)\n            elif isinstance(f, str):\n                ufields.append((f, str))\n            else:\n                raise ValueError(f\"unique field {f} must be a string or a string-type tuple\")\n        return ufields\n\n    def get_dbfile(self) -&gt; Path:\n        \"\"\"\n        Get the full path to the database file.\n\n        Returns\n        -------\n        Path\n            Path object pointing to the database file.\n        \"\"\"\n        return Path(self.db_path) / (self.db_name + self.db_ext)\n\n    def save_backup(self, return_out: bool = False) -&gt; Optional[CompletedProcess]:\n        \"\"\"Save a backup of the database to the backup path specified in metadata.\n\n        Parameters\n        ----------\n        return_out : bool, optional\n            If True, return the output of the robocopy command.\n\n        Returns\n        -------\n        CompletedProcess or None\n            CompletedProcess object from the robocopy command (if return_out is True).\n            Returns None if return_out is False.\n        \"\"\"\n        source_path = self.db_path\n        target_path = self.backup_path\n        source_file = self.db_name + self.db_ext\n        robocopy_arguments = f\"robocopy {source_path} {target_path} {source_file}\"\n        outs = run(robocopy_arguments, capture_output=True, text=True)\n        if return_out:\n            return outs\n\n    def connect(self) -&gt; pyodbc.Connection:\n        \"\"\"\n        Establish a connection to the database.\n\n        Creates a pyodbc connection using the appropriate driver string based on\n        the database file extension. Currently configured for Microsoft Access\n        databases (.accdb, .mdb files).\n\n        Returns\n        -------\n        pyodbc.Connection\n            Database connection object.\n\n        Raises\n        ------\n        AssertionError\n            If the host type for the database extension is not supported.\n\n        Notes\n        -----\n        To support additional database types:\n        1. Determine the appropriate pyodbc driver string for your database\n        2. Add it to the driver_string dictionary in this method\n        3. Update the host_types dictionary at the module level to map your\n           file extension to the driver key\n\n        See https://www.connectionstrings.com/ for driver string examples.\n        \"\"\"\n        driver_string = {\"access\": r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\" + rf\"DBQ={self.get_dbfile()};\"}\n\n        # Make sure connections are possible for this hosttype\n        failure_message = (\n            f\"Requested host_type ({self.host_type}) is not available. The only ones that are coded are: {[k for k in driver_string.keys()]}\\n\\n\"\n            f\"For support with writing a driver string for a different host, use the fantastic website: https://www.connectionstrings.com/\"\n        )\n        assert self.host_type in driver_string, failure_message\n\n        # Return a connection to the database\n        return pyodbc.connect(driver_string[self.host_type])\n\n    @contextmanager\n    def open_cursor(self, commit_changes: bool = False) -&gt; Generator[pyodbc.Cursor, None, None]:\n        \"\"\"\n        Context manager to open a database cursor and manage connections.\n\n        This context manager provides a convenient way to open a cursor to the database,\n        perform database operations, and manage connections. It also allows you to\n        commit changes if needed.\n\n        Parameters\n        ----------\n        commit_changes : bool, optional\n            Whether to commit changes to the database. Default is False.\n\n        Yields\n        ------\n        pyodbc.Cursor\n            A database cursor for executing SQL queries.\n\n        Raises\n        ------\n        Exception\n            If an error occurs while connecting to the database.\n\n        Example\n        -------\n        Use the context manager to perform database operations:\n\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     cursor.execute(\"SELECT * FROM your_table\")\n\n        \"\"\"\n        try:\n            # Attempt to open a cursor to the database\n            conn = self.connect()\n            cursor = conn.cursor()\n            yield cursor\n        except Exception as ex:\n            print(f\"An exception occurred while trying to connect to {self.db_name}!\")\n            print(ex)\n            raise ex\n        else:\n            # if no exception was raised, commit changes\n            if commit_changes:\n                conn.commit()\n        finally:\n            # Always close the cursor and connection\n            cursor.close()\n            conn.close()\n\n    # == display meta data for database ==\n    def show_metadata(self) -&gt; None:\n        \"\"\"\n        Display metadata associated with the open database.\n\n        Prints information about the database location, name, table, unique ID field,\n        backup path, and default filtering conditions.\n        \"\"\"\n        print(f\"{self.host_type} database located at {self.db_path}\")\n        print(f\"Database name: {self.db_name}{self.db_ext}, table name: {self.table_name}, with uid: {self.uid}\")\n        if self.backup_path is not None:\n            print(f\"Backup path located at: {self.backup_path}\")\n        else:\n            print(f\"No backup path specified...\")\n        if self.default_conditions:\n            print(f\"Default database filters:\")\n            for key, val in self.default_conditions.items():\n                print(\"  \", self.construct_filter_string(key, self.process_filter_value(val)))\n        else:\n            print(f\"No default filters.\")\n\n    def table_column_info(self) -&gt; Tuple[List[str], List[str], List[bool]]:\n        \"\"\"\n        Retrieve the column names, data types, and nullable status of the table.\n\n        Returns\n        -------\n        tuple\n            A tuple containing three elements:\n            - A list of strings representing the column names of the table.\n            - A list of strings representing the data types of the table.\n            - A list of booleans representing the nullable status of the table.\n        \"\"\"\n        with self.open_cursor(commit_changes=False) as cursor:\n            query = f\"SELECT * FROM {self.table_name} WHERE 1=0\"\n            cursor.execute(query)\n            column_descriptions = cursor.description\n        column_name, data_type, _, _, _, _, nullable = map(list, zip(*column_descriptions))\n        return column_name, data_type, nullable\n\n    # == retrieve table data ==\n    def table_data(self) -&gt; Tuple[List[str], List[Tuple[Any, ...]]]:\n        \"\"\"\n        Retrieve data and field names from the specified table.\n\n        This method retrieves the field names and table elements from the table specified\n        in the `BaseDatabase` instance.\n\n        Returns\n        -------\n        tuple\n            A tuple containing two elements:\n            - A list of strings representing the field names of the table.\n            - A list of tuples representing the data rows of the table.\n        \"\"\"\n        with self.open_cursor() as cursor:\n            field_names = [col.column_name for col in cursor.columns(table=self.table_name)]\n            cursor.execute(f\"SELECT * FROM {self.table_name}\")\n            table_elements = cursor.fetchall()\n\n        return field_names, table_elements\n\n    def get_table(self, use_default: bool = True, **kw_conditions: Any) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieve data from table in database and return as dataframe with optional filtering.\n\n        This method retrieves all data from the primary table in the database specified in\n        BaseDatabase instance. It automatically filters the data using the defaultConditions\n        defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is\n        a conflict.\n\n        Parameters\n        ----------\n        use_default : bool, default=True\n            Use default conditions if true, if False ignore them\n        **kw_conditions : dict, optional\n            Additional filtering conditions as keyword arguments.\n            Each condition should match a column name in the table.\n            Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair.\n            The operation defaults to '==', but you can use anything that works as a df query.\n\n            Examples:\n                - Simple equality: ``imaging=True`` filters where imaging column equals True\n                - Comparison operators: ``sessionID=(5, '&gt;')`` filters where sessionID &gt; 5\n                - Multiple conditions: ``imaging=True, mouseName='ATL028'`` applies AND logic\n\n            Note: this is limited in the sense that empty data can't be identified with key:None.\n            (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())\n\n        Returns\n        -------\n        df : pandas dataframe\n            A dataframe containing the filtered data from the primary database table.\n\n        Example\n        -------\n        &gt;&gt;&gt; vrdb = YourDatabaseClass()\n        &gt;&gt;&gt; df = vrdb.get_table(imaging=True)\n        &gt;&gt;&gt; df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))\n        \"\"\"\n\n        field_names, table_data = self.table_data()\n        df = pd.DataFrame.from_records(table_data, columns=field_names)\n        conditions = copy(self.default_conditions) if use_default else {}\n        conditions.update(kw_conditions)\n        if conditions:\n            for key, val in conditions.items():\n                assert key in field_names, f\"{key} is not a column name in {self.table_name}\"\n                conditions[key] = self.process_filter_value(val)  # make sure it's a value/operation pair\n            query = \" &amp; \".join([self.construct_filter_string(key, val_op_tuple) for key, val_op_tuple in conditions.items()])\n            df = df.query(query)\n        return df\n\n    def process_filter_value(self, val: Union[Any, Tuple[Any, str]]) -&gt; Tuple[Any, str]:\n        \"\"\"\n        Ensure filter value has an operation associated with it.\n\n        Filters are passed to pandas DataFrame queries as {key}{operation}{value}.\n        This method ensures each value is a tuple of (value, operation), defaulting\n        to '==' if no operation is specified.\n\n        Parameters\n        ----------\n        val : any or tuple\n            Filter value. If a tuple, should be (value, operation). If not a tuple,\n            will be converted to (val, '==').\n\n        Returns\n        -------\n        tuple\n            Tuple of (value, operation) for use in DataFrame queries.\n        \"\"\"\n        if not isinstance(val, tuple):\n            val = (val, \"==\")\n        return val\n\n    def construct_filter_string(self, key: str, val_op_tuple: Tuple[Any, str]) -&gt; str:\n        \"\"\"\n        Construct a string to be used as a pandas DataFrame query expression.\n\n        Parameters\n        ----------\n        key : str\n            Column name to filter on.\n        val_op_tuple : tuple\n            Tuple of (value, operation) where operation is a comparison operator\n            (e.g., '==', '!=', '&gt;', '&lt;').\n\n        Returns\n        -------\n        str\n            Query string in the format `column_name`operator'value'.\n        \"\"\"\n        val, op = val_op_tuple\n        return f\"`{key}`{op}{val!r}\"\n\n    # == methods for adding records and updating information to the database ==\n    def create_update_statement(self, field: str, uid: Any) -&gt; str:\n        \"\"\"\n        Create an SQL UPDATE statement for a single field and record.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n        uid : any\n            Unique identifier value for the record to update.\n\n        Returns\n        -------\n        str\n            SQL UPDATE statement with parameter placeholder.\n\n        Example\n        -------\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)\n        \"\"\"\n        return f\"UPDATE {self.table_name} set {field} = ? WHERE {self.uid} = {uid}\"\n\n    def create_update_many_statement(self, field: str) -&gt; str:\n        \"\"\"\n        Create an SQL UPDATE statement for batch updating a single field.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n\n        Returns\n        -------\n        str\n            SQL UPDATE statement with parameter placeholders for value and uid.\n\n        Example\n        -------\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     stmt = self.create_update_many_statement(\"fieldName\")\n        ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])\n        \"\"\"\n        return f\"UPDATE {self.table_name} set {field} = ? where {self.uid} = ?\"\n\n    def update_database_field(self, field: str, val: Any, **kw_conditions: Any) -&gt; None:\n        \"\"\"\n        Update a database field for all records matching specified conditions.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n        val : any\n            Value to set for the field.\n        **kw_conditions : dict, optional\n            Filtering conditions to identify records to update.\n            See get_table() documentation for filtering syntax.\n            Examples: ``mouseName='ATL028'``, ``imaging=True``\n\n        Raises\n        ------\n        AssertionError\n            If the specified field is not in the database table.\n        \"\"\"\n        assert field in self.table_data()[0], f\"Requested field ({field}) is not in table. Use 'self.table_data()[0]' to see available fields.\"\n        df = self.get_table(**kw_conditions)\n        update_statement = self.create_update_many_statement(field)\n        uids = df[self.uid].tolist()  # uids of all sessions requested\n        val_as_list = [val] * len(uids)\n        print(f\"Setting {field}={val} for all requested records...\")\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(update_statement, zip(val_as_list, uids))\n\n    # == method for adding a record to the database ==\n    def add_record(self, insert_statement: str, columns: List[str], values: List[Any]) -&gt; str:\n        \"\"\"\n        Add a single record to the database.\n\n        First checks if a record with matching unique field values already exists.\n        If so, prevents duplicate insertion and returns a message. Otherwise,\n        adds the new record to the database.\n\n        Parameters\n        ----------\n        insert_statement : str\n            SQL INSERT statement with parameter placeholders.\n        columns : list\n            List of column names matching the insert statement.\n        values : list\n            List of values to insert, corresponding to the columns.\n\n        Returns\n        -------\n        str\n            Success or duplicate record message.\n        \"\"\"\n        d = dict(zip(columns, values))\n        unique_values = [d[uf[0]] for uf in self.unique_fields]  # get values associated with unique fields\n        for ii, uv in enumerate(unique_values):\n            if isinstance(uv, date) or isinstance(uv, datetime):\n                # this is required for communicating with Access\n                unique_values[ii] = uv.strftime(\"%Y-%m-%d\")\n        unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n        if self.get_record(*unique_values, verbose=False) is not None:\n            print(f\"Record already exists for {unique_combo}\")\n            return f\"Record already exists for {unique_combo}\"\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(insert_statement, values)\n            print(f\"Successfully added new record for {unique_combo}\")\n        return \"Successfully added new record\"\n\n    def get_record(self, *unique_values: Any, verbose: bool = True) -&gt; Optional[pd.Series]:\n        \"\"\"\n        Retrieve single record from table in database and return as dataframe.\n\n        This method retrieves a single record(row) from the table in the database. The metadata for\n        each database defines a set of fields that comprise a unique set (each combination of values\n        for the unique fields is only represented once in the database).\n\n        Parameters\n        ----------\n        *unique_values: variable length list of values associated with the unique fields\n            - must be the same length as self.uniqueFields\n            - the second value of the uniqueField tuple (string by default) determines how\n              to query the unique value\n\n        Returns\n        -------\n        record : pandas Series\n\n        Example\n        -------\n        &gt;&gt;&gt; vrdb = YourDatabaseClass()\n        &gt;&gt;&gt; record = vrdb.get_record(*unique_conditions)\n        \"\"\"\n\n        # Check if correct values are provided\n        if len(unique_values) != len(self.unique_fields):\n            expected_list = \", \".join([uf[0] for uf in self.unique_fields])\n            raise ValueError(f\"{len(unique_values)} values provided but *get_record* is expecting values for: {expected_list}\")\n\n        # Get table and compare\n        df = self.get_table()\n        for uf, uv in zip(self.unique_fields, unique_values):\n            if uf[1] == str:\n                df = df[df[uf[0]] == uv]\n            elif uf[1] == datetime:\n                df = df[df[uf[0]].apply(lambda sd: sd.strftime(\"%Y-%m-%d\")) == uv]\n            elif uf[1] == int:\n                df = df[df[uf[0]] == int(uv)]\n            else:\n                raise ValueError(f\"uniqueField type ({uf[1]}) not recognized, add the appropriate query to this method!\")\n\n        if len(df) == 0:\n            if verbose:\n                unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n                print(f\"No session found under: {unique_combo}\")\n            return None\n\n        if len(df) &gt; 1:\n            unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n            raise ValueError(f\"Multiple sessions found under: {unique_combo}\")\n        return df.iloc[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.__init__","title":"<code>__init__(db_name)</code>","text":"<p>Initialize a new database instance.</p> <p>This constructor initializes a new instance of the BaseDatabase class. It sets the default values for the table name, database name, and database path. It is built to work with the Microsoft Access application; however, a few small changes can make it compatible with other SQL-based database systems.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>(str, required)</code> <p>The name of the database to access.</p> required Example <p>db = BaseDatabase('vrSessions') print(vrdb.table_name) 'sessiondb' print(vrdb.db_name) 'vrDatabase'</p> Notes <ul> <li>This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.</li> <li>If you are using this on a new system, then you should edit your path, database name, and default table in that function.</li> </ul> Source code in <code>vrAnalysis/database.py</code> <pre><code>def __init__(self, db_name: str):\n    \"\"\"\n    Initialize a new database instance.\n\n    This constructor initializes a new instance of the BaseDatabase class. It sets the default\n    values for the table name, database name, and database path. It is built to work with\n    the Microsoft Access application; however, a few small changes can make it compatible with\n    other SQL-based database systems.\n\n    Parameters\n    ----------\n    db_name : str, required\n        The name of the database to access.\n\n    Example\n    -------\n    &gt;&gt;&gt; db = BaseDatabase('vrSessions')\n    &gt;&gt;&gt; print(vrdb.table_name)\n    'sessiondb'\n    &gt;&gt;&gt; print(vrdb.db_name)\n    'vrDatabase'\n\n    Notes\n    -----\n    - This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.\n    - If you are using this on a new system, then you should edit your path, database name, and default table in that function.\n    \"\"\"\n\n    metadata = get_database_metadata(db_name)\n    self.db_path = metadata[\"db_path\"]\n    self.db_name = metadata[\"db_name\"]\n    self.db_ext = metadata[\"db_ext\"]\n    self.table_name = metadata[\"table_name\"]\n    self.uid = metadata[\"uid\"]\n    self.backup_path = metadata[\"backup_path\"]\n    self.host_type = host_types[self.db_ext]\n    self.unique_fields = self.process_unique_fields(metadata[\"unique_fields\"])\n    self.default_conditions = metadata[\"default_conditions\"]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.add_record","title":"<code>add_record(insert_statement, columns, values)</code>","text":"<p>Add a single record to the database.</p> <p>First checks if a record with matching unique field values already exists. If so, prevents duplicate insertion and returns a message. Otherwise, adds the new record to the database.</p> <p>Parameters:</p> Name Type Description Default <code>insert_statement</code> <code>str</code> <p>SQL INSERT statement with parameter placeholders.</p> required <code>columns</code> <code>list</code> <p>List of column names matching the insert statement.</p> required <code>values</code> <code>list</code> <p>List of values to insert, corresponding to the columns.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Success or duplicate record message.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def add_record(self, insert_statement: str, columns: List[str], values: List[Any]) -&gt; str:\n    \"\"\"\n    Add a single record to the database.\n\n    First checks if a record with matching unique field values already exists.\n    If so, prevents duplicate insertion and returns a message. Otherwise,\n    adds the new record to the database.\n\n    Parameters\n    ----------\n    insert_statement : str\n        SQL INSERT statement with parameter placeholders.\n    columns : list\n        List of column names matching the insert statement.\n    values : list\n        List of values to insert, corresponding to the columns.\n\n    Returns\n    -------\n    str\n        Success or duplicate record message.\n    \"\"\"\n    d = dict(zip(columns, values))\n    unique_values = [d[uf[0]] for uf in self.unique_fields]  # get values associated with unique fields\n    for ii, uv in enumerate(unique_values):\n        if isinstance(uv, date) or isinstance(uv, datetime):\n            # this is required for communicating with Access\n            unique_values[ii] = uv.strftime(\"%Y-%m-%d\")\n    unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n    if self.get_record(*unique_values, verbose=False) is not None:\n        print(f\"Record already exists for {unique_combo}\")\n        return f\"Record already exists for {unique_combo}\"\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.execute(insert_statement, values)\n        print(f\"Successfully added new record for {unique_combo}\")\n    return \"Successfully added new record\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.connect","title":"<code>connect()</code>","text":"<p>Establish a connection to the database.</p> <p>Creates a pyodbc connection using the appropriate driver string based on the database file extension. Currently configured for Microsoft Access databases (.accdb, .mdb files).</p> <p>Returns:</p> Type Description <code>Connection</code> <p>Database connection object.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the host type for the database extension is not supported.</p> Notes <p>To support additional database types: 1. Determine the appropriate pyodbc driver string for your database 2. Add it to the driver_string dictionary in this method 3. Update the host_types dictionary at the module level to map your    file extension to the driver key</p> <p>See https://www.connectionstrings.com/ for driver string examples.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def connect(self) -&gt; pyodbc.Connection:\n    \"\"\"\n    Establish a connection to the database.\n\n    Creates a pyodbc connection using the appropriate driver string based on\n    the database file extension. Currently configured for Microsoft Access\n    databases (.accdb, .mdb files).\n\n    Returns\n    -------\n    pyodbc.Connection\n        Database connection object.\n\n    Raises\n    ------\n    AssertionError\n        If the host type for the database extension is not supported.\n\n    Notes\n    -----\n    To support additional database types:\n    1. Determine the appropriate pyodbc driver string for your database\n    2. Add it to the driver_string dictionary in this method\n    3. Update the host_types dictionary at the module level to map your\n       file extension to the driver key\n\n    See https://www.connectionstrings.com/ for driver string examples.\n    \"\"\"\n    driver_string = {\"access\": r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\" + rf\"DBQ={self.get_dbfile()};\"}\n\n    # Make sure connections are possible for this hosttype\n    failure_message = (\n        f\"Requested host_type ({self.host_type}) is not available. The only ones that are coded are: {[k for k in driver_string.keys()]}\\n\\n\"\n        f\"For support with writing a driver string for a different host, use the fantastic website: https://www.connectionstrings.com/\"\n    )\n    assert self.host_type in driver_string, failure_message\n\n    # Return a connection to the database\n    return pyodbc.connect(driver_string[self.host_type])\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.construct_filter_string","title":"<code>construct_filter_string(key, val_op_tuple)</code>","text":"<p>Construct a string to be used as a pandas DataFrame query expression.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Column name to filter on.</p> required <code>val_op_tuple</code> <code>tuple</code> <p>Tuple of (value, operation) where operation is a comparison operator (e.g., '==', '!=', '&gt;', '&lt;').</p> required <p>Returns:</p> Type Description <code>str</code> <p>Query string in the format <code>column_name</code>operator'value'.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def construct_filter_string(self, key: str, val_op_tuple: Tuple[Any, str]) -&gt; str:\n    \"\"\"\n    Construct a string to be used as a pandas DataFrame query expression.\n\n    Parameters\n    ----------\n    key : str\n        Column name to filter on.\n    val_op_tuple : tuple\n        Tuple of (value, operation) where operation is a comparison operator\n        (e.g., '==', '!=', '&gt;', '&lt;').\n\n    Returns\n    -------\n    str\n        Query string in the format `column_name`operator'value'.\n    \"\"\"\n    val, op = val_op_tuple\n    return f\"`{key}`{op}{val!r}\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.create_update_many_statement","title":"<code>create_update_many_statement(field)</code>","text":"<p>Create an SQL UPDATE statement for batch updating a single field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL UPDATE statement with parameter placeholders for value and uid.</p> Example <p>with self.open_cursor(commit_changes=True) as cursor: ...     stmt = self.create_update_many_statement(\"fieldName\") ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def create_update_many_statement(self, field: str) -&gt; str:\n    \"\"\"\n    Create an SQL UPDATE statement for batch updating a single field.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n\n    Returns\n    -------\n    str\n        SQL UPDATE statement with parameter placeholders for value and uid.\n\n    Example\n    -------\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     stmt = self.create_update_many_statement(\"fieldName\")\n    ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])\n    \"\"\"\n    return f\"UPDATE {self.table_name} set {field} = ? where {self.uid} = ?\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.create_update_statement","title":"<code>create_update_statement(field, uid)</code>","text":"<p>Create an SQL UPDATE statement for a single field and record.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <code>uid</code> <code>any</code> <p>Unique identifier value for the record to update.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL UPDATE statement with parameter placeholder.</p> Example <p>with self.open_cursor(commit_changes=True) as cursor: ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def create_update_statement(self, field: str, uid: Any) -&gt; str:\n    \"\"\"\n    Create an SQL UPDATE statement for a single field and record.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n    uid : any\n        Unique identifier value for the record to update.\n\n    Returns\n    -------\n    str\n        SQL UPDATE statement with parameter placeholder.\n\n    Example\n    -------\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)\n    \"\"\"\n    return f\"UPDATE {self.table_name} set {field} = ? WHERE {self.uid} = {uid}\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.get_dbfile","title":"<code>get_dbfile()</code>","text":"<p>Get the full path to the database file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the database file.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_dbfile(self) -&gt; Path:\n    \"\"\"\n    Get the full path to the database file.\n\n    Returns\n    -------\n    Path\n        Path object pointing to the database file.\n    \"\"\"\n    return Path(self.db_path) / (self.db_name + self.db_ext)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.get_record","title":"<code>get_record(*unique_values, verbose=True)</code>","text":"<p>Retrieve single record from table in database and return as dataframe.</p> <p>This method retrieves a single record(row) from the table in the database. The metadata for each database defines a set of fields that comprise a unique set (each combination of values for the unique fields is only represented once in the database).</p> <p>Parameters:</p> Name Type Description Default <code>*unique_values</code> <code>Any</code> <ul> <li>must be the same length as self.uniqueFields</li> <li>the second value of the uniqueField tuple (string by default) determines how   to query the unique value</li> </ul> <code>()</code> <p>Returns:</p> Name Type Description <code>record</code> <code>pandas Series</code> Example <p>vrdb = YourDatabaseClass() record = vrdb.get_record(*unique_conditions)</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_record(self, *unique_values: Any, verbose: bool = True) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Retrieve single record from table in database and return as dataframe.\n\n    This method retrieves a single record(row) from the table in the database. The metadata for\n    each database defines a set of fields that comprise a unique set (each combination of values\n    for the unique fields is only represented once in the database).\n\n    Parameters\n    ----------\n    *unique_values: variable length list of values associated with the unique fields\n        - must be the same length as self.uniqueFields\n        - the second value of the uniqueField tuple (string by default) determines how\n          to query the unique value\n\n    Returns\n    -------\n    record : pandas Series\n\n    Example\n    -------\n    &gt;&gt;&gt; vrdb = YourDatabaseClass()\n    &gt;&gt;&gt; record = vrdb.get_record(*unique_conditions)\n    \"\"\"\n\n    # Check if correct values are provided\n    if len(unique_values) != len(self.unique_fields):\n        expected_list = \", \".join([uf[0] for uf in self.unique_fields])\n        raise ValueError(f\"{len(unique_values)} values provided but *get_record* is expecting values for: {expected_list}\")\n\n    # Get table and compare\n    df = self.get_table()\n    for uf, uv in zip(self.unique_fields, unique_values):\n        if uf[1] == str:\n            df = df[df[uf[0]] == uv]\n        elif uf[1] == datetime:\n            df = df[df[uf[0]].apply(lambda sd: sd.strftime(\"%Y-%m-%d\")) == uv]\n        elif uf[1] == int:\n            df = df[df[uf[0]] == int(uv)]\n        else:\n            raise ValueError(f\"uniqueField type ({uf[1]}) not recognized, add the appropriate query to this method!\")\n\n    if len(df) == 0:\n        if verbose:\n            unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n            print(f\"No session found under: {unique_combo}\")\n        return None\n\n    if len(df) &gt; 1:\n        unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n        raise ValueError(f\"Multiple sessions found under: {unique_combo}\")\n    return df.iloc[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.get_table","title":"<code>get_table(use_default=True, **kw_conditions)</code>","text":"<p>Retrieve data from table in database and return as dataframe with optional filtering.</p> <p>This method retrieves all data from the primary table in the database specified in BaseDatabase instance. It automatically filters the data using the defaultConditions defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is a conflict.</p> <p>Parameters:</p> Name Type Description Default <code>use_default</code> <code>bool</code> <p>Use default conditions if true, if False ignore them</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions as keyword arguments. Each condition should match a column name in the table. Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair. The operation defaults to '==', but you can use anything that works as a df query.</p> <p>Examples:     - Simple equality: <code>imaging=True</code> filters where imaging column equals True     - Comparison operators: <code>sessionID=(5, '&gt;')</code> filters where sessionID &gt; 5     - Multiple conditions: <code>imaging=True, mouseName='ATL028'</code> applies AND logic</p> <p>Note: this is limited in the sense that empty data can't be identified with key:None. (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas dataframe</code> <p>A dataframe containing the filtered data from the primary database table.</p> Example <p>vrdb = YourDatabaseClass() df = vrdb.get_table(imaging=True) df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_table(self, use_default: bool = True, **kw_conditions: Any) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieve data from table in database and return as dataframe with optional filtering.\n\n    This method retrieves all data from the primary table in the database specified in\n    BaseDatabase instance. It automatically filters the data using the defaultConditions\n    defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is\n    a conflict.\n\n    Parameters\n    ----------\n    use_default : bool, default=True\n        Use default conditions if true, if False ignore them\n    **kw_conditions : dict, optional\n        Additional filtering conditions as keyword arguments.\n        Each condition should match a column name in the table.\n        Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair.\n        The operation defaults to '==', but you can use anything that works as a df query.\n\n        Examples:\n            - Simple equality: ``imaging=True`` filters where imaging column equals True\n            - Comparison operators: ``sessionID=(5, '&gt;')`` filters where sessionID &gt; 5\n            - Multiple conditions: ``imaging=True, mouseName='ATL028'`` applies AND logic\n\n        Note: this is limited in the sense that empty data can't be identified with key:None.\n        (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())\n\n    Returns\n    -------\n    df : pandas dataframe\n        A dataframe containing the filtered data from the primary database table.\n\n    Example\n    -------\n    &gt;&gt;&gt; vrdb = YourDatabaseClass()\n    &gt;&gt;&gt; df = vrdb.get_table(imaging=True)\n    &gt;&gt;&gt; df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))\n    \"\"\"\n\n    field_names, table_data = self.table_data()\n    df = pd.DataFrame.from_records(table_data, columns=field_names)\n    conditions = copy(self.default_conditions) if use_default else {}\n    conditions.update(kw_conditions)\n    if conditions:\n        for key, val in conditions.items():\n            assert key in field_names, f\"{key} is not a column name in {self.table_name}\"\n            conditions[key] = self.process_filter_value(val)  # make sure it's a value/operation pair\n        query = \" &amp; \".join([self.construct_filter_string(key, val_op_tuple) for key, val_op_tuple in conditions.items()])\n        df = df.query(query)\n    return df\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.open_cursor","title":"<code>open_cursor(commit_changes=False)</code>","text":"<p>Context manager to open a database cursor and manage connections.</p> <p>This context manager provides a convenient way to open a cursor to the database, perform database operations, and manage connections. It also allows you to commit changes if needed.</p> <p>Parameters:</p> Name Type Description Default <code>commit_changes</code> <code>bool</code> <p>Whether to commit changes to the database. Default is False.</p> <code>False</code> <p>Yields:</p> Type Description <code>Cursor</code> <p>A database cursor for executing SQL queries.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while connecting to the database.</p> Example <p>Use the context manager to perform database operations:</p> <p>with self.open_cursor(commit_changes=True) as cursor: ...     cursor.execute(\"SELECT * FROM your_table\")</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>@contextmanager\ndef open_cursor(self, commit_changes: bool = False) -&gt; Generator[pyodbc.Cursor, None, None]:\n    \"\"\"\n    Context manager to open a database cursor and manage connections.\n\n    This context manager provides a convenient way to open a cursor to the database,\n    perform database operations, and manage connections. It also allows you to\n    commit changes if needed.\n\n    Parameters\n    ----------\n    commit_changes : bool, optional\n        Whether to commit changes to the database. Default is False.\n\n    Yields\n    ------\n    pyodbc.Cursor\n        A database cursor for executing SQL queries.\n\n    Raises\n    ------\n    Exception\n        If an error occurs while connecting to the database.\n\n    Example\n    -------\n    Use the context manager to perform database operations:\n\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     cursor.execute(\"SELECT * FROM your_table\")\n\n    \"\"\"\n    try:\n        # Attempt to open a cursor to the database\n        conn = self.connect()\n        cursor = conn.cursor()\n        yield cursor\n    except Exception as ex:\n        print(f\"An exception occurred while trying to connect to {self.db_name}!\")\n        print(ex)\n        raise ex\n    else:\n        # if no exception was raised, commit changes\n        if commit_changes:\n            conn.commit()\n    finally:\n        # Always close the cursor and connection\n        cursor.close()\n        conn.close()\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.process_filter_value","title":"<code>process_filter_value(val)</code>","text":"<p>Ensure filter value has an operation associated with it.</p> <p>Filters are passed to pandas DataFrame queries as {key}{operation}{value}. This method ensures each value is a tuple of (value, operation), defaulting to '==' if no operation is specified.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>any or tuple</code> <p>Filter value. If a tuple, should be (value, operation). If not a tuple, will be converted to (val, '==').</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (value, operation) for use in DataFrame queries.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def process_filter_value(self, val: Union[Any, Tuple[Any, str]]) -&gt; Tuple[Any, str]:\n    \"\"\"\n    Ensure filter value has an operation associated with it.\n\n    Filters are passed to pandas DataFrame queries as {key}{operation}{value}.\n    This method ensures each value is a tuple of (value, operation), defaulting\n    to '==' if no operation is specified.\n\n    Parameters\n    ----------\n    val : any or tuple\n        Filter value. If a tuple, should be (value, operation). If not a tuple,\n        will be converted to (val, '==').\n\n    Returns\n    -------\n    tuple\n        Tuple of (value, operation) for use in DataFrame queries.\n    \"\"\"\n    if not isinstance(val, tuple):\n        val = (val, \"==\")\n    return val\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.process_unique_fields","title":"<code>process_unique_fields(fields)</code>","text":"<p>Process and validate unique field definitions.</p> <p>Converts unique field specifications into a standardized format where each field is a tuple of (field_name, field_type). String fields can be specified as just the name and will default to str type.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>List of field specifications. Each can be: - A string (field name, defaults to str type) - A tuple of (field_name, type) where type is a Python type class</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of tuples, each containing (field_name, field_type).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a field specification is not a string or a valid (string, type) tuple.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def process_unique_fields(self, fields: List[Union[str, Tuple[str, type]]]) -&gt; List[Tuple[str, type]]:\n    \"\"\"\n    Process and validate unique field definitions.\n\n    Converts unique field specifications into a standardized format where each\n    field is a tuple of (field_name, field_type). String fields can be specified\n    as just the name and will default to str type.\n\n    Parameters\n    ----------\n    fields : list\n        List of field specifications. Each can be:\n        - A string (field name, defaults to str type)\n        - A tuple of (field_name, type) where type is a Python type class\n\n    Returns\n    -------\n    list\n        List of tuples, each containing (field_name, field_type).\n\n    Raises\n    ------\n    ValueError\n        If a field specification is not a string or a valid (string, type) tuple.\n    \"\"\"\n    ufields = []\n    for f in fields:\n        if isinstance(f, tuple) and len(f) == 2 and type(f[1]) == type and isinstance(f[0], str):\n            ufields.append(f)\n        elif isinstance(f, str):\n            ufields.append((f, str))\n        else:\n            raise ValueError(f\"unique field {f} must be a string or a string-type tuple\")\n    return ufields\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.save_backup","title":"<code>save_backup(return_out=False)</code>","text":"<p>Save a backup of the database to the backup path specified in metadata.</p> <p>Parameters:</p> Name Type Description Default <code>return_out</code> <code>bool</code> <p>If True, return the output of the robocopy command.</p> <code>False</code> <p>Returns:</p> Type Description <code>CompletedProcess or None</code> <p>CompletedProcess object from the robocopy command (if return_out is True). Returns None if return_out is False.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def save_backup(self, return_out: bool = False) -&gt; Optional[CompletedProcess]:\n    \"\"\"Save a backup of the database to the backup path specified in metadata.\n\n    Parameters\n    ----------\n    return_out : bool, optional\n        If True, return the output of the robocopy command.\n\n    Returns\n    -------\n    CompletedProcess or None\n        CompletedProcess object from the robocopy command (if return_out is True).\n        Returns None if return_out is False.\n    \"\"\"\n    source_path = self.db_path\n    target_path = self.backup_path\n    source_file = self.db_name + self.db_ext\n    robocopy_arguments = f\"robocopy {source_path} {target_path} {source_file}\"\n    outs = run(robocopy_arguments, capture_output=True, text=True)\n    if return_out:\n        return outs\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.show_metadata","title":"<code>show_metadata()</code>","text":"<p>Display metadata associated with the open database.</p> <p>Prints information about the database location, name, table, unique ID field, backup path, and default filtering conditions.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def show_metadata(self) -&gt; None:\n    \"\"\"\n    Display metadata associated with the open database.\n\n    Prints information about the database location, name, table, unique ID field,\n    backup path, and default filtering conditions.\n    \"\"\"\n    print(f\"{self.host_type} database located at {self.db_path}\")\n    print(f\"Database name: {self.db_name}{self.db_ext}, table name: {self.table_name}, with uid: {self.uid}\")\n    if self.backup_path is not None:\n        print(f\"Backup path located at: {self.backup_path}\")\n    else:\n        print(f\"No backup path specified...\")\n    if self.default_conditions:\n        print(f\"Default database filters:\")\n        for key, val in self.default_conditions.items():\n            print(\"  \", self.construct_filter_string(key, self.process_filter_value(val)))\n    else:\n        print(f\"No default filters.\")\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.table_column_info","title":"<code>table_column_info()</code>","text":"<p>Retrieve the column names, data types, and nullable status of the table.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing three elements: - A list of strings representing the column names of the table. - A list of strings representing the data types of the table. - A list of booleans representing the nullable status of the table.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def table_column_info(self) -&gt; Tuple[List[str], List[str], List[bool]]:\n    \"\"\"\n    Retrieve the column names, data types, and nullable status of the table.\n\n    Returns\n    -------\n    tuple\n        A tuple containing three elements:\n        - A list of strings representing the column names of the table.\n        - A list of strings representing the data types of the table.\n        - A list of booleans representing the nullable status of the table.\n    \"\"\"\n    with self.open_cursor(commit_changes=False) as cursor:\n        query = f\"SELECT * FROM {self.table_name} WHERE 1=0\"\n        cursor.execute(query)\n        column_descriptions = cursor.description\n    column_name, data_type, _, _, _, _, nullable = map(list, zip(*column_descriptions))\n    return column_name, data_type, nullable\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.table_data","title":"<code>table_data()</code>","text":"<p>Retrieve data and field names from the specified table.</p> <p>This method retrieves the field names and table elements from the table specified in the <code>BaseDatabase</code> instance.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two elements: - A list of strings representing the field names of the table. - A list of tuples representing the data rows of the table.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def table_data(self) -&gt; Tuple[List[str], List[Tuple[Any, ...]]]:\n    \"\"\"\n    Retrieve data and field names from the specified table.\n\n    This method retrieves the field names and table elements from the table specified\n    in the `BaseDatabase` instance.\n\n    Returns\n    -------\n    tuple\n        A tuple containing two elements:\n        - A list of strings representing the field names of the table.\n        - A list of tuples representing the data rows of the table.\n    \"\"\"\n    with self.open_cursor() as cursor:\n        field_names = [col.column_name for col in cursor.columns(table=self.table_name)]\n        cursor.execute(f\"SELECT * FROM {self.table_name}\")\n        table_elements = cursor.fetchall()\n\n    return field_names, table_elements\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.update_database_field","title":"<code>update_database_field(field, val, **kw_conditions)</code>","text":"<p>Update a database field for all records matching specified conditions.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <code>val</code> <code>any</code> <p>Value to set for the field.</p> required <code>**kw_conditions</code> <code>dict</code> <p>Filtering conditions to identify records to update. See get_table() documentation for filtering syntax. Examples: <code>mouseName='ATL028'</code>, <code>imaging=True</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the specified field is not in the database table.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def update_database_field(self, field: str, val: Any, **kw_conditions: Any) -&gt; None:\n    \"\"\"\n    Update a database field for all records matching specified conditions.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n    val : any\n        Value to set for the field.\n    **kw_conditions : dict, optional\n        Filtering conditions to identify records to update.\n        See get_table() documentation for filtering syntax.\n        Examples: ``mouseName='ATL028'``, ``imaging=True``\n\n    Raises\n    ------\n    AssertionError\n        If the specified field is not in the database table.\n    \"\"\"\n    assert field in self.table_data()[0], f\"Requested field ({field}) is not in table. Use 'self.table_data()[0]' to see available fields.\"\n    df = self.get_table(**kw_conditions)\n    update_statement = self.create_update_many_statement(field)\n    uids = df[self.uid].tolist()  # uids of all sessions requested\n    val_as_list = [val] * len(uids)\n    print(f\"Setting {field}={val} for all requested records...\")\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(update_statement, zip(val_as_list, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase","title":"<code>SessionDatabase</code>","text":"<p>               Bases: <code>BaseDatabase</code></p> <p>Database class for handling VR session data.</p> <p>Specialized database class that extends BaseDatabase with session-specific functionality, including methods for creating session objects, managing registration workflows, and handling quality control processes.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>class SessionDatabase(BaseDatabase):\n    \"\"\"\n    Database class for handling VR session data.\n\n    Specialized database class that extends BaseDatabase with session-specific\n    functionality, including methods for creating session objects, managing\n    registration workflows, and handling quality control processes.\n    \"\"\"\n\n    def iter_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; List[B2Session]:\n        \"\"\"Iterate over sessions matching conditions.\n\n        Parameters\n        ----------\n        session_params : dict, default={}\n            Additional parameters to pass to the session constructor when creating\n            B2Session objects. These are passed through to create_b2session().\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n            Examples: ``mouseName='ATL028'``, ``imaging=True``, ``sessionID=(5, '&gt;')``\n\n        Returns\n        -------\n        sessions : list[B2Session]\n            List of sessions matching the conditions.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        sessions = []\n        for _, row in df.iterrows():\n            sessions.append(create_b2session(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params))\n        return sessions\n\n    # == EVERYTHING BELOW HERE IS THE SAME AS THE ORIGINAL DATABASE CLASS ==\n    # == It should be refactored to use the new vrAnalysis classes eventually, but I'm leaving it here for now ==\n    # == It should work as is as long as vrAnalysis stays on the path! ==\n\n    # == vrExperiment related methods ==\n    def session_name(self, row: pd.Series) -&gt; Tuple[str, str, str]:\n        \"\"\"\n        Extract session identifiers from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n\n        Returns\n        -------\n        tuple\n            Tuple of (mouse_name, session_date, session_id) where session_date is\n            formatted as 'YYYY-MM-DD' and session_id is converted to string.\n        \"\"\"\n        mouse_name = row[\"mouseName\"]\n        session_date = row[\"sessionDate\"].strftime(\"%Y-%m-%d\")\n        session_id = str(row[\"sessionID\"])\n        return mouse_name, session_date, session_id\n\n    def make_b2session(self, row: pd.Series) -&gt; B2Session:\n        \"\"\"\n        Create a B2Session object from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n\n        Returns\n        -------\n        B2Session\n            Session object initialized with data from the record.\n        \"\"\"\n        mouse_name, session_date, session_id = self.session_name(row)\n        return create_b2session(mouse_name, session_date, session_id)\n\n    def make_b2registration(self, row: pd.Series, opts: B2RegistrationOpts) -&gt; B2Registration:\n        \"\"\"\n        Create a B2Registration object from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n        opts : B2RegistrationOpts\n            Registration options to use for the session.\n\n        Returns\n        -------\n        B2Registration\n            Registration object initialized with session data and options.\n        \"\"\"\n        mouse_name, session_date, session_id = self.session_name(row)\n        return B2Registration(mouse_name, session_date, session_id, opts)\n\n    # == helper functions for figuring out what needs work ==\n    def needs_registration(self, skip_errors: bool = True, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need registration preprocessing.\n\n        Parameters\n        ----------\n        skip_errors : bool, default=True\n            If True, exclude sessions that had registration errors.\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need registration.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        if skip_errors:\n            df = df[df[\"vrRegistrationError\"] == False]\n        df = df[df[\"vrRegistration\"] == False]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                session = self.make_b2session(row)\n                print(f\"Session needs registration: {session.session_print()}\")\n            return None\n\n    def update_s2p_date_time(self) -&gt; None:\n        \"\"\"\n        Update suite2p creation dates in the database based on file modification times.\n\n        For all sessions where suite2p processing is complete, finds the most recent\n        file modification time in the suite2p output directory and updates the\n        suite2pDate field in the database.\n        \"\"\"\n        df = self.get_table()\n        s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n        uids = s2p_done[self.uid].tolist()\n        s2p_creation_date = []\n        for idx, row in s2p_done.iterrows():\n            session = self.make_b2session(row)  # create vrSession to point to session folder\n            c_latest_mod = 0\n            for p in session.s2p_path.rglob(\"*\"):\n                if not (p.is_dir()):\n                    c_latest_mod = max(p.stat().st_mtime, c_latest_mod)\n            c_date_time = datetime.fromtimestamp(c_latest_mod)\n            s2p_creation_date.append(c_date_time)  # get suite2p path creation date\n\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(self.create_update_many_statement(\"suite2pDate\"), zip(s2p_creation_date, uids))\n\n    def needs_s2p(\n        self, needs_qc: bool = False, return_df: bool = True, print_targets: bool = True, **kw_conditions: Any\n    ) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need suite2p processing or quality control.\n\n        Parameters\n        ----------\n        needs_qc : bool, default=False\n            If False, returns/prints sessions that need suite2p processing.\n            If True, returns/prints sessions that need suite2p quality control.\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        print_targets : bool, default=True\n            If True and return_df=False, prints suite2p target information for sessions needing processing.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        if needs_qc:\n            df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"suite2pQC\"] == False)]\n        else:\n            df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                session = self.make_b2session(row)\n                if needs_qc:\n                    print(f\"Database indicates that suite2p has been run but not QC'd: {session.session_print()}\")\n                else:\n                    print(f\"Database indicates that suite2p has not been run: {session.session_print()}\")\n                    if print_targets:\n                        mouse_name, session_date, session_id = self.session_name(row)\n                        s2p_targets(mouse_name, session_date, session_id)\n                        print(\"\")\n            return None\n\n    def check_s2p(self, with_database_update: bool = False, return_check: bool = False) -&gt; Optional[bool]:\n        \"\"\"\n        Verify suite2p status consistency between database and file system.\n\n        Checks for discrepancies where:\n        - Database says suite2p is done but files don't exist\n        - Files exist but database says suite2p wasn't done\n\n        Parameters\n        ----------\n        with_database_update : bool, default=False\n            If True, automatically corrects database entries when discrepancies are found.\n        return_check : bool, default=False\n            If True, returns a boolean indicating whether any discrepancies were found.\n\n        Returns\n        -------\n        bool or None\n            If return_check is True, returns True if any discrepancies were found,\n            False otherwise. Returns None if return_check is False.\n        \"\"\"\n        df = self.get_table()\n\n        # Check sessions where database says suite2p is done but files don't exist\n        check_s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n        checked_not_done = check_s2p_done.apply(lambda row: not (self.make_b2session(row).s2p_path.exists()), axis=1)\n        not_actually_done = check_s2p_done[checked_not_done]\n\n        # Check sessions where files exist but database says suite2p wasn't done\n        check_s2p_needed = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n        checked_not_needed = check_s2p_needed.apply(lambda row: self.make_b2session(row).s2p_path.exists(), axis=1)\n        not_actually_needed = check_s2p_needed[checked_not_needed]\n\n        # Print database errors to workspace\n        for idx, row in not_actually_done.iterrows():\n            print(f\"Database said suite2p has been ran, but it actually hasn't: {self.make_b2session(row).session_print()}\")\n        for idx, row in not_actually_needed.iterrows():\n            print(f\"Database said suite2p didn't run, but it already did: {self.make_b2session(row).session_print()}\")\n\n        # If with_database_update is True, correct the database\n        if with_database_update:\n            for idx, row in not_actually_done.iterrows():\n                with self.open_cursor(commit_changes=True) as cursor:\n                    cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), False)\n\n            for idx, row in not_actually_needed.iterrows():\n                with self.open_cursor(commit_changes=True) as cursor:\n                    cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), True)\n\n        # If return_check is requested, return True if any records were invalid\n        if return_check:\n            return checked_not_done.any() or checked_not_needed.any()\n\n    # == for communicating with the database about red cell quality control ==\n    def update_red_cell_qc_date_time(self) -&gt; None:\n        \"\"\"\n        Update red cell QC dates in the database based on file modification times.\n\n        For all sessions where red cell QC is complete, finds the most recent\n        modification time of relevant red cell QC files and updates the\n        redCellQCDate field in the database.\n        \"\"\"\n        relevant_one_files = [\n            \"mpciROIs.redCellIdx.npy\",\n            \"mpciROIs.redCellManualAssignment.npy\",\n            \"parametersRed*\",  # wildcard because there are multiple possibilities\n        ]\n\n        df = self.get_table()\n        red_cell_qc_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"redCellQC\"] == True)]\n        uids = red_cell_qc_done[self.uid].tolist()\n        rc_edit_date = []\n        for idx, row in red_cell_qc_done.iterrows():\n            session = self.make_b2session(row)  # create vrSession to point to session folder\n            c_latest_mod = 0\n            for f in relevant_one_files:\n                for file in session.one_path.rglob(f):\n                    c_latest_mod = max(file.stat().st_mtime, c_latest_mod)\n            c_date_time = datetime.fromtimestamp(c_latest_mod)\n            rc_edit_date.append(c_date_time)  # get red cell QC file modification date\n\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(self.create_update_many_statement(\"redCellQCDate\"), zip(rc_edit_date, uids))\n\n    def needs_red_cell_qc(self, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need red cell quality control.\n\n        Parameters\n        ----------\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need red cell QC.\n            Sessions must have imaging, suite2p processing, and registration completed.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"vrRegistration\"] == True) &amp; (df[\"redCellQC\"] == False)]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                print(f\"Database indicates that redCellQC has not been performed for session: {self.make_b2session(row).session_print()}\")\n            return None\n\n    def iter_sessions_need_red_cell_qc(self, **kw_conditions: Any) -&gt; List[B2Session]:\n        \"\"\"\n        Get list of sessions that require red cell quality control.\n\n        Parameters\n        ----------\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        list[B2Session]\n            List of session objects that need red cell QC.\n        \"\"\"\n        df = self.needs_red_cell_qc(return_df=True, **kw_conditions)\n        sessions = []\n        for idx, row in df.iterrows():\n            sessions.append(self.make_b2session(row))\n        return sessions\n\n    def set_red_cell_qc(self, mouse_name: str, date_string: str, session_id: Union[str, int], state: bool = True) -&gt; bool:\n        \"\"\"\n        Set the red cell QC status for a specific session.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Mouse name identifier.\n        date_string : str\n            Session date in 'YYYY-MM-DD' format.\n        session_id : str or int\n            Session ID.\n        state : bool, default=True\n            Red cell QC status to set. If True, also sets the QC date to now.\n\n        Returns\n        -------\n        bool\n            True if update was successful, False otherwise.\n        \"\"\"\n        record = self.get_record(mouse_name, date_string, session_id)\n        if record is None:\n            print(f\"Could not find session {mouse_name}/{date_string}/{session_id} in database.\")\n            return False\n\n        try:\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"redCellQC\", record[self.uid]), state)\n                if state == True:\n                    # If setting red cell QC to true, add the date\n                    cursor.execute(\n                        self.create_update_statement(\"redCellQCDate\", record[self.uid]),\n                        datetime.now(),\n                    )\n                else:\n                    # Otherwise remove the date\n                    cursor.execute(self.create_update_statement(\"redCellQCDate\", record[self.uid]), \"\")\n            return True\n\n        except Exception as ex:\n            print(f\"Failed to update database for session: {mouse_name}/{date_string}/{session_id}\")\n            print(f\"Error: {ex}\")\n            return False\n\n    # == operating vrExperiment pipeline ==\n    def register_record(self, record: pd.Series, raise_exception: bool = False, imaging: Optional[bool] = None) -&gt; Tuple[bool, int]:\n        \"\"\"\n        Perform registration preprocessing for a single session record.\n\n        Creates a B2Registration object and runs preprocessing. Updates the database\n        with success/failure status and error information if applicable.\n\n        Parameters\n        ----------\n        record : pandas.Series\n            Database record containing session information.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting. If None (default), uses the value from the\n            database record. If True or False, overrides the database value.\n\n        Returns\n        -------\n        tuple\n            Tuple of (success, data_size) where:\n            - success: bool indicating if registration succeeded\n            - data_size: int size in bytes of registered oneData (0 if failed)\n\n        Notes\n        -----\n        On failure, clears all oneData files and updates database error fields.\n        On success, updates registration status and date in the database.\n        \"\"\"\n        opts = B2RegistrationOpts()\n        opts.imaging = bool(imaging) if imaging is not None else bool(record[\"imaging\"])\n        opts.facecam = bool(record[\"faceCamera\"])\n        opts.vrBehaviorVersion = record[\"vrBehaviorVersion\"]\n        b2reg = self.make_b2registration(record, opts)\n        try:\n            print(f\"Registering data for session: {b2reg.session_print()}\")\n            b2reg.register()\n        except Exception as ex:\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), True)\n                cursor.execute(\n                    self.create_update_statement(\"vrRegistrationException\", record[self.uid]),\n                    str(ex),\n                )\n            if raise_exception:\n                raise ex\n            print(f\"The following exception was raised when trying to preprocess session: {b2reg.session_print()}. Clearing all oneData.\")\n            b2reg.clear_one_data(certainty=True)\n            error_print(f\"Last traceback: {traceback.extract_tb(ex.__traceback__, limit=-1)}\")\n            error_print(f\"Exception: {ex}\")\n            # If failed, return (False, 0)\n            out = (False, 0)\n        else:\n            with self.open_cursor(commit_changes=True) as cursor:\n                # Tell the database that vrRegistration was performed and the time of processing\n                cursor.execute(self.create_update_statement(\"vrRegistration\", record[self.uid]), True)\n                cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), False)\n                cursor.execute(self.create_update_statement(\"vrRegistrationException\", record[self.uid]), \"\")\n                cursor.execute(\n                    self.create_update_statement(\"vrRegistrationDate\", record[self.uid]),\n                    datetime.now(),\n                )\n            # If successful, return (True, size of registered oneData)\n            out = (True, sum([one_file.stat().st_size for one_file in b2reg.get_saved_one()]))\n            print(f\"Session {b2reg.session_print()} registered with {readable_bytes(out[1])} oneData.\")\n        finally:\n            del b2reg\n        return out\n\n    def register_single_session(\n        self,\n        mouse_name: str,\n        session_date: str,\n        session_id: Union[str, int],\n        raise_exception: bool = False,\n        imaging: Optional[bool] = None,\n    ) -&gt; Optional[bool]:\n        \"\"\"\n        Register a single session by its identifiers.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Mouse name identifier.\n        session_date : str\n            Session date in 'YYYY-MM-DD' format.\n        session_id : str or int\n            Session ID.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting. If None (default), uses the value from the\n            database record. If True or False, overrides the database value to enable\n            or disable imaging processing during registration.\n\n        Returns\n        -------\n        bool or None\n            True if registration succeeded, False if failed, None if session not found.\n        \"\"\"\n        record = self.get_record(mouse_name, session_date, session_id)\n        if record is None:\n            print(f\"Session {'/'.join([mouse_name, session_date, session_id])} is not in the database\")\n            return\n        out = self.register_record(record, raise_exception=raise_exception, imaging=imaging)\n        return out[0]\n\n    def register_sessions(\n        self,\n        max_data: float = 30e9,\n        skip_errors: bool = True,\n        raise_exception: bool = False,\n        imaging: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"\n        Register multiple sessions that need registration.\n\n        Processes sessions in batches, stopping when the total data size limit is reached.\n        Provides progress updates including accumulated data size and estimates.\n\n        Parameters\n        ----------\n        max_data : float, default=30e9\n            Maximum total data size (in bytes) to process before stopping.\n            Default is 30 GB.\n        skip_errors : bool, default=True\n            If True, skip sessions that had previous registration errors.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting for all sessions. If None (default), uses the\n            value from each session's database record. If True or False, overrides the\n            database value for all sessions to enable or disable imaging processing.\n\n        Notes\n        -----\n        Prints progress information including:\n        - Accumulated oneData registered\n        - Average data size per session\n        - Estimated remaining data to process\n        \"\"\"\n        count_sessions = 0\n        total_one_data = 0.0\n        df_to_register = self.needs_registration(skip_errors=skip_errors)\n\n        for idx, (_, row) in enumerate(df_to_register.iterrows()):\n            if total_one_data &gt; max_data:\n                print(f\"\\nMax data limit reached. Total processed: {readable_bytes(total_one_data)}. Limit: {readable_bytes(max_data)}\")\n                return\n            print(\"\")\n            out = self.register_record(row, raise_exception=raise_exception, imaging=imaging)\n            if out[0]:\n                count_sessions += 1  # count successful sessions\n                total_one_data += out[1]  # accumulated oneData registered\n                estimate_remaining = len(df_to_register) - idx - 1\n                print(\n                    f\"Accumulated oneData registered: {readable_bytes(total_one_data)}. \"\n                    f\"Averaging: {readable_bytes(total_one_data/count_sessions)} / session. \"\n                    f\"Estimate remaining: {readable_bytes(total_one_data/count_sessions*estimate_remaining)}\"\n                )\n\n    def print_registration_errors(self, **kw_conditions: Any) -&gt; None:\n        \"\"\"\n        Print registration errors for sessions that failed registration.\n\n        Parameters\n        ----------\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        for idx, row in df[df[\"vrRegistrationError\"] == True].iterrows():\n            print(f\"{'/'.join(self.session_name(row))} had error: {row['vrRegistrationException']}\")\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.check_s2p","title":"<code>check_s2p(with_database_update=False, return_check=False)</code>","text":"<p>Verify suite2p status consistency between database and file system.</p> <p>Checks for discrepancies where: - Database says suite2p is done but files don't exist - Files exist but database says suite2p wasn't done</p> <p>Parameters:</p> Name Type Description Default <code>with_database_update</code> <code>bool</code> <p>If True, automatically corrects database entries when discrepancies are found.</p> <code>False</code> <code>return_check</code> <code>bool</code> <p>If True, returns a boolean indicating whether any discrepancies were found.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool or None</code> <p>If return_check is True, returns True if any discrepancies were found, False otherwise. Returns None if return_check is False.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def check_s2p(self, with_database_update: bool = False, return_check: bool = False) -&gt; Optional[bool]:\n    \"\"\"\n    Verify suite2p status consistency between database and file system.\n\n    Checks for discrepancies where:\n    - Database says suite2p is done but files don't exist\n    - Files exist but database says suite2p wasn't done\n\n    Parameters\n    ----------\n    with_database_update : bool, default=False\n        If True, automatically corrects database entries when discrepancies are found.\n    return_check : bool, default=False\n        If True, returns a boolean indicating whether any discrepancies were found.\n\n    Returns\n    -------\n    bool or None\n        If return_check is True, returns True if any discrepancies were found,\n        False otherwise. Returns None if return_check is False.\n    \"\"\"\n    df = self.get_table()\n\n    # Check sessions where database says suite2p is done but files don't exist\n    check_s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n    checked_not_done = check_s2p_done.apply(lambda row: not (self.make_b2session(row).s2p_path.exists()), axis=1)\n    not_actually_done = check_s2p_done[checked_not_done]\n\n    # Check sessions where files exist but database says suite2p wasn't done\n    check_s2p_needed = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n    checked_not_needed = check_s2p_needed.apply(lambda row: self.make_b2session(row).s2p_path.exists(), axis=1)\n    not_actually_needed = check_s2p_needed[checked_not_needed]\n\n    # Print database errors to workspace\n    for idx, row in not_actually_done.iterrows():\n        print(f\"Database said suite2p has been ran, but it actually hasn't: {self.make_b2session(row).session_print()}\")\n    for idx, row in not_actually_needed.iterrows():\n        print(f\"Database said suite2p didn't run, but it already did: {self.make_b2session(row).session_print()}\")\n\n    # If with_database_update is True, correct the database\n    if with_database_update:\n        for idx, row in not_actually_done.iterrows():\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), False)\n\n        for idx, row in not_actually_needed.iterrows():\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), True)\n\n    # If return_check is requested, return True if any records were invalid\n    if return_check:\n        return checked_not_done.any() or checked_not_needed.any()\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.iter_sessions","title":"<code>iter_sessions(session_params={}, **kw_conditions)</code>","text":"<p>Iterate over sessions matching conditions.</p> <p>Parameters:</p> Name Type Description Default <code>session_params</code> <code>dict</code> <p>Additional parameters to pass to the session constructor when creating B2Session objects. These are passed through to create_b2session().</p> <code>{}</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax. Examples: <code>mouseName='ATL028'</code>, <code>imaging=True</code>, <code>sessionID=(5, '&gt;')</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>sessions</code> <code>list[B2Session]</code> <p>List of sessions matching the conditions.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def iter_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; List[B2Session]:\n    \"\"\"Iterate over sessions matching conditions.\n\n    Parameters\n    ----------\n    session_params : dict, default={}\n        Additional parameters to pass to the session constructor when creating\n        B2Session objects. These are passed through to create_b2session().\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n        Examples: ``mouseName='ATL028'``, ``imaging=True``, ``sessionID=(5, '&gt;')``\n\n    Returns\n    -------\n    sessions : list[B2Session]\n        List of sessions matching the conditions.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    sessions = []\n    for _, row in df.iterrows():\n        sessions.append(create_b2session(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params))\n    return sessions\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.iter_sessions_need_red_cell_qc","title":"<code>iter_sessions_need_red_cell_qc(**kw_conditions)</code>","text":"<p>Get list of sessions that require red cell quality control.</p> <p>Parameters:</p> Name Type Description Default <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[B2Session]</code> <p>List of session objects that need red cell QC.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def iter_sessions_need_red_cell_qc(self, **kw_conditions: Any) -&gt; List[B2Session]:\n    \"\"\"\n    Get list of sessions that require red cell quality control.\n\n    Parameters\n    ----------\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    list[B2Session]\n        List of session objects that need red cell QC.\n    \"\"\"\n    df = self.needs_red_cell_qc(return_df=True, **kw_conditions)\n    sessions = []\n    for idx, row in df.iterrows():\n        sessions.append(self.make_b2session(row))\n    return sessions\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.make_b2registration","title":"<code>make_b2registration(row, opts)</code>","text":"<p>Create a B2Registration object from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <code>opts</code> <code>B2RegistrationOpts</code> <p>Registration options to use for the session.</p> required <p>Returns:</p> Type Description <code>B2Registration</code> <p>Registration object initialized with session data and options.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def make_b2registration(self, row: pd.Series, opts: B2RegistrationOpts) -&gt; B2Registration:\n    \"\"\"\n    Create a B2Registration object from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n    opts : B2RegistrationOpts\n        Registration options to use for the session.\n\n    Returns\n    -------\n    B2Registration\n        Registration object initialized with session data and options.\n    \"\"\"\n    mouse_name, session_date, session_id = self.session_name(row)\n    return B2Registration(mouse_name, session_date, session_id, opts)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.make_b2session","title":"<code>make_b2session(row)</code>","text":"<p>Create a B2Session object from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <p>Returns:</p> Type Description <code>B2Session</code> <p>Session object initialized with data from the record.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def make_b2session(self, row: pd.Series) -&gt; B2Session:\n    \"\"\"\n    Create a B2Session object from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n\n    Returns\n    -------\n    B2Session\n        Session object initialized with data from the record.\n    \"\"\"\n    mouse_name, session_date, session_id = self.session_name(row)\n    return create_b2session(mouse_name, session_date, session_id)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.needs_red_cell_qc","title":"<code>needs_red_cell_qc(return_df=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need red cell quality control.</p> <p>Parameters:</p> Name Type Description Default <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need red cell QC. Sessions must have imaging, suite2p processing, and registration completed. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def needs_red_cell_qc(self, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need red cell quality control.\n\n    Parameters\n    ----------\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need red cell QC.\n        Sessions must have imaging, suite2p processing, and registration completed.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"vrRegistration\"] == True) &amp; (df[\"redCellQC\"] == False)]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            print(f\"Database indicates that redCellQC has not been performed for session: {self.make_b2session(row).session_print()}\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.needs_registration","title":"<code>needs_registration(skip_errors=True, return_df=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need registration preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>skip_errors</code> <code>bool</code> <p>If True, exclude sessions that had registration errors.</p> <code>True</code> <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need registration. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def needs_registration(self, skip_errors: bool = True, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need registration preprocessing.\n\n    Parameters\n    ----------\n    skip_errors : bool, default=True\n        If True, exclude sessions that had registration errors.\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need registration.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    if skip_errors:\n        df = df[df[\"vrRegistrationError\"] == False]\n    df = df[df[\"vrRegistration\"] == False]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            session = self.make_b2session(row)\n            print(f\"Session needs registration: {session.session_print()}\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.needs_s2p","title":"<code>needs_s2p(needs_qc=False, return_df=True, print_targets=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need suite2p processing or quality control.</p> <p>Parameters:</p> Name Type Description Default <code>needs_qc</code> <code>bool</code> <p>If False, returns/prints sessions that need suite2p processing. If True, returns/prints sessions that need suite2p quality control.</p> <code>False</code> <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>print_targets</code> <code>bool</code> <p>If True and return_df=False, prints suite2p target information for sessions needing processing.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def needs_s2p(\n    self, needs_qc: bool = False, return_df: bool = True, print_targets: bool = True, **kw_conditions: Any\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need suite2p processing or quality control.\n\n    Parameters\n    ----------\n    needs_qc : bool, default=False\n        If False, returns/prints sessions that need suite2p processing.\n        If True, returns/prints sessions that need suite2p quality control.\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    print_targets : bool, default=True\n        If True and return_df=False, prints suite2p target information for sessions needing processing.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    if needs_qc:\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"suite2pQC\"] == False)]\n    else:\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            session = self.make_b2session(row)\n            if needs_qc:\n                print(f\"Database indicates that suite2p has been run but not QC'd: {session.session_print()}\")\n            else:\n                print(f\"Database indicates that suite2p has not been run: {session.session_print()}\")\n                if print_targets:\n                    mouse_name, session_date, session_id = self.session_name(row)\n                    s2p_targets(mouse_name, session_date, session_id)\n                    print(\"\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.print_registration_errors","title":"<code>print_registration_errors(**kw_conditions)</code>","text":"<p>Print registration errors for sessions that failed registration.</p> <p>Parameters:</p> Name Type Description Default <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> Source code in <code>vrAnalysis/database.py</code> <pre><code>def print_registration_errors(self, **kw_conditions: Any) -&gt; None:\n    \"\"\"\n    Print registration errors for sessions that failed registration.\n\n    Parameters\n    ----------\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    for idx, row in df[df[\"vrRegistrationError\"] == True].iterrows():\n        print(f\"{'/'.join(self.session_name(row))} had error: {row['vrRegistrationException']}\")\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.register_record","title":"<code>register_record(record, raise_exception=False, imaging=None)</code>","text":"<p>Perform registration preprocessing for a single session record.</p> <p>Creates a B2Registration object and runs preprocessing. Updates the database with success/failure status and error information if applicable.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Series</code> <p>Database record containing session information.</p> required <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting. If None (default), uses the value from the database record. If True or False, overrides the database value.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (success, data_size) where: - success: bool indicating if registration succeeded - data_size: int size in bytes of registered oneData (0 if failed)</p> Notes <p>On failure, clears all oneData files and updates database error fields. On success, updates registration status and date in the database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def register_record(self, record: pd.Series, raise_exception: bool = False, imaging: Optional[bool] = None) -&gt; Tuple[bool, int]:\n    \"\"\"\n    Perform registration preprocessing for a single session record.\n\n    Creates a B2Registration object and runs preprocessing. Updates the database\n    with success/failure status and error information if applicable.\n\n    Parameters\n    ----------\n    record : pandas.Series\n        Database record containing session information.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting. If None (default), uses the value from the\n        database record. If True or False, overrides the database value.\n\n    Returns\n    -------\n    tuple\n        Tuple of (success, data_size) where:\n        - success: bool indicating if registration succeeded\n        - data_size: int size in bytes of registered oneData (0 if failed)\n\n    Notes\n    -----\n    On failure, clears all oneData files and updates database error fields.\n    On success, updates registration status and date in the database.\n    \"\"\"\n    opts = B2RegistrationOpts()\n    opts.imaging = bool(imaging) if imaging is not None else bool(record[\"imaging\"])\n    opts.facecam = bool(record[\"faceCamera\"])\n    opts.vrBehaviorVersion = record[\"vrBehaviorVersion\"]\n    b2reg = self.make_b2registration(record, opts)\n    try:\n        print(f\"Registering data for session: {b2reg.session_print()}\")\n        b2reg.register()\n    except Exception as ex:\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), True)\n            cursor.execute(\n                self.create_update_statement(\"vrRegistrationException\", record[self.uid]),\n                str(ex),\n            )\n        if raise_exception:\n            raise ex\n        print(f\"The following exception was raised when trying to preprocess session: {b2reg.session_print()}. Clearing all oneData.\")\n        b2reg.clear_one_data(certainty=True)\n        error_print(f\"Last traceback: {traceback.extract_tb(ex.__traceback__, limit=-1)}\")\n        error_print(f\"Exception: {ex}\")\n        # If failed, return (False, 0)\n        out = (False, 0)\n    else:\n        with self.open_cursor(commit_changes=True) as cursor:\n            # Tell the database that vrRegistration was performed and the time of processing\n            cursor.execute(self.create_update_statement(\"vrRegistration\", record[self.uid]), True)\n            cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), False)\n            cursor.execute(self.create_update_statement(\"vrRegistrationException\", record[self.uid]), \"\")\n            cursor.execute(\n                self.create_update_statement(\"vrRegistrationDate\", record[self.uid]),\n                datetime.now(),\n            )\n        # If successful, return (True, size of registered oneData)\n        out = (True, sum([one_file.stat().st_size for one_file in b2reg.get_saved_one()]))\n        print(f\"Session {b2reg.session_print()} registered with {readable_bytes(out[1])} oneData.\")\n    finally:\n        del b2reg\n    return out\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.register_sessions","title":"<code>register_sessions(max_data=30000000000.0, skip_errors=True, raise_exception=False, imaging=None)</code>","text":"<p>Register multiple sessions that need registration.</p> <p>Processes sessions in batches, stopping when the total data size limit is reached. Provides progress updates including accumulated data size and estimates.</p> <p>Parameters:</p> Name Type Description Default <code>max_data</code> <code>float</code> <p>Maximum total data size (in bytes) to process before stopping. Default is 30 GB.</p> <code>30e9</code> <code>skip_errors</code> <code>bool</code> <p>If True, skip sessions that had previous registration errors.</p> <code>True</code> <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting for all sessions. If None (default), uses the value from each session's database record. If True or False, overrides the database value for all sessions to enable or disable imaging processing.</p> <code>None</code> Notes <p>Prints progress information including: - Accumulated oneData registered - Average data size per session - Estimated remaining data to process</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def register_sessions(\n    self,\n    max_data: float = 30e9,\n    skip_errors: bool = True,\n    raise_exception: bool = False,\n    imaging: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"\n    Register multiple sessions that need registration.\n\n    Processes sessions in batches, stopping when the total data size limit is reached.\n    Provides progress updates including accumulated data size and estimates.\n\n    Parameters\n    ----------\n    max_data : float, default=30e9\n        Maximum total data size (in bytes) to process before stopping.\n        Default is 30 GB.\n    skip_errors : bool, default=True\n        If True, skip sessions that had previous registration errors.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting for all sessions. If None (default), uses the\n        value from each session's database record. If True or False, overrides the\n        database value for all sessions to enable or disable imaging processing.\n\n    Notes\n    -----\n    Prints progress information including:\n    - Accumulated oneData registered\n    - Average data size per session\n    - Estimated remaining data to process\n    \"\"\"\n    count_sessions = 0\n    total_one_data = 0.0\n    df_to_register = self.needs_registration(skip_errors=skip_errors)\n\n    for idx, (_, row) in enumerate(df_to_register.iterrows()):\n        if total_one_data &gt; max_data:\n            print(f\"\\nMax data limit reached. Total processed: {readable_bytes(total_one_data)}. Limit: {readable_bytes(max_data)}\")\n            return\n        print(\"\")\n        out = self.register_record(row, raise_exception=raise_exception, imaging=imaging)\n        if out[0]:\n            count_sessions += 1  # count successful sessions\n            total_one_data += out[1]  # accumulated oneData registered\n            estimate_remaining = len(df_to_register) - idx - 1\n            print(\n                f\"Accumulated oneData registered: {readable_bytes(total_one_data)}. \"\n                f\"Averaging: {readable_bytes(total_one_data/count_sessions)} / session. \"\n                f\"Estimate remaining: {readable_bytes(total_one_data/count_sessions*estimate_remaining)}\"\n            )\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.register_single_session","title":"<code>register_single_session(mouse_name, session_date, session_id, raise_exception=False, imaging=None)</code>","text":"<p>Register a single session by its identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Mouse name identifier.</p> required <code>session_date</code> <code>str</code> <p>Session date in 'YYYY-MM-DD' format.</p> required <code>session_id</code> <code>str or int</code> <p>Session ID.</p> required <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting. If None (default), uses the value from the database record. If True or False, overrides the database value to enable or disable imaging processing during registration.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool or None</code> <p>True if registration succeeded, False if failed, None if session not found.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def register_single_session(\n    self,\n    mouse_name: str,\n    session_date: str,\n    session_id: Union[str, int],\n    raise_exception: bool = False,\n    imaging: Optional[bool] = None,\n) -&gt; Optional[bool]:\n    \"\"\"\n    Register a single session by its identifiers.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Mouse name identifier.\n    session_date : str\n        Session date in 'YYYY-MM-DD' format.\n    session_id : str or int\n        Session ID.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting. If None (default), uses the value from the\n        database record. If True or False, overrides the database value to enable\n        or disable imaging processing during registration.\n\n    Returns\n    -------\n    bool or None\n        True if registration succeeded, False if failed, None if session not found.\n    \"\"\"\n    record = self.get_record(mouse_name, session_date, session_id)\n    if record is None:\n        print(f\"Session {'/'.join([mouse_name, session_date, session_id])} is not in the database\")\n        return\n    out = self.register_record(record, raise_exception=raise_exception, imaging=imaging)\n    return out[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.session_name","title":"<code>session_name(row)</code>","text":"<p>Extract session identifiers from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (mouse_name, session_date, session_id) where session_date is formatted as 'YYYY-MM-DD' and session_id is converted to string.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def session_name(self, row: pd.Series) -&gt; Tuple[str, str, str]:\n    \"\"\"\n    Extract session identifiers from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n\n    Returns\n    -------\n    tuple\n        Tuple of (mouse_name, session_date, session_id) where session_date is\n        formatted as 'YYYY-MM-DD' and session_id is converted to string.\n    \"\"\"\n    mouse_name = row[\"mouseName\"]\n    session_date = row[\"sessionDate\"].strftime(\"%Y-%m-%d\")\n    session_id = str(row[\"sessionID\"])\n    return mouse_name, session_date, session_id\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.set_red_cell_qc","title":"<code>set_red_cell_qc(mouse_name, date_string, session_id, state=True)</code>","text":"<p>Set the red cell QC status for a specific session.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Mouse name identifier.</p> required <code>date_string</code> <code>str</code> <p>Session date in 'YYYY-MM-DD' format.</p> required <code>session_id</code> <code>str or int</code> <p>Session ID.</p> required <code>state</code> <code>bool</code> <p>Red cell QC status to set. If True, also sets the QC date to now.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful, False otherwise.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def set_red_cell_qc(self, mouse_name: str, date_string: str, session_id: Union[str, int], state: bool = True) -&gt; bool:\n    \"\"\"\n    Set the red cell QC status for a specific session.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Mouse name identifier.\n    date_string : str\n        Session date in 'YYYY-MM-DD' format.\n    session_id : str or int\n        Session ID.\n    state : bool, default=True\n        Red cell QC status to set. If True, also sets the QC date to now.\n\n    Returns\n    -------\n    bool\n        True if update was successful, False otherwise.\n    \"\"\"\n    record = self.get_record(mouse_name, date_string, session_id)\n    if record is None:\n        print(f\"Could not find session {mouse_name}/{date_string}/{session_id} in database.\")\n        return False\n\n    try:\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(self.create_update_statement(\"redCellQC\", record[self.uid]), state)\n            if state == True:\n                # If setting red cell QC to true, add the date\n                cursor.execute(\n                    self.create_update_statement(\"redCellQCDate\", record[self.uid]),\n                    datetime.now(),\n                )\n            else:\n                # Otherwise remove the date\n                cursor.execute(self.create_update_statement(\"redCellQCDate\", record[self.uid]), \"\")\n        return True\n\n    except Exception as ex:\n        print(f\"Failed to update database for session: {mouse_name}/{date_string}/{session_id}\")\n        print(f\"Error: {ex}\")\n        return False\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.update_red_cell_qc_date_time","title":"<code>update_red_cell_qc_date_time()</code>","text":"<p>Update red cell QC dates in the database based on file modification times.</p> <p>For all sessions where red cell QC is complete, finds the most recent modification time of relevant red cell QC files and updates the redCellQCDate field in the database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def update_red_cell_qc_date_time(self) -&gt; None:\n    \"\"\"\n    Update red cell QC dates in the database based on file modification times.\n\n    For all sessions where red cell QC is complete, finds the most recent\n    modification time of relevant red cell QC files and updates the\n    redCellQCDate field in the database.\n    \"\"\"\n    relevant_one_files = [\n        \"mpciROIs.redCellIdx.npy\",\n        \"mpciROIs.redCellManualAssignment.npy\",\n        \"parametersRed*\",  # wildcard because there are multiple possibilities\n    ]\n\n    df = self.get_table()\n    red_cell_qc_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"redCellQC\"] == True)]\n    uids = red_cell_qc_done[self.uid].tolist()\n    rc_edit_date = []\n    for idx, row in red_cell_qc_done.iterrows():\n        session = self.make_b2session(row)  # create vrSession to point to session folder\n        c_latest_mod = 0\n        for f in relevant_one_files:\n            for file in session.one_path.rglob(f):\n                c_latest_mod = max(file.stat().st_mtime, c_latest_mod)\n        c_date_time = datetime.fromtimestamp(c_latest_mod)\n        rc_edit_date.append(c_date_time)  # get red cell QC file modification date\n\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(self.create_update_many_statement(\"redCellQCDate\"), zip(rc_edit_date, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.update_s2p_date_time","title":"<code>update_s2p_date_time()</code>","text":"<p>Update suite2p creation dates in the database based on file modification times.</p> <p>For all sessions where suite2p processing is complete, finds the most recent file modification time in the suite2p output directory and updates the suite2pDate field in the database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def update_s2p_date_time(self) -&gt; None:\n    \"\"\"\n    Update suite2p creation dates in the database based on file modification times.\n\n    For all sessions where suite2p processing is complete, finds the most recent\n    file modification time in the suite2p output directory and updates the\n    suite2pDate field in the database.\n    \"\"\"\n    df = self.get_table()\n    s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n    uids = s2p_done[self.uid].tolist()\n    s2p_creation_date = []\n    for idx, row in s2p_done.iterrows():\n        session = self.make_b2session(row)  # create vrSession to point to session folder\n        c_latest_mod = 0\n        for p in session.s2p_path.rglob(\"*\"):\n            if not (p.is_dir()):\n                c_latest_mod = max(p.stat().st_mtime, c_latest_mod)\n        c_date_time = datetime.fromtimestamp(c_latest_mod)\n        s2p_creation_date.append(c_date_time)  # get suite2p path creation date\n\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(self.create_update_many_statement(\"suite2pDate\"), zip(s2p_creation_date, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.get_database","title":"<code>get_database(db_name)</code>","text":"<p>Retrieve an appropriate database object instance.</p> <p>This function retrieves metadata for the specified database and instantiates the appropriate database class (BaseDatabase or a subclass like SessionDatabase).</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database to retrieve.</p> required <p>Returns:</p> Type Description <code>BaseDatabase or SessionDatabase</code> <p>An instance of the appropriate database class as specified in the metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the constructor specified in metadata is not a subclass of BaseDatabase.</p> See Also <p>get_database_metadata : Retrieve metadata for a database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_database(db_name: str) -&gt; Union[\"BaseDatabase\", \"SessionDatabase\"]:\n    \"\"\"\n    Retrieve an appropriate database object instance.\n\n    This function retrieves metadata for the specified database and instantiates\n    the appropriate database class (BaseDatabase or a subclass like SessionDatabase).\n\n    Parameters\n    ----------\n    db_name : str\n        The name of the database to retrieve.\n\n    Returns\n    -------\n    BaseDatabase or SessionDatabase\n        An instance of the appropriate database class as specified in the metadata.\n\n    Raises\n    ------\n    ValueError\n        If the constructor specified in metadata is not a subclass of BaseDatabase.\n\n    See Also\n    --------\n    get_database_metadata : Retrieve metadata for a database.\n    \"\"\"\n    metadata = get_database_metadata(db_name)\n    if \"constructor\" in metadata:\n        if issubclass(metadata[\"constructor\"], BaseDatabase):\n            constructor = metadata[\"constructor\"]  # get class constructor method for this database\n        else:\n            raise ValueError(f\"{metadata['constructor']} must be a subclass of the `BaseDatabase` class!\")\n    else:\n        constructor = BaseDatabase\n    return constructor(db_name)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.get_database_metadata","title":"<code>get_database_metadata(db_name)</code>","text":"<p>Retrieve metadata for a specified database.</p> <p>This function retrieves metadata for a specified database from the <code>dbdict</code> dictionary. The <code>dbdict</code> dictionary contains the database paths, names, and primary table name.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database for which to retrieve metadata.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing metadata for the specified database. It requires the following keys:     'db_path': path to the database file     'db_name': name of the database file     'db_ext': extension of the database file     'table_name': name of the table to use     'uid': name of the field defining a unique ID for each row in the table     'backup_path': path to the database backup (None if there isn't one)     'unique_fields': list of names of fields for which there should only be one                     database row per combination of the values in unique_fields                     note: assumes string, but make it a tuple for different types     'default_conditions': dictionary containing key-value pairs of any default                          conditions to filter by when retrieving table data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>db_name</code> is not recognized as a valid database name.</p> Example <p>metadata = get_database_metadata('vrSessions') print(metadata['db_path']) 'C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement' print(metadata['db_name']) 'vrDatabase'</p> Notes <ul> <li>The <code>dbdict</code> dictionary contains metadata for recognized databases.</li> <li>Edit this function to specify the path, database, and primary table on your computer.</li> </ul> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_database_metadata(db_name: str) -&gt; dict:\n    \"\"\"\n    Retrieve metadata for a specified database.\n\n    This function retrieves metadata for a specified database from the `dbdict` dictionary.\n    The `dbdict` dictionary contains the database paths, names, and primary table name.\n\n    Parameters\n    ----------\n    db_name : str\n        The name of the database for which to retrieve metadata.\n\n    Returns\n    -------\n    dict\n        A dictionary containing metadata for the specified database.\n        It requires the following keys:\n            'db_path': path to the database file\n            'db_name': name of the database file\n            'db_ext': extension of the database file\n            'table_name': name of the table to use\n            'uid': name of the field defining a unique ID for each row in the table\n            'backup_path': path to the database backup (None if there isn't one)\n            'unique_fields': list of names of fields for which there should only be one\n                            database row per combination of the values in unique_fields\n                            note: assumes string, but make it a tuple for different types\n            'default_conditions': dictionary containing key-value pairs of any default\n                                 conditions to filter by when retrieving table data\n\n    Raises\n    ------\n    ValueError\n        If the provided `db_name` is not recognized as a valid database name.\n\n    Example\n    -------\n    &gt;&gt;&gt; metadata = get_database_metadata('vrSessions')\n    &gt;&gt;&gt; print(metadata['db_path'])\n    'C:\\\\Users\\\\andrew\\\\Documents\\\\localData\\\\vrDatabaseManagement'\n    &gt;&gt;&gt; print(metadata['db_name'])\n    'vrDatabase'\n\n    Notes\n    -----\n    - The `dbdict` dictionary contains metadata for recognized databases.\n    - Edit this function to specify the path, database, and primary table on your computer.\n    \"\"\"\n\n    dbdict = {\n        \"vrSessions\": {\n            \"db_path\": r\"C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"sessiondb\",\n            \"uid\": \"uSessionID\",\n            \"backup_path\": r\"D:\\localData\\vrDatabaseManagement\",\n            \"unique_fields\": [(\"mouseName\", str), (\"sessionDate\", datetime), (\"sessionID\", int)],\n            \"default_conditions\": {\n                \"sessionQC\": True,\n            },\n            \"constructor\": SessionDatabase,\n        },\n        \"vrMice\": {\n            \"db_path\": r\"C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"mousedb\",\n            \"uid\": \"uMouseID\",\n            \"backup_path\": r\"D:\\localData\\vrDatabaseManagement\",\n            \"unique_fields\": [(\"mouseName\", str)],\n            \"default_conditions\": {},\n            \"constructor\": BaseDatabase,\n        },\n    }\n    if db_name not in dbdict.keys():\n        raise ValueError(f\"Did not recognize database={db_name}, valid database names are: {[key for key in dbdict.keys()]}\")\n    return dbdict[db_name]\n</code></pre>"},{"location":"api/processors/","title":"Processors API Reference","text":""},{"location":"api/processors/#vrAnalysis.processors","title":"<code>processors</code>","text":"<p>Processors module for vrAnalysis.</p> <p>This module provides data processing pipelines that transform session data into analysis-ready formats.</p>"},{"location":"api/processors/#vrAnalysis.processors.Maps","title":"<code>Maps</code>  <code>dataclass</code>","text":"<p>Base class for occupancy, speed, and spike maps.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass Maps:\n    \"\"\"Base class for occupancy, speed, and spike maps.\"\"\"\n\n    occmap: np.ndarray | list[np.ndarray]\n    speedmap: np.ndarray | list[np.ndarray]\n    spkmap: np.ndarray | list[np.ndarray]\n    by_environment: bool\n    rois_first: bool\n    environments: list[int] | None = None\n    distcenters: np.ndarray | None = None\n    _averaged: bool = field(default=False, init=False)\n\n    def __post_init__(self):\n        if self.occmap is None or self.speedmap is None or self.spkmap is None:\n            raise ValueError(\"occmap, speedmap, and spkmap must be provided\")\n\n        if self.by_environment:\n            if self.environments is None:\n                raise ValueError(\"environments must be provided if by_environment is True\")\n            if not isinstance(self.occmap, list) or not isinstance(self.speedmap, list) or not isinstance(self.spkmap, list):\n                raise ValueError(\"occmap, speedmap, and spkmap must be lists if by_environment is True\")\n        else:\n            if isinstance(self.occmap, list) or isinstance(self.speedmap, list) or isinstance(self.spkmap, list):\n                raise ValueError(\"occmap, speedmap, and spkmap must be single arrays if by_environment is False\")\n\n        if not self.by_environment:\n            spkmap_shape = self.spkmap.shape[1:] if self.rois_first else self.spkmap.shape[:2]\n            if not (self.occmap.shape == self.speedmap.shape == spkmap_shape):\n                raise ValueError(\"occmap, speedmap, and spkmap must have the same shape\")\n        else:\n            if not (len(self.occmap) == len(self.speedmap) == len(self.spkmap) == len(self.environments)):\n                raise ValueError(\"occmap, speedmap, and spkmap must have the same number of environments\")\n            for i in range(len(self.environments)):\n                spkmap_shape = self.spkmap[i].shape[1:] if self.rois_first else self.spkmap[i].shape[:2]\n                if not (self.occmap[i].shape == self.speedmap[i].shape == spkmap_shape):\n                    raise ValueError(\"occmap, speedmap, and spkmap must have the same shape for each environment\")\n            roi_axis = 0 if self.rois_first else -1\n            rois_per_env = [spkmap.shape[roi_axis] for spkmap in self.spkmap]\n            if not all([rpe == rois_per_env[0] for rpe in rois_per_env]):\n                raise ValueError(\"All environments must have the same number of ROIs\")\n\n    def __repr__(self) -&gt; str:\n        # Get number of positions\n        if self.by_environment:\n            num_positions = self.occmap[0].shape[-1]\n        else:\n            num_positions = self.occmap.shape[-1]\n        # Get number of trials\n        if self._averaged:\n            num_trials = \"averaged\"\n        else:\n            if self.by_environment:\n                num_trials = [occmap.shape[0] for occmap in self.occmap]\n                num_trials = \"{\" + \", \".join([str(nt) for nt in num_trials]) + \"}\"\n            else:\n                num_trials = self.occmap.shape[0]\n        # Get number of ROIs\n        if self.by_environment:\n            num_rois = self.spkmap[0].shape[0] if self.rois_first else self.spkmap[0].shape[1]\n        else:\n            num_rois = self.spkmap.shape[0] if self.rois_first else self.spkmap.shape[1]\n        environments = f\", environments={{{', '.join([str(env) for env in self.environments])}}}\" if self.by_environment else \"\"\n        return f\"Maps(num_trials={num_trials}, num_positions={num_positions}, num_rois={num_rois}{environments}, rois_first={self.rois_first})\"\n\n    @classmethod\n    def create_raw_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n        return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=False)\n\n    @classmethod\n    def create_processed_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n        return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=True)\n\n    @classmethod\n    def create_environment_maps(\n        cls, occmap: list[np.ndarray], speedmap: list[np.ndarray], spkmap: list[np.ndarray], environments: list[int], distcenters: np.ndarray = None\n    ) -&gt; \"Maps\":\n        return cls(\n            occmap=occmap,\n            speedmap=speedmap,\n            spkmap=spkmap,\n            distcenters=distcenters,\n            environments=environments,\n            by_environment=True,\n            rois_first=True,\n        )\n\n    @classmethod\n    def map_types(self) -&gt; List[str]:\n        return [\"occmap\", \"speedmap\", \"spkmap\"]\n\n    def __getitem__(self, key: str) -&gt; np.ndarray:\n        return getattr(self, key)\n\n    def __setitem__(self, key: str, value: np.ndarray):\n        setattr(self, key, value)\n\n    def _get_position_axis(self, mapname: str) -&gt; int:\n        \"\"\"The only time the position axis isn't the last one is for spkmap when rois_first is False\"\"\"\n        average_offset = -1 if self._averaged else 0\n        if mapname == \"spkmap\" and not self.rois_first:\n            return -2 + average_offset\n        else:\n            return -1\n\n    def filter_positions(self, idx_positions: np.ndarray) -&gt; None:\n        if self.distcenters is not None:\n            self.distcenters = self.distcenters[idx_positions]\n        for mapname in self.map_types():\n            axis = self._get_position_axis(mapname)\n            if self.by_environment:\n                self[mapname] = [np.take(x, idx_positions, axis=axis) for x in self[mapname]]\n            else:\n                self[mapname] = np.take(self[mapname], idx_positions, axis=axis)\n\n    def filter_rois(self, idx_rois: np.ndarray) -&gt; None:\n        axis = 0 if self.rois_first else -1\n        if self.by_environment:\n            self.spkmap = [np.take(x, idx_rois, axis=axis) for x in self.spkmap]\n        else:\n            self.spkmap = np.take(self.spkmap, idx_rois, axis=axis)\n\n    def filter_environments(self, environments: list[int]) -&gt; None:\n        if self.by_environment:\n            idx_to_requested_env = [i for i, env in enumerate(self.environments) if env in environments]\n            self.occmap = [self.occmap[i] for i in idx_to_requested_env]\n            self.speedmap = [self.speedmap[i] for i in idx_to_requested_env]\n            self.spkmap = [self.spkmap[i] for i in idx_to_requested_env]\n            self.environments = [self.environments[i] for i in idx_to_requested_env]\n        else:\n            raise ValueError(\"Cannot filter environments when maps aren't separated by environment!\")\n\n    def pop_nan_positions(self) -&gt; None:\n        \"\"\"Remove positions with nans from the maps\"\"\"\n        if self.by_environment:\n            idx_valid_positions = np.where(~np.any(np.stack([np.any(np.isnan(occmap), axis=0) for occmap in self.occmap], axis=0), axis=0))[0]\n        else:\n            idx_valid_positions = np.where(~np.any(np.isnan(self.occmap), axis=0))[0]\n        self.filter_positions(idx_valid_positions)\n\n    def smooth_maps(self, positions: np.ndarray, kernel_width: float) -&gt; None:\n        \"\"\"Smooth the maps using a Gaussian kernel\"\"\"\n        kernel = get_gauss_kernel(positions, kernel_width)\n\n        # Replace nans with 0s\n        if self.by_environment:\n            idxnan = [np.isnan(occmap) for occmap in self.occmap]\n        else:\n            idxnan = np.isnan(self.occmap)\n\n        if self.rois_first:\n            # Move the rois axis to the last axis\n            if self.by_environment:\n                self.spkmap = [np.moveaxis(map, 0, -1) for map in self.spkmap]\n            else:\n                self.spkmap = np.moveaxis(self.spkmap, 0, -1)\n\n        for mapname in self.map_types():\n            if self.by_environment:\n                for ienv, inanenv in enumerate(idxnan):\n                    self[mapname][ienv][inanenv] = 0\n            else:\n                self[mapname][idxnan] = 0\n\n        for mapname in self.map_types():\n            # Since we moved ROIs to the last axis position will be axis=1 for all map types\n            if self.by_environment:\n                self[mapname] = [convolve_toeplitz(map, kernel, axis=1) for map in self[mapname]]\n            else:\n                self[mapname] = convolve_toeplitz(self[mapname], kernel, axis=1)\n\n        # Put nans back in place\n        for mapname in self.map_types():\n            if self.by_environment:\n                for ienv, inanenv in enumerate(idxnan):\n                    self[mapname][ienv][inanenv] = np.nan\n            else:\n                self[mapname][idxnan] = np.nan\n\n        # Move the rois axis back to the first axis\n        if self.rois_first:\n            if self.by_environment:\n                self.spkmap = [np.moveaxis(map, -1, 0) for map in self.spkmap]\n            else:\n                self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n\n    def average_trials(self, keepdims: bool = False) -&gt; None:\n        \"\"\"Average the trials within each environment\"\"\"\n        if self._averaged:\n            return\n        for mapname in self.map_types():\n            axis = 1 if mapname == \"spkmap\" and self.rois_first else 0\n            if self.by_environment:\n                self[mapname] = [ss.mean(map, axis=axis, keepdims=keepdims) for map in self[mapname]]\n            else:\n                self[mapname] = ss.mean(self[mapname], axis=axis, keepdims=keepdims)\n        self._averaged = True\n\n    def nbytes(self) -&gt; int:\n        num_bytes = 0\n        for name in self.map_types():\n            if self.by_environment:\n                num_bytes += sum(x.nbytes for x in getattr(self, name))\n            else:\n                num_bytes += getattr(self, name).nbytes\n        return num_bytes\n\n    def raw_to_processed(self, positions: np.ndarray, smooth_width: float | None = None) -&gt; \"Maps\":\n        \"\"\"Convert raw maps to processed maps\"\"\"\n        if smooth_width is not None:\n            self.smooth_maps(positions, smooth_width)\n\n        self.speedmap = correct_map(self.occmap, self.speedmap)\n        self.spkmap = correct_map(self.occmap, self.spkmap)\n\n        # Change spkmap to be ROIs first\n        self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n        self.rois_first = True\n\n        return self\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.Maps.average_trials","title":"<code>average_trials(keepdims=False)</code>","text":"<p>Average the trials within each environment</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def average_trials(self, keepdims: bool = False) -&gt; None:\n    \"\"\"Average the trials within each environment\"\"\"\n    if self._averaged:\n        return\n    for mapname in self.map_types():\n        axis = 1 if mapname == \"spkmap\" and self.rois_first else 0\n        if self.by_environment:\n            self[mapname] = [ss.mean(map, axis=axis, keepdims=keepdims) for map in self[mapname]]\n        else:\n            self[mapname] = ss.mean(self[mapname], axis=axis, keepdims=keepdims)\n    self._averaged = True\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.Maps.pop_nan_positions","title":"<code>pop_nan_positions()</code>","text":"<p>Remove positions with nans from the maps</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def pop_nan_positions(self) -&gt; None:\n    \"\"\"Remove positions with nans from the maps\"\"\"\n    if self.by_environment:\n        idx_valid_positions = np.where(~np.any(np.stack([np.any(np.isnan(occmap), axis=0) for occmap in self.occmap], axis=0), axis=0))[0]\n    else:\n        idx_valid_positions = np.where(~np.any(np.isnan(self.occmap), axis=0))[0]\n    self.filter_positions(idx_valid_positions)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.Maps.raw_to_processed","title":"<code>raw_to_processed(positions, smooth_width=None)</code>","text":"<p>Convert raw maps to processed maps</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def raw_to_processed(self, positions: np.ndarray, smooth_width: float | None = None) -&gt; \"Maps\":\n    \"\"\"Convert raw maps to processed maps\"\"\"\n    if smooth_width is not None:\n        self.smooth_maps(positions, smooth_width)\n\n    self.speedmap = correct_map(self.occmap, self.speedmap)\n    self.spkmap = correct_map(self.occmap, self.spkmap)\n\n    # Change spkmap to be ROIs first\n    self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n    self.rois_first = True\n\n    return self\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.Maps.smooth_maps","title":"<code>smooth_maps(positions, kernel_width)</code>","text":"<p>Smooth the maps using a Gaussian kernel</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def smooth_maps(self, positions: np.ndarray, kernel_width: float) -&gt; None:\n    \"\"\"Smooth the maps using a Gaussian kernel\"\"\"\n    kernel = get_gauss_kernel(positions, kernel_width)\n\n    # Replace nans with 0s\n    if self.by_environment:\n        idxnan = [np.isnan(occmap) for occmap in self.occmap]\n    else:\n        idxnan = np.isnan(self.occmap)\n\n    if self.rois_first:\n        # Move the rois axis to the last axis\n        if self.by_environment:\n            self.spkmap = [np.moveaxis(map, 0, -1) for map in self.spkmap]\n        else:\n            self.spkmap = np.moveaxis(self.spkmap, 0, -1)\n\n    for mapname in self.map_types():\n        if self.by_environment:\n            for ienv, inanenv in enumerate(idxnan):\n                self[mapname][ienv][inanenv] = 0\n        else:\n            self[mapname][idxnan] = 0\n\n    for mapname in self.map_types():\n        # Since we moved ROIs to the last axis position will be axis=1 for all map types\n        if self.by_environment:\n            self[mapname] = [convolve_toeplitz(map, kernel, axis=1) for map in self[mapname]]\n        else:\n            self[mapname] = convolve_toeplitz(self[mapname], kernel, axis=1)\n\n    # Put nans back in place\n    for mapname in self.map_types():\n        if self.by_environment:\n            for ienv, inanenv in enumerate(idxnan):\n                self[mapname][ienv][inanenv] = np.nan\n        else:\n            self[mapname][idxnan] = np.nan\n\n    # Move the rois axis back to the first axis\n    if self.rois_first:\n        if self.by_environment:\n            self.spkmap = [np.moveaxis(map, -1, 0) for map in self.spkmap]\n        else:\n            self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapParams","title":"<code>SpkmapParams</code>  <code>dataclass</code>","text":"<p>Parameters for spike map processing.</p> <p>Contains configuration settings that control how spike maps are processed, including distance steps, speed thresholds, and standardization options.</p> <p>Parameters:</p> Name Type Description Default <code>dist_step</code> <code>float</code> <p>Step size for distance calculations in spatial units</p> <code>1</code> <code>speed_threshold</code> <code>float</code> <p>Minimum speed threshold for valid movement periods</p> <code>1.0</code> <code>speed_max_allowed</code> <code>float</code> <p>Maximum speed allowed for valid movement periods (default is no maximum, can be useful when behavioral computer allows jumps in position which are usually due to hardware issues</p> <code>np.inf</code> <code>full_trial_flexibility</code> <code>float | None</code> <p>Flexibility parameter for trial alignment. If None, no flexibility</p> <code>None</code> <code>standardize_spks</code> <code>bool</code> <p>Whether to standardize spike counts by dividing by the standard deviation</p> <code>True</code> <code>smooth_width</code> <code>float | None</code> <p>Width of the Gaussian smoothing kernel to apply to the maps (width in spatial units)</p> <code>1</code> <code>reliability_method</code> <code>str</code> <p>Method to use for calculating reliability</p> <code>\"leave_one_out\"</code> <code>autosave</code> <code>bool</code> <p>Whether to save the cache automatically</p> <code>True</code> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass SpkmapParams:\n    \"\"\"Parameters for spike map processing.\n\n    Contains configuration settings that control how spike maps are processed,\n    including distance steps, speed thresholds, and standardization options.\n\n    Parameters\n    ----------\n    dist_step : float, default=1\n        Step size for distance calculations in spatial units\n    speed_threshold : float, default=1.0\n        Minimum speed threshold for valid movement periods\n    speed_max_allowed : float, default=np.inf\n        Maximum speed allowed for valid movement periods (default is no maximum,\n        can be useful when behavioral computer allows jumps in position which\n        are usually due to hardware issues\n    full_trial_flexibility : float | None, default=None\n        Flexibility parameter for trial alignment. If None, no flexibility\n    standardize_spks : bool, default=True\n        Whether to standardize spike counts by dividing by the standard deviation\n    smooth_width : float | None, default=1\n        Width of the Gaussian smoothing kernel to apply to the maps (width in spatial units)\n    reliability_method : str, default=\"leave_one_out\"\n        Method to use for calculating reliability\n    autosave : bool, default=True\n        Whether to save the cache automatically\n    \"\"\"\n\n    dist_step: float = 1.0\n    speed_threshold: float = 1.0\n    speed_max_allowed: float = np.inf\n    full_trial_flexibility: Union[float, None] = 3.0\n    standardize_spks: bool = True\n    smooth_width: Union[float, None] = 1.0\n    reliability_method: str = \"leave_one_out\"\n    autosave: bool = False\n\n    def __repr__(self) -&gt; str:\n        class_fields = fields(self)\n        lines = []\n        for field in class_fields:\n            field_name = field.name\n            field_value = getattr(self, field_name)\n            lines.append(f\"{field_name}={repr(field_value)}\")\n\n        class_name = self.__class__.__name__\n        joined_lines = \",\\n    \".join(lines)\n        return f\"{class_name}(\\n    {joined_lines}\\n)\"\n\n    @classmethod\n    def from_dict(cls, params_dict: dict) -&gt; \"SpkmapParams\":\n        \"\"\"Create a SpkmapParams instance from a dictionary, using defaults for missing values\"\"\"\n        return cls(**{k: params_dict[k] for k in params_dict})\n\n    @classmethod\n    def from_path(cls, path: Path) -&gt; \"SpkmapParams\":\n        \"\"\"Create a SpkmapParams instance from a json file\"\"\"\n        with open(path, \"r\") as f:\n            return cls.from_dict(json.load(f))\n\n    def compare(self, other: \"SpkmapParams\", filter_keys: Optional[List[str]] = None) -&gt; bool:\n        \"\"\"Compare two SpkmapParams instances\"\"\"\n        if filter_keys is None:\n            return self == other\n        else:\n            return all(getattr(self, key) == getattr(other, key) for key in filter_keys)\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save the parameters to a json file\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(self), f, sort_keys=True)\n\n    def __post_init__(self):\n        if self.dist_step &lt;= 0:\n            raise ValueError(\"dist_step must be positive\")\n        if self.speed_threshold &lt;= 0:\n            raise ValueError(\"speed_threshold must be positive\")\n        if self.full_trial_flexibility is not None and self.full_trial_flexibility &lt; 0:\n            raise ValueError(\"If used, full_trial_flexibility must be nonnegative (can also be None)\")\n        if self.smooth_width is not None and self.smooth_width &lt;= 0:\n            raise ValueError(\"smooth_width must be positive (can also be None)\")\n        # Convert floats to floats when not None\n        self.dist_step = float(self.dist_step)\n        self.speed_threshold = float(self.speed_threshold)\n        self.speed_max_allowed = float(self.speed_max_allowed)\n        self.full_trial_flexibility = float(self.full_trial_flexibility) if self.full_trial_flexibility is not None else None\n        self.smooth_width = float(self.smooth_width) if self.smooth_width is not None else None\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapParams.compare","title":"<code>compare(other, filter_keys=None)</code>","text":"<p>Compare two SpkmapParams instances</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def compare(self, other: \"SpkmapParams\", filter_keys: Optional[List[str]] = None) -&gt; bool:\n    \"\"\"Compare two SpkmapParams instances\"\"\"\n    if filter_keys is None:\n        return self == other\n    else:\n        return all(getattr(self, key) == getattr(other, key) for key in filter_keys)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapParams.from_dict","title":"<code>from_dict(params_dict)</code>  <code>classmethod</code>","text":"<p>Create a SpkmapParams instance from a dictionary, using defaults for missing values</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef from_dict(cls, params_dict: dict) -&gt; \"SpkmapParams\":\n    \"\"\"Create a SpkmapParams instance from a dictionary, using defaults for missing values\"\"\"\n    return cls(**{k: params_dict[k] for k in params_dict})\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapParams.from_path","title":"<code>from_path(path)</code>  <code>classmethod</code>","text":"<p>Create a SpkmapParams instance from a json file</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path) -&gt; \"SpkmapParams\":\n    \"\"\"Create a SpkmapParams instance from a json file\"\"\"\n    with open(path, \"r\") as f:\n        return cls.from_dict(json.load(f))\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapParams.save","title":"<code>save(path)</code>","text":"<p>Save the parameters to a json file</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save the parameters to a json file\"\"\"\n    with open(path, \"w\") as f:\n        json.dump(asdict(self), f, sort_keys=True)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor","title":"<code>SpkmapProcessor</code>  <code>dataclass</code>","text":"<p>Class for processing and caching spike maps from session data</p> <p>NOTES ON ENGINEERING: I want the variables required for processing spkmaps to be properties (@property) that have hidden attributes for caching. Therefore, we can use the property method to get the attribute and each property method can do whatever processing is needed for that attribute. (Uh, duh). Time to get modern. lol.</p> <p>Right now I've almost got the register_spkmaps method working again (not tested yet) but now is when the dataclass refactoring comes in. 1. Make it possible to separate the occmap from the spkmap loading.    - do so by making the preliminary variables properties with caching 2. Consider how to implement smoothing then correctMap functionality -- it should    be possible to do this in a way that allows me to iteratively try different    parameterizations without having to go through the whole pipeline again. 3. Consider how / when to implement reliability measures. In PCSS, they're done all    right there with get_spkmaps. But it's probably not always necessary and can    actually take a bit of time? It would also be nice to save reliability scores for    the neurons... but then we'd also need an independent params saving system for them. 4. Re: the point above, I wonder if the one.data loading system is ideal or if I should    use a more explicit and dedicated SpkmapProcessor saving / loading system.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass SpkmapProcessor:\n    \"\"\"Class for processing and caching spike maps from session data\n\n    NOTES ON ENGINEERING:\n    I want the variables required for processing spkmaps to be properties (@property)\n    that have hidden attributes for caching. Therefore, we can use the property method\n    to get the attribute and each property method can do whatever processing is needed\n    for that attribute. (Uh, duh). Time to get modern. lol.\n\n    Right now I've almost got the register_spkmaps method working again (not tested yet)\n    but now is when the dataclass refactoring comes in.\n    1. Make it possible to separate the occmap from the spkmap loading.\n       - do so by making the preliminary variables properties with caching\n    2. Consider how to implement smoothing then correctMap functionality -- it should\n       be possible to do this in a way that allows me to iteratively try different\n       parameterizations without having to go through the whole pipeline again.\n    3. Consider how / when to implement reliability measures. In PCSS, they're done all\n       right there with get_spkmaps. But it's probably not always necessary and can\n       actually take a bit of time? It would also be nice to save reliability scores for\n       the neurons... but then we'd also need an independent params saving system for them.\n    4. Re: the point above, I wonder if the one.data loading system is ideal or if I should\n       use a more explicit and dedicated SpkmapProcessor saving / loading system.\n    \"\"\"\n\n    session: Union[SessionData, B2Session, SessionToSpkmapProtocol]\n    params: SpkmapParams = field(default_factory=SpkmapParams, repr=False)\n    data_cache: dict = field(default_factory=dict, repr=False, init=False)\n\n    def __post_init__(self):\n        # Check if the session provided is compatible with SpkmapProcessing\n        if not isinstance(self.session, SessionData):\n            raise ValueError(f\"session must be a SessionData instance, not {type(self.session)}\")\n        # (Don't check if it's a SessionToSpkmapProtocol because hasattr() will call properties which loads data...)\n\n        # We need to handle the case where params is a dictionary of partial updates to the default params\n        self.params = helpers.resolve_dataclass(self.params, SpkmapParams)\n\n    def cached_dependencies(self, data_type: str) -&gt; List[str]:\n        \"\"\"Get the dependencies for a given data type\"\"\"\n        if data_type == \"raw_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\"]\n        elif data_type == \"processed_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\"]\n        elif data_type == \"env_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\", \"full_trial_flexibility\"]\n        elif data_type == \"reliability\":\n            return [\n                \"dist_step\",\n                \"speed_threshold\",\n                \"speed_max_allowed\",\n                \"standardize_spks\",\n                \"smooth_width\",\n                \"full_trial_flexibility\",\n                \"reliability_method\",\n            ]\n        # Otherwise just return all params\n        return list(self.params.__dict__.keys())\n\n    def show_cache(self, data_type: Optional[str] = None) -&gt; str:\n        \"\"\"Helper function that scrapes the cache directory and shows cached files\n\n        Parameters\n        ----------\n        data_type: Optional[str] = None\n            Indicate a data type to filter which parts of the cache to show\n\n        Returns\n        -------\n        str\n            Formatted string showing cache information including data_type, size, parameters, and date\n        \"\"\"\n        import os\n        from datetime import datetime\n\n        # Get the base cache directory\n        base_cache_dir = self.cache_directory()\n\n        if not base_cache_dir.exists():\n            return f\"No cache directory found at: {base_cache_dir}\"\n\n        # Collect information about all cache files\n        cache_info = []\n\n        # Define the data types to check\n        if data_type is not None:\n            data_types_to_check = [data_type]\n        else:\n            data_types_to_check = [\"raw_maps\", \"processed_maps\", \"env_maps\", \"reliability\"]\n\n        for dt in data_types_to_check:\n            cache_dir = self.cache_directory(dt)\n            if not cache_dir.exists():\n                continue\n\n            # Find all parameter files (they define what caches exist)\n            param_files = list(cache_dir.glob(\"params_*.npz\"))\n\n            for param_file in param_files:\n                # Extract the hash from the filename\n                params_hash = param_file.stem.replace(\"params_\", \"\")\n\n                # Load the parameters\n                try:\n                    cached_params = dict(np.load(param_file))\n                    param_str = \", \".join([f\"{k}={v}\" for k, v in cached_params.items()])\n                except Exception as e:\n                    param_str = f\"Error loading params: {e}\"\n\n                # Get file modification time\n                mod_time = datetime.fromtimestamp(param_file.stat().st_mtime)\n                date_str = mod_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n                # Calculate total size of all related cache files\n                total_size = param_file.stat().st_size\n\n                if dt in [\"raw_maps\", \"processed_maps\"]:\n                    # For maps, look for data files for each map type\n                    for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                        data_file = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                        if data_file.exists():\n                            total_size += data_file.stat().st_size\n\n                elif dt == \"env_maps\":\n                    # For env_maps, look for environment file and individual environment data files\n                    env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                    if env_file.exists():\n                        total_size += env_file.stat().st_size\n                        # Load environments to find all data files\n                        try:\n                            environments = np.load(env_file)\n                            for env in environments:\n                                for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                                    data_file = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                                    if data_file.exists():\n                                        total_size += data_file.stat().st_size\n                        except Exception:\n                            pass  # Continue even if we can't load environments\n\n                elif dt == \"reliability\":\n                    # For reliability, look for environments and reliability data files\n                    env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                    rel_file = cache_dir / f\"data_reliability_{params_hash}.npy\"\n                    if env_file.exists():\n                        total_size += env_file.stat().st_size\n                    if rel_file.exists():\n                        total_size += rel_file.stat().st_size\n\n                # Convert size to human readable format\n                size_str = self._format_file_size(total_size)\n\n                cache_info.append(\n                    {\n                        \"data_type\": dt,\n                        \"size\": size_str,\n                        \"parameters\": param_str,\n                        \"date\": date_str,\n                        \"hash\": params_hash[:8],  # Show first 8 chars of hash\n                    }\n                )\n\n        if not cache_info:\n            return \"No cache files found.\"\n\n        # Format the output as a table\n        output_lines = []\n        output_lines.append(\"Cache Files Summary\")\n        output_lines.append(\"=\" * 80)\n        output_lines.append(f\"{'Data Type':&lt;15} {'Size':&lt;10} {'Date':&lt;20} {'Hash':&lt;10} {'Parameters'}\")\n        output_lines.append(\"-\" * 80)\n\n        for info in cache_info:\n            output_lines.append(f\"{info['data_type']:&lt;15} {info['size']:&lt;10} {info['date']:&lt;20} \" f\"{info['hash']:&lt;10} {info['parameters']}\")\n\n        output_lines.append(\"-\" * 80)\n        output_lines.append(f\"Total cache entries: {len(cache_info)}\")\n\n        result = \"\\n\".join(output_lines)\n        print(result)\n\n    def _format_file_size(self, size_bytes: int) -&gt; str:\n        \"\"\"Convert bytes to human readable format\"\"\"\n        if size_bytes == 0:\n            return \"0 B\"\n\n        size_names = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n        import math\n\n        i = int(math.floor(math.log(size_bytes, 1024)))\n        p = math.pow(1024, i)\n        s = round(size_bytes / p, 2)\n        return f\"{s} {size_names[i]}\"\n\n    def cache_directory(self, data_type: Optional[str] = None) -&gt; Path:\n        \"\"\"Get the cache directory for a given data type and spks_type\"\"\"\n        if data_type is None:\n            return self.session.data_path / \"spkmaps\"\n        else:\n            folder_name = f\"{data_type}_{self.session.spks_type}\"\n            return self.session.data_path / \"spkmaps\" / folder_name\n\n    def dependent_params(self, data_type: str) -&gt; dict:\n        \"\"\"Get the dependent parameters for a given data type\"\"\"\n        return {k: getattr(self.params, k) for k in self.cached_dependencies(data_type)}\n\n    def _params_hash(self, data_type: str) -&gt; str:\n        \"\"\"Get the hash of the dependent parameters for a given data type\"\"\"\n        return hashlib.sha256(json.dumps(self.dependent_params(data_type), sort_keys=True).encode()).hexdigest()\n\n    def save_cache(self, data_type: str, data: Union[Maps, Reliability]):\n        \"\"\"Save the cached params and data for a given data type\"\"\"\n        cache_dir = self.cache_directory(data_type)\n        params_hash = self._params_hash(data_type)\n        cache_param_path = cache_dir / f\"params_{params_hash}.npz\"\n        if not cache_dir.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n        np.savez(cache_param_path, **self.dependent_params(data_type))\n        if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n            for mapname in Maps.map_types():\n                cache_data_path = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                np.save(cache_data_path, getattr(data, mapname))\n        elif data_type == \"env_maps\":\n            environments = data.environments\n            np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n            for ienv, env in enumerate(environments):\n                for mapname in Maps.map_types():\n                    cache_data_path = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                    np.save(cache_data_path, getattr(data, mapname)[ienv])\n        elif data_type == \"reliability\":\n            values = data.values\n            environments = data.environments\n            # don't need data.method because it's in params...\n            np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n            np.save(cache_dir / f\"data_reliability_{params_hash}.npy\", values)\n        else:\n            raise ValueError(f\"Unknown data type: {data_type}\")\n\n    def load_from_cache(self, data_type: str) -&gt; Tuple[Union[Maps, Reliability], bool]:\n        \"\"\"Get the cached params and data for a given data type\"\"\"\n        cache_dir = self.cache_directory(data_type)\n        if cache_dir.exists():\n            # If the directory exists, check if there are any cached params that match the expected hash\n            params_hash = self._params_hash(data_type)\n            cached_params_path = cache_dir / f\"params_{params_hash}.npz\"\n            if cached_params_path.exists():\n                cached_params = dict(np.load(cached_params_path))\n                # Check if the cached params match the dependent params\n                if self.check_params_match(cached_params):\n                    return self._load_from_cache(data_type, params_hash, params=cached_params), True\n        return None, False\n\n    def check_params_match(self, cached_params: dict) -&gt; bool:\n        \"\"\"Check if the cached params and the current params are the same.\n\n        Parameters\n        ----------\n        cached_params : dict\n            The cached params to check against the current params\n\n        Returns\n        -------\n        bool\n            True if the cached params are nonempty and match the current params, False otherwise\n        \"\"\"\n        return cached_params and all(cached_params[k] == getattr(self.params, k) for k in cached_params)\n\n    def _load_from_cache(self, data_type: str, params_hash: str, params: Optional[Dict[str, Any]] | None = None) -&gt; Union[Maps, Reliability]:\n        \"\"\"Load the cached data for a given data type\"\"\"\n        cache_dir = self.cache_directory(data_type)\n        if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n            cached_data = {}\n            for name in Maps.map_types():\n                cached_data[name] = np.load(cache_dir / f\"data_{name}_{params_hash}.npy\", mmap_mode=\"r\")\n            if data_type == \"raw_maps\":\n                return Maps.create_raw_maps(**cached_data)\n            elif data_type == \"processed_maps\":\n                return Maps.create_processed_maps(**cached_data)\n        elif data_type == \"env_maps\":\n            environments = np.load(cache_dir / f\"data_environments_{params_hash}.npy\")\n            cached_data = dict(environments=environments)\n            for name in Maps.map_types():\n                cached_data[name] = []\n                for env in environments:\n                    cached_data[name].append(np.load(cache_dir / f\"data_{name}_{env}_{params_hash}.npy\", mmap_mode=\"r\"))\n            return Maps.create_environment_maps(**cached_data)\n        elif data_type == \"reliability\":\n            environments = np.load(cache_dir / f\"data_environments_{params_hash}.npy\")\n            values = np.load(cache_dir / f\"data_reliability_{params_hash}.npy\")\n            method = params[\"reliability_method\"]\n            return Reliability(values, environments, method)\n        else:\n            raise ValueError(f\"Unknown data type: {data_type}\")\n\n    @manage_one_cache\n    def _filter_environments(\n        self,\n        envnum: Union[int, Iterable[int], None] = None,\n        clear_one_cache: bool = True,\n    ) -&gt; np.ndarray[bool]:\n        \"\"\"Filter the session data to only include trials from certain environments\n\n        NOTE:\n        This assumes that the trials are in order. We might want to use the third output of session.positions to\n        get the \"real\" trial numbers which aren't always contiguous and 0 indexed.\n\n        If envnum is not provided, will return all trials.\n        \"\"\"\n        if envnum is None:\n            envnum = self.session.environments\n        envnum = helpers.check_iterable(envnum)\n        return np.isin(self.session.trial_environment, envnum)\n\n    @property\n    def dist_edges(self) -&gt; np.ndarray[float]:\n        \"\"\"Distance edges for the position bins\"\"\"\n        if not hasattr(self, \"_env_length\"):\n            env_length = self.session.env_length\n            if hasattr(env_length, \"__len__\"):\n                if np.unique(env_length).size != 1:\n                    msg = \"SpkmapProcessor (currently) requires all trials to have the same env length!\"\n                    raise ValueError(msg)\n                env_length = env_length[0]\n            self._env_length = env_length\n\n        num_positions = int(self._env_length / self.params.dist_step)\n        return np.linspace(0, self._env_length, num_positions + 1)\n\n    @property\n    def dist_centers(self) -&gt; np.ndarray[float]:\n        \"\"\"Distance centers for the position bins\"\"\"\n        return helpers.edge2center(self.dist_edges)\n\n    @manage_one_cache\n    def _idx_required_position_bins(self, clear_one_cache: bool = True) -&gt; np.ndarray:\n        \"\"\"Get the indices of the position bins that are required for a full trial\n\n        Parameters\n        ----------\n        clear_one_cache : bool, default=False\n            Whether to clear the onefile cache after getting the indices\n\n        Returns\n        -------\n        np.ndarray\n            The indices of the position bins that are required for a trial to be considered full\n        \"\"\"\n        num_position_bins = len(self.dist_centers)\n        if self.params.full_trial_flexibility is None:\n            idx_to_required_bins = np.arange(num_position_bins)\n        else:\n            start_idx = np.where(self.dist_edges &gt;= self.params.full_trial_flexibility)[0][0]\n            end_idx = np.where(self.dist_edges &lt;= self.dist_edges[-1] - self.params.full_trial_flexibility)[0][-1]\n            idx_to_required_bins = np.arange(start_idx, end_idx)\n        return idx_to_required_bins\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"raw_maps\", disable=False)\n    def get_raw_maps(\n        self,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get maps (occupancy, speed, spkmap) from session data by processing with provided parameters.\n\n        Parameters\n        ----------\n        force_recompute : bool, default=False\n            Whether to force the recomputation of the maps even if they exist in the cache.\n        clear_one_cache : bool, default=False\n            Whether to clear the onefile cache after getting the maps (only clears the onecache for this method)\n        params : SpkmapParams, dict, or None, default=None\n            Parameters for the maps. If None, the parameters will be taken from the SpkmapProcessor instance.\n            If a dictionary, it will be used to update the parameters.\n            Will always be temporary -- so the original parameters will be restored after the method is finished.\n        \"\"\"\n        dist_edges = self.dist_edges\n        dist_centers = self.dist_centers\n        num_positions = len(dist_centers)\n\n        # Get behavioral timestamps and positions\n        timestamps, positions, trial_numbers, idx_behave_to_frame = self.session.positions\n\n        # compute behavioral speed on each sample\n        within_trial_sample = np.append(np.diff(trial_numbers) == 0, True)\n        sample_duration = np.append(np.diff(timestamps), 0)\n        speeds = np.append(np.diff(positions) / sample_duration[:-1], 0)\n        # do this after division so no /0 errors\n        sample_duration = sample_duration * within_trial_sample\n        # speed 0 in last sample for each trial (it's undefined)\n        speeds = speeds * within_trial_sample\n        # Convert positions to position bins\n        position_bin = np.digitize(positions, dist_edges) - 1\n\n        # get imaging information\n        frame_time_stamps = self.session.timestamps\n        sampling_period = np.median(np.diff(frame_time_stamps))\n        dist_cutoff = sampling_period / 2\n        delay_position_to_imaging = frame_time_stamps[idx_behave_to_frame] - timestamps\n\n        # get spiking information\n        spks = self.session.spks\n        num_rois = self.session.get_value(\"numROIs\")\n\n        # Do standardization\n        if self.params.standardize_spks:\n            spks = median_zscore(spks, median_subtract=not self.session.zero_baseline_spks)\n\n        # Get high resolution occupancy and speed maps\n        dtype = np.float32\n        occmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        speedmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        spkmap = np.zeros((self.session.num_trials, num_positions, num_rois), dtype=dtype)\n        extra_counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n\n        # Get maps -- doing this independently for each map allows for more\n        # flexibility in which data to load (basically the occmap &amp; speedmap\n        # are instantaneous, but the spkmap is a bit slower)\n        get_summation_map(\n            sample_duration,\n            trial_numbers,\n            position_bin,\n            occmap,\n            counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=False,\n            use_sample_to_value_idx=False,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n        get_summation_map(\n            speeds,\n            trial_numbers,\n            position_bin,\n            speedmap,\n            counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=True,\n            use_sample_to_value_idx=False,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n        get_summation_map(\n            spks,\n            trial_numbers,\n            position_bin,\n            spkmap,\n            extra_counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=True,\n            use_sample_to_value_idx=True,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n\n        # Figure out the valid range (outside of this range, set the maps to nan, because their values are not meaningful)\n        position_bin_per_trial = [position_bin[trial_numbers == tnum] for tnum in range(self.session.num_trials)]\n\n        # offsetting by 1 because there is a bug in the vrControl software where the first sample is always set\n        # to the minimum position (which is 0), but if there is a built-up buffer in the rotary encoder, the position\n        # will jump at the second sample. In general this will always work unless the mice have a truly ridiculous\n        # speed at the beginning of the trial...\n        first_valid_bin = [np.min(bpb[1:] if len(bpb) &gt; 1 else bpb) for bpb in position_bin_per_trial]\n        last_valid_bin = [np.max(bpb) for bpb in position_bin_per_trial]\n\n        # set bins to nan when mouse didn't visit them\n        occmap = replace_missing_data(occmap, first_valid_bin, last_valid_bin)\n        speedmap = replace_missing_data(speedmap, first_valid_bin, last_valid_bin)\n        spkmap = replace_missing_data(spkmap, first_valid_bin, last_valid_bin)\n\n        return Maps.create_raw_maps(occmap, speedmap, spkmap)\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"processed_maps\", disable=False)\n    def get_processed_maps(\n        self,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Process the maps\"\"\"\n        # Get the raw maps first (don't need to specify params because they're already set by this method)\n        maps = self.get_raw_maps(\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n        )\n\n        # Process the maps (smooth, divide by occupancy, and change to ROIs first)\n        return maps.raw_to_processed(self.dist_centers, self.params.smooth_width)\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"env_maps\", disable=False)\n    def get_env_maps(\n        self,\n        use_session_filters: bool = True,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get the map for a given environment number\"\"\"\n        # Make sure it's an iterable -- the output will always be a list\n        envnum = helpers.check_iterable(self.session.environments)\n\n        # Get the indices of the trials to each environment\n        idx_each_environment = [self._filter_environments(env) for env in envnum]\n\n        # Then get the indices of the position bins that are required for a full trial\n        idx_required_position_bins = self._idx_required_position_bins(clear_one_cache)\n\n        # Get the processed maps (don't need to specify params because they're already set by the decorator)\n        maps = self.get_processed_maps(\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n        )\n\n        # Add the list of environments to the maps\n        maps.environments = envnum\n\n        # Make a list of the maps we are processing\n        maps_to_process = Maps.map_types()\n\n        # Filter the maps to only include the ROIs we want\n        if use_session_filters:\n            idx_rois = np.where(self.session.idx_rois)[0]\n        else:\n            idx_rois = np.arange(self.session.get_value(\"numROIs\"), dtype=int)\n\n        # Filter the maps to only include the full trials\n        full_trials = np.where(np.all(~np.isnan(maps.occmap[:, idx_required_position_bins]), axis=1))[0]\n\n        # Implement trial &amp; ROI filtering here\n        for mapname in maps_to_process:\n            if mapname == \"spkmap\":\n                maps[mapname] = np.take(np.take(maps[mapname], idx_rois, axis=0), full_trials, axis=1)\n            else:\n                maps[mapname] = np.take(maps[mapname], full_trials, axis=0)\n\n        # Filter the trial indices to only include full trials\n        idx_each_environment = [np.where(np.take(idx, full_trials, axis=0))[0] for idx in idx_each_environment]\n\n        # Then group each one by environment\n        # -&gt; this is now (trials_in_env, position_bins, ...(roi if spkmap)...)\n        maps.by_environment = True\n        for mapname in maps_to_process:\n            if mapname == \"spkmap\":\n                maps[mapname] = [np.take(maps[mapname], idx, axis=1) for idx in idx_each_environment]\n            else:\n                maps[mapname] = [np.take(maps[mapname], idx, axis=0) for idx in idx_each_environment]\n\n        return maps\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"reliability\", disable=False)\n    def get_reliability(\n        self,\n        use_session_filters: bool = True,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ):\n        \"\"\"Get the reliability of the maps\"\"\"\n        envnum = helpers.check_iterable(self.session.environments)\n\n        # A list of the requested environments (all if not specified)\n        maps = self.get_env_maps(\n            use_session_filters=use_session_filters,\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n            params={\"autosave\": False},  # Prevent saving in the case of a recompute\n        )\n\n        # All reliability measures require no NaNs\n        maps.pop_nan_positions()\n\n        if self.params.reliability_method == \"leave_one_out\":\n            rel_values = [helpers.reliability_loo(spkmap) for spkmap in maps.spkmap]\n        elif self.params.reliability_method == \"correlation\" or self.params.reliability_method == \"mse\":\n            rel_mse, rel_cor = helpers.named_transpose([helpers.measureReliability(spkmap) for spkmap in maps.spkmap])\n            rel_values = rel_mse if self.params.reliability_method == \"mse\" else rel_cor\n        else:\n            raise ValueError(f\"Method {self.params.reliability_method} not supported\")\n\n        return Reliability(\n            np.stack(rel_values),\n            environments=envnum,\n            method=self.params.reliability_method,\n        )\n\n    # ------------------- convert between imaging and behavioral time -------------------\n    @with_temp_params\n    @manage_one_cache\n    def get_frame_behavior(self, clear_one_cache: bool = True, params: Union[SpkmapParams, Dict[str, Any], None] = None):\n        \"\"\"\n        get position and environment data for each frame in imaging data\n        nan if no position data is available for that frame (e.g. if the closest\n        behavioral sample is further away in time than the sampling period)\n        \"\"\"\n        timestamps = self.session.loadone(\"positionTracking.times\")\n        position = self.session.loadone(\"positionTracking.position\")\n        idx_behave_to_frame = self.session.loadone(\"positionTracking.mpci\")\n        trial_start_index = self.session.loadone(\"trials.positionTracking\")\n        num_samples = len(position)\n        trial_numbers = np.arange(len(trial_start_index))\n        trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n        trial_numbers = np.repeat(trial_numbers, trial_lengths)\n        trial_environment = self.session.loadone(\"trials.environmentIndex\")\n        trial_environment = np.repeat(trial_environment, trial_lengths)\n\n        within_trial = np.append(np.diff(trial_numbers) == 0, True)\n        sample_duration = np.append(np.diff(timestamps), 0)\n        speed = np.append(np.diff(position) / sample_duration[:-1], 0)\n        sample_duration = sample_duration * within_trial\n        speed = speed * within_trial\n\n        frame_timestamps = self.session.loadone(\"mpci.times\")\n        difference_timestamps = np.abs(timestamps - frame_timestamps[idx_behave_to_frame])\n        sampling_period = np.median(np.diff(frame_timestamps))\n        dist_cutoff = sampling_period / 2\n\n        frame_position = np.zeros_like(frame_timestamps)\n        count = np.zeros_like(frame_timestamps)\n        helpers.get_average_frame_position(position, idx_behave_to_frame, difference_timestamps, dist_cutoff, frame_position, count)\n        frame_position[count &gt; 0] /= count[count &gt; 0]\n        frame_position[count == 0] = np.nan\n        frame_speed = np.diff(frame_position) / np.diff(frame_timestamps)\n        frame_speed = np.append(frame_speed, 0)\n\n        # Let the last frame of each trial have a speed equal to previous frame\n        idx_first_nan = np.where(np.diff(1.0 * np.isnan(frame_position)) == 1.0)[0]\n        frame_speed[idx_first_nan] = frame_speed[idx_first_nan - 1]\n\n        idx_frame_to_behave, dist_frame_to_behave = helpers.nearestpoint(frame_timestamps, timestamps)\n        idx_get_position = dist_frame_to_behave &lt; dist_cutoff\n\n        frame_environment = np.full(len(frame_timestamps), np.nan)\n        frame_environment[idx_get_position] = trial_environment[idx_frame_to_behave[idx_get_position]]\n        frame_environment[count == 0] = np.nan\n\n        frame_trial = np.full(len(frame_timestamps), np.nan)\n        frame_trial[idx_get_position] = trial_numbers[idx_frame_to_behave[idx_get_position]]\n        frame_trial[count == 0] = np.nan\n\n        return frame_position, frame_speed, frame_environment, frame_trial\n\n    @with_temp_params\n    @manage_one_cache\n    def get_placefield_prediction(\n        self,\n        use_session_filters: bool = True,\n        spks_type: Union[str, None] = None,\n        use_speed_threshold: bool = True,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ):\n        \"\"\"\n        get placefield prediction of session spks data from spkmaps\n        \"\"\"\n        if spks_type is not None:\n            _spks_type = self.session.spks_type\n            self.session.params.spks_type = spks_type\n\n        frame_position, frame_speed, frame_environment, _ = self.get_frame_behavior(clear_one_cache, params)\n        idx_valid = ~np.isnan(frame_position)\n        if use_speed_threshold:\n            idx_valid = idx_valid &amp; (frame_speed &gt; self.params.speed_threshold)\n\n        # Convert frame position to bins indices\n        frame_position_index = np.searchsorted(self.dist_edges, frame_position, side=\"right\") - 1\n\n        # Get the place field for each neuron\n        env_maps = self.get_env_maps(use_session_filters=use_session_filters)\n        env_maps.average_trials()\n\n        # Convert frame environment to indices\n        env_to_idx = {env: i for i, env in enumerate(env_maps.environments)}\n        frame_environment_index = np.array([env_to_idx[env] if not np.isnan(env) else -1000 for env in frame_environment], dtype=int)\n\n        # Get the original spks data\n        spks = self.session.spks\n        if use_session_filters:\n            spks = spks[:, self.session.idx_rois]\n\n        # Use a numba speed up to get the placefield prediction (single pass simple algorithm)\n        placefield_prediction = np.full(spks.shape, np.nan)\n        placefield_prediction = _placefield_prediction_numba(\n            placefield_prediction,\n            env_maps.spkmap,\n            frame_environment_index,\n            frame_position_index,\n            idx_valid,\n        )\n\n        # This will add samples for which a place field was not estimable (at the edges of the environment)\n        idx_valid = np.all(~np.isnan(placefield_prediction), axis=1)\n\n        # Reset spks_type\n        if spks_type is not None:\n            self.session.params.spks_type = _spks_type\n\n        # Include extra details in a dictionary for forward compatibility\n        extras = dict(\n            frame_position_index=frame_position_index,\n            frame_environment_index=frame_environment_index,\n            idx_valid=idx_valid,\n        )\n\n        return placefield_prediction, extras\n\n    def get_traversals(\n        self,\n        idx_roi: int,\n        idx_env: int,\n        width: int = 10,\n        placefield_threshold: float = 5.0,  # in cm (or whatever units the frame_position is in)\n        fill_nan: bool = False,\n        spks: np.ndarray = None,\n        spks_prediction: np.ndarray = None,\n    ):\n        frame_position, _, frame_environment, frame_trial = self.get_frame_behavior()\n        if spks_prediction is None:\n            spks_prediction = self.get_placefield_prediction(use_session_filters=True)[0]\n        if spks is None:\n            spks = self.session.spks[:, self.session.idx_rois]\n\n        if spks.shape != spks_prediction.shape:\n            raise ValueError(\"spks and spks_prediction must have the same shape\")\n\n        env_maps = self.get_env_maps()\n        pos_peak = self.dist_centers[np.nanargmax(np.nanmean(env_maps.spkmap[idx_env][idx_roi], axis=0))]\n        envnum = env_maps.environments[idx_env]\n\n        env_trials = np.unique(frame_trial[frame_environment == envnum])\n\n        num_trials = len(env_trials)\n        idx_traversal = -1 * np.ones(num_trials, dtype=int)\n        for itrial, trialnum in enumerate(env_trials):\n            idx_trial = frame_trial == trialnum\n            idx_closest_pos = np.nanargmin(np.abs(frame_position - pos_peak) + 10000 * ~idx_trial)\n\n            # Only include the trial if the closest position is within placefield threshold of the peak\n            if np.abs(frame_position[idx_closest_pos] - pos_peak) &lt; placefield_threshold:\n                idx_traversal[itrial] = idx_closest_pos\n\n        # Filter out trials that don't have a traversal\n        idx_traversal = idx_traversal[idx_traversal != -1]\n\n        # Get traversals through place field in requested environment\n        traversals = np.zeros((len(idx_traversal), width * 2 + 1))\n        pred_travs = np.zeros((len(idx_traversal), width * 2 + 1))\n        for ii, it in enumerate(idx_traversal):\n            istart = it - width\n            iend = it + width + 1\n            istartoffset = max(0, -istart)\n            iendoffset = max(0, iend - spks.shape[0])\n            traversals[ii, istartoffset : width * 2 + 1 - iendoffset] = spks[istart + istartoffset : iend - iendoffset, idx_roi]\n            pred_travs[ii, istartoffset : width * 2 + 1 - iendoffset] = spks_prediction[istart + istartoffset : iend - iendoffset, idx_roi]\n\n        if fill_nan:\n            traversals[np.isnan(traversals)] = 0.0\n            pred_travs[np.isnan(pred_travs)] = 0.0\n\n        return traversals, pred_travs\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.dist_centers","title":"<code>dist_centers</code>  <code>property</code>","text":"<p>Distance centers for the position bins</p>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.dist_edges","title":"<code>dist_edges</code>  <code>property</code>","text":"<p>Distance edges for the position bins</p>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.cache_directory","title":"<code>cache_directory(data_type=None)</code>","text":"<p>Get the cache directory for a given data type and spks_type</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def cache_directory(self, data_type: Optional[str] = None) -&gt; Path:\n    \"\"\"Get the cache directory for a given data type and spks_type\"\"\"\n    if data_type is None:\n        return self.session.data_path / \"spkmaps\"\n    else:\n        folder_name = f\"{data_type}_{self.session.spks_type}\"\n        return self.session.data_path / \"spkmaps\" / folder_name\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.cached_dependencies","title":"<code>cached_dependencies(data_type)</code>","text":"<p>Get the dependencies for a given data type</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def cached_dependencies(self, data_type: str) -&gt; List[str]:\n    \"\"\"Get the dependencies for a given data type\"\"\"\n    if data_type == \"raw_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\"]\n    elif data_type == \"processed_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\"]\n    elif data_type == \"env_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\", \"full_trial_flexibility\"]\n    elif data_type == \"reliability\":\n        return [\n            \"dist_step\",\n            \"speed_threshold\",\n            \"speed_max_allowed\",\n            \"standardize_spks\",\n            \"smooth_width\",\n            \"full_trial_flexibility\",\n            \"reliability_method\",\n        ]\n    # Otherwise just return all params\n    return list(self.params.__dict__.keys())\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.check_params_match","title":"<code>check_params_match(cached_params)</code>","text":"<p>Check if the cached params and the current params are the same.</p> <p>Parameters:</p> Name Type Description Default <code>cached_params</code> <code>dict</code> <p>The cached params to check against the current params</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the cached params are nonempty and match the current params, False otherwise</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def check_params_match(self, cached_params: dict) -&gt; bool:\n    \"\"\"Check if the cached params and the current params are the same.\n\n    Parameters\n    ----------\n    cached_params : dict\n        The cached params to check against the current params\n\n    Returns\n    -------\n    bool\n        True if the cached params are nonempty and match the current params, False otherwise\n    \"\"\"\n    return cached_params and all(cached_params[k] == getattr(self.params, k) for k in cached_params)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.dependent_params","title":"<code>dependent_params(data_type)</code>","text":"<p>Get the dependent parameters for a given data type</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def dependent_params(self, data_type: str) -&gt; dict:\n    \"\"\"Get the dependent parameters for a given data type\"\"\"\n    return {k: getattr(self.params, k) for k in self.cached_dependencies(data_type)}\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.get_env_maps","title":"<code>get_env_maps(use_session_filters=True, force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get the map for a given environment number</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"env_maps\", disable=False)\ndef get_env_maps(\n    self,\n    use_session_filters: bool = True,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get the map for a given environment number\"\"\"\n    # Make sure it's an iterable -- the output will always be a list\n    envnum = helpers.check_iterable(self.session.environments)\n\n    # Get the indices of the trials to each environment\n    idx_each_environment = [self._filter_environments(env) for env in envnum]\n\n    # Then get the indices of the position bins that are required for a full trial\n    idx_required_position_bins = self._idx_required_position_bins(clear_one_cache)\n\n    # Get the processed maps (don't need to specify params because they're already set by the decorator)\n    maps = self.get_processed_maps(\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n    )\n\n    # Add the list of environments to the maps\n    maps.environments = envnum\n\n    # Make a list of the maps we are processing\n    maps_to_process = Maps.map_types()\n\n    # Filter the maps to only include the ROIs we want\n    if use_session_filters:\n        idx_rois = np.where(self.session.idx_rois)[0]\n    else:\n        idx_rois = np.arange(self.session.get_value(\"numROIs\"), dtype=int)\n\n    # Filter the maps to only include the full trials\n    full_trials = np.where(np.all(~np.isnan(maps.occmap[:, idx_required_position_bins]), axis=1))[0]\n\n    # Implement trial &amp; ROI filtering here\n    for mapname in maps_to_process:\n        if mapname == \"spkmap\":\n            maps[mapname] = np.take(np.take(maps[mapname], idx_rois, axis=0), full_trials, axis=1)\n        else:\n            maps[mapname] = np.take(maps[mapname], full_trials, axis=0)\n\n    # Filter the trial indices to only include full trials\n    idx_each_environment = [np.where(np.take(idx, full_trials, axis=0))[0] for idx in idx_each_environment]\n\n    # Then group each one by environment\n    # -&gt; this is now (trials_in_env, position_bins, ...(roi if spkmap)...)\n    maps.by_environment = True\n    for mapname in maps_to_process:\n        if mapname == \"spkmap\":\n            maps[mapname] = [np.take(maps[mapname], idx, axis=1) for idx in idx_each_environment]\n        else:\n            maps[mapname] = [np.take(maps[mapname], idx, axis=0) for idx in idx_each_environment]\n\n    return maps\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.get_frame_behavior","title":"<code>get_frame_behavior(clear_one_cache=True, params=None)</code>","text":"<p>get position and environment data for each frame in imaging data nan if no position data is available for that frame (e.g. if the closest behavioral sample is further away in time than the sampling period)</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\ndef get_frame_behavior(self, clear_one_cache: bool = True, params: Union[SpkmapParams, Dict[str, Any], None] = None):\n    \"\"\"\n    get position and environment data for each frame in imaging data\n    nan if no position data is available for that frame (e.g. if the closest\n    behavioral sample is further away in time than the sampling period)\n    \"\"\"\n    timestamps = self.session.loadone(\"positionTracking.times\")\n    position = self.session.loadone(\"positionTracking.position\")\n    idx_behave_to_frame = self.session.loadone(\"positionTracking.mpci\")\n    trial_start_index = self.session.loadone(\"trials.positionTracking\")\n    num_samples = len(position)\n    trial_numbers = np.arange(len(trial_start_index))\n    trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n    trial_numbers = np.repeat(trial_numbers, trial_lengths)\n    trial_environment = self.session.loadone(\"trials.environmentIndex\")\n    trial_environment = np.repeat(trial_environment, trial_lengths)\n\n    within_trial = np.append(np.diff(trial_numbers) == 0, True)\n    sample_duration = np.append(np.diff(timestamps), 0)\n    speed = np.append(np.diff(position) / sample_duration[:-1], 0)\n    sample_duration = sample_duration * within_trial\n    speed = speed * within_trial\n\n    frame_timestamps = self.session.loadone(\"mpci.times\")\n    difference_timestamps = np.abs(timestamps - frame_timestamps[idx_behave_to_frame])\n    sampling_period = np.median(np.diff(frame_timestamps))\n    dist_cutoff = sampling_period / 2\n\n    frame_position = np.zeros_like(frame_timestamps)\n    count = np.zeros_like(frame_timestamps)\n    helpers.get_average_frame_position(position, idx_behave_to_frame, difference_timestamps, dist_cutoff, frame_position, count)\n    frame_position[count &gt; 0] /= count[count &gt; 0]\n    frame_position[count == 0] = np.nan\n    frame_speed = np.diff(frame_position) / np.diff(frame_timestamps)\n    frame_speed = np.append(frame_speed, 0)\n\n    # Let the last frame of each trial have a speed equal to previous frame\n    idx_first_nan = np.where(np.diff(1.0 * np.isnan(frame_position)) == 1.0)[0]\n    frame_speed[idx_first_nan] = frame_speed[idx_first_nan - 1]\n\n    idx_frame_to_behave, dist_frame_to_behave = helpers.nearestpoint(frame_timestamps, timestamps)\n    idx_get_position = dist_frame_to_behave &lt; dist_cutoff\n\n    frame_environment = np.full(len(frame_timestamps), np.nan)\n    frame_environment[idx_get_position] = trial_environment[idx_frame_to_behave[idx_get_position]]\n    frame_environment[count == 0] = np.nan\n\n    frame_trial = np.full(len(frame_timestamps), np.nan)\n    frame_trial[idx_get_position] = trial_numbers[idx_frame_to_behave[idx_get_position]]\n    frame_trial[count == 0] = np.nan\n\n    return frame_position, frame_speed, frame_environment, frame_trial\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.get_placefield_prediction","title":"<code>get_placefield_prediction(use_session_filters=True, spks_type=None, use_speed_threshold=True, clear_one_cache=True, params=None)</code>","text":"<p>get placefield prediction of session spks data from spkmaps</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\ndef get_placefield_prediction(\n    self,\n    use_session_filters: bool = True,\n    spks_type: Union[str, None] = None,\n    use_speed_threshold: bool = True,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n):\n    \"\"\"\n    get placefield prediction of session spks data from spkmaps\n    \"\"\"\n    if spks_type is not None:\n        _spks_type = self.session.spks_type\n        self.session.params.spks_type = spks_type\n\n    frame_position, frame_speed, frame_environment, _ = self.get_frame_behavior(clear_one_cache, params)\n    idx_valid = ~np.isnan(frame_position)\n    if use_speed_threshold:\n        idx_valid = idx_valid &amp; (frame_speed &gt; self.params.speed_threshold)\n\n    # Convert frame position to bins indices\n    frame_position_index = np.searchsorted(self.dist_edges, frame_position, side=\"right\") - 1\n\n    # Get the place field for each neuron\n    env_maps = self.get_env_maps(use_session_filters=use_session_filters)\n    env_maps.average_trials()\n\n    # Convert frame environment to indices\n    env_to_idx = {env: i for i, env in enumerate(env_maps.environments)}\n    frame_environment_index = np.array([env_to_idx[env] if not np.isnan(env) else -1000 for env in frame_environment], dtype=int)\n\n    # Get the original spks data\n    spks = self.session.spks\n    if use_session_filters:\n        spks = spks[:, self.session.idx_rois]\n\n    # Use a numba speed up to get the placefield prediction (single pass simple algorithm)\n    placefield_prediction = np.full(spks.shape, np.nan)\n    placefield_prediction = _placefield_prediction_numba(\n        placefield_prediction,\n        env_maps.spkmap,\n        frame_environment_index,\n        frame_position_index,\n        idx_valid,\n    )\n\n    # This will add samples for which a place field was not estimable (at the edges of the environment)\n    idx_valid = np.all(~np.isnan(placefield_prediction), axis=1)\n\n    # Reset spks_type\n    if spks_type is not None:\n        self.session.params.spks_type = _spks_type\n\n    # Include extra details in a dictionary for forward compatibility\n    extras = dict(\n        frame_position_index=frame_position_index,\n        frame_environment_index=frame_environment_index,\n        idx_valid=idx_valid,\n    )\n\n    return placefield_prediction, extras\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.get_processed_maps","title":"<code>get_processed_maps(force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Process the maps</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"processed_maps\", disable=False)\ndef get_processed_maps(\n    self,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Process the maps\"\"\"\n    # Get the raw maps first (don't need to specify params because they're already set by this method)\n    maps = self.get_raw_maps(\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n    )\n\n    # Process the maps (smooth, divide by occupancy, and change to ROIs first)\n    return maps.raw_to_processed(self.dist_centers, self.params.smooth_width)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.get_raw_maps","title":"<code>get_raw_maps(force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get maps (occupancy, speed, spkmap) from session data by processing with provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>force_recompute</code> <code>bool</code> <p>Whether to force the recomputation of the maps even if they exist in the cache.</p> <code>False</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after getting the maps (only clears the onecache for this method)</p> <code>False</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for the maps. If None, the parameters will be taken from the SpkmapProcessor instance. If a dictionary, it will be used to update the parameters. Will always be temporary -- so the original parameters will be restored after the method is finished.</p> <code>None</code> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"raw_maps\", disable=False)\ndef get_raw_maps(\n    self,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get maps (occupancy, speed, spkmap) from session data by processing with provided parameters.\n\n    Parameters\n    ----------\n    force_recompute : bool, default=False\n        Whether to force the recomputation of the maps even if they exist in the cache.\n    clear_one_cache : bool, default=False\n        Whether to clear the onefile cache after getting the maps (only clears the onecache for this method)\n    params : SpkmapParams, dict, or None, default=None\n        Parameters for the maps. If None, the parameters will be taken from the SpkmapProcessor instance.\n        If a dictionary, it will be used to update the parameters.\n        Will always be temporary -- so the original parameters will be restored after the method is finished.\n    \"\"\"\n    dist_edges = self.dist_edges\n    dist_centers = self.dist_centers\n    num_positions = len(dist_centers)\n\n    # Get behavioral timestamps and positions\n    timestamps, positions, trial_numbers, idx_behave_to_frame = self.session.positions\n\n    # compute behavioral speed on each sample\n    within_trial_sample = np.append(np.diff(trial_numbers) == 0, True)\n    sample_duration = np.append(np.diff(timestamps), 0)\n    speeds = np.append(np.diff(positions) / sample_duration[:-1], 0)\n    # do this after division so no /0 errors\n    sample_duration = sample_duration * within_trial_sample\n    # speed 0 in last sample for each trial (it's undefined)\n    speeds = speeds * within_trial_sample\n    # Convert positions to position bins\n    position_bin = np.digitize(positions, dist_edges) - 1\n\n    # get imaging information\n    frame_time_stamps = self.session.timestamps\n    sampling_period = np.median(np.diff(frame_time_stamps))\n    dist_cutoff = sampling_period / 2\n    delay_position_to_imaging = frame_time_stamps[idx_behave_to_frame] - timestamps\n\n    # get spiking information\n    spks = self.session.spks\n    num_rois = self.session.get_value(\"numROIs\")\n\n    # Do standardization\n    if self.params.standardize_spks:\n        spks = median_zscore(spks, median_subtract=not self.session.zero_baseline_spks)\n\n    # Get high resolution occupancy and speed maps\n    dtype = np.float32\n    occmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    speedmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    spkmap = np.zeros((self.session.num_trials, num_positions, num_rois), dtype=dtype)\n    extra_counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n\n    # Get maps -- doing this independently for each map allows for more\n    # flexibility in which data to load (basically the occmap &amp; speedmap\n    # are instantaneous, but the spkmap is a bit slower)\n    get_summation_map(\n        sample_duration,\n        trial_numbers,\n        position_bin,\n        occmap,\n        counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=False,\n        use_sample_to_value_idx=False,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n    get_summation_map(\n        speeds,\n        trial_numbers,\n        position_bin,\n        speedmap,\n        counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=True,\n        use_sample_to_value_idx=False,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n    get_summation_map(\n        spks,\n        trial_numbers,\n        position_bin,\n        spkmap,\n        extra_counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=True,\n        use_sample_to_value_idx=True,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n\n    # Figure out the valid range (outside of this range, set the maps to nan, because their values are not meaningful)\n    position_bin_per_trial = [position_bin[trial_numbers == tnum] for tnum in range(self.session.num_trials)]\n\n    # offsetting by 1 because there is a bug in the vrControl software where the first sample is always set\n    # to the minimum position (which is 0), but if there is a built-up buffer in the rotary encoder, the position\n    # will jump at the second sample. In general this will always work unless the mice have a truly ridiculous\n    # speed at the beginning of the trial...\n    first_valid_bin = [np.min(bpb[1:] if len(bpb) &gt; 1 else bpb) for bpb in position_bin_per_trial]\n    last_valid_bin = [np.max(bpb) for bpb in position_bin_per_trial]\n\n    # set bins to nan when mouse didn't visit them\n    occmap = replace_missing_data(occmap, first_valid_bin, last_valid_bin)\n    speedmap = replace_missing_data(speedmap, first_valid_bin, last_valid_bin)\n    spkmap = replace_missing_data(spkmap, first_valid_bin, last_valid_bin)\n\n    return Maps.create_raw_maps(occmap, speedmap, spkmap)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.get_reliability","title":"<code>get_reliability(use_session_filters=True, force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get the reliability of the maps</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"reliability\", disable=False)\ndef get_reliability(\n    self,\n    use_session_filters: bool = True,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n):\n    \"\"\"Get the reliability of the maps\"\"\"\n    envnum = helpers.check_iterable(self.session.environments)\n\n    # A list of the requested environments (all if not specified)\n    maps = self.get_env_maps(\n        use_session_filters=use_session_filters,\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n        params={\"autosave\": False},  # Prevent saving in the case of a recompute\n    )\n\n    # All reliability measures require no NaNs\n    maps.pop_nan_positions()\n\n    if self.params.reliability_method == \"leave_one_out\":\n        rel_values = [helpers.reliability_loo(spkmap) for spkmap in maps.spkmap]\n    elif self.params.reliability_method == \"correlation\" or self.params.reliability_method == \"mse\":\n        rel_mse, rel_cor = helpers.named_transpose([helpers.measureReliability(spkmap) for spkmap in maps.spkmap])\n        rel_values = rel_mse if self.params.reliability_method == \"mse\" else rel_cor\n    else:\n        raise ValueError(f\"Method {self.params.reliability_method} not supported\")\n\n    return Reliability(\n        np.stack(rel_values),\n        environments=envnum,\n        method=self.params.reliability_method,\n    )\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.load_from_cache","title":"<code>load_from_cache(data_type)</code>","text":"<p>Get the cached params and data for a given data type</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def load_from_cache(self, data_type: str) -&gt; Tuple[Union[Maps, Reliability], bool]:\n    \"\"\"Get the cached params and data for a given data type\"\"\"\n    cache_dir = self.cache_directory(data_type)\n    if cache_dir.exists():\n        # If the directory exists, check if there are any cached params that match the expected hash\n        params_hash = self._params_hash(data_type)\n        cached_params_path = cache_dir / f\"params_{params_hash}.npz\"\n        if cached_params_path.exists():\n            cached_params = dict(np.load(cached_params_path))\n            # Check if the cached params match the dependent params\n            if self.check_params_match(cached_params):\n                return self._load_from_cache(data_type, params_hash, params=cached_params), True\n    return None, False\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.save_cache","title":"<code>save_cache(data_type, data)</code>","text":"<p>Save the cached params and data for a given data type</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def save_cache(self, data_type: str, data: Union[Maps, Reliability]):\n    \"\"\"Save the cached params and data for a given data type\"\"\"\n    cache_dir = self.cache_directory(data_type)\n    params_hash = self._params_hash(data_type)\n    cache_param_path = cache_dir / f\"params_{params_hash}.npz\"\n    if not cache_dir.exists():\n        cache_dir.mkdir(parents=True, exist_ok=True)\n    np.savez(cache_param_path, **self.dependent_params(data_type))\n    if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n        for mapname in Maps.map_types():\n            cache_data_path = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n            np.save(cache_data_path, getattr(data, mapname))\n    elif data_type == \"env_maps\":\n        environments = data.environments\n        np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n        for ienv, env in enumerate(environments):\n            for mapname in Maps.map_types():\n                cache_data_path = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                np.save(cache_data_path, getattr(data, mapname)[ienv])\n    elif data_type == \"reliability\":\n        values = data.values\n        environments = data.environments\n        # don't need data.method because it's in params...\n        np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n        np.save(cache_dir / f\"data_reliability_{params_hash}.npy\", values)\n    else:\n        raise ValueError(f\"Unknown data type: {data_type}\")\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.SpkmapProcessor.show_cache","title":"<code>show_cache(data_type=None)</code>","text":"<p>Helper function that scrapes the cache directory and shows cached files</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Optional[str]</code> <p>Indicate a data type to filter which parts of the cache to show</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string showing cache information including data_type, size, parameters, and date</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def show_cache(self, data_type: Optional[str] = None) -&gt; str:\n    \"\"\"Helper function that scrapes the cache directory and shows cached files\n\n    Parameters\n    ----------\n    data_type: Optional[str] = None\n        Indicate a data type to filter which parts of the cache to show\n\n    Returns\n    -------\n    str\n        Formatted string showing cache information including data_type, size, parameters, and date\n    \"\"\"\n    import os\n    from datetime import datetime\n\n    # Get the base cache directory\n    base_cache_dir = self.cache_directory()\n\n    if not base_cache_dir.exists():\n        return f\"No cache directory found at: {base_cache_dir}\"\n\n    # Collect information about all cache files\n    cache_info = []\n\n    # Define the data types to check\n    if data_type is not None:\n        data_types_to_check = [data_type]\n    else:\n        data_types_to_check = [\"raw_maps\", \"processed_maps\", \"env_maps\", \"reliability\"]\n\n    for dt in data_types_to_check:\n        cache_dir = self.cache_directory(dt)\n        if not cache_dir.exists():\n            continue\n\n        # Find all parameter files (they define what caches exist)\n        param_files = list(cache_dir.glob(\"params_*.npz\"))\n\n        for param_file in param_files:\n            # Extract the hash from the filename\n            params_hash = param_file.stem.replace(\"params_\", \"\")\n\n            # Load the parameters\n            try:\n                cached_params = dict(np.load(param_file))\n                param_str = \", \".join([f\"{k}={v}\" for k, v in cached_params.items()])\n            except Exception as e:\n                param_str = f\"Error loading params: {e}\"\n\n            # Get file modification time\n            mod_time = datetime.fromtimestamp(param_file.stat().st_mtime)\n            date_str = mod_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            # Calculate total size of all related cache files\n            total_size = param_file.stat().st_size\n\n            if dt in [\"raw_maps\", \"processed_maps\"]:\n                # For maps, look for data files for each map type\n                for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                    data_file = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                    if data_file.exists():\n                        total_size += data_file.stat().st_size\n\n            elif dt == \"env_maps\":\n                # For env_maps, look for environment file and individual environment data files\n                env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                if env_file.exists():\n                    total_size += env_file.stat().st_size\n                    # Load environments to find all data files\n                    try:\n                        environments = np.load(env_file)\n                        for env in environments:\n                            for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                                data_file = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                                if data_file.exists():\n                                    total_size += data_file.stat().st_size\n                    except Exception:\n                        pass  # Continue even if we can't load environments\n\n            elif dt == \"reliability\":\n                # For reliability, look for environments and reliability data files\n                env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                rel_file = cache_dir / f\"data_reliability_{params_hash}.npy\"\n                if env_file.exists():\n                    total_size += env_file.stat().st_size\n                if rel_file.exists():\n                    total_size += rel_file.stat().st_size\n\n            # Convert size to human readable format\n            size_str = self._format_file_size(total_size)\n\n            cache_info.append(\n                {\n                    \"data_type\": dt,\n                    \"size\": size_str,\n                    \"parameters\": param_str,\n                    \"date\": date_str,\n                    \"hash\": params_hash[:8],  # Show first 8 chars of hash\n                }\n            )\n\n    if not cache_info:\n        return \"No cache files found.\"\n\n    # Format the output as a table\n    output_lines = []\n    output_lines.append(\"Cache Files Summary\")\n    output_lines.append(\"=\" * 80)\n    output_lines.append(f\"{'Data Type':&lt;15} {'Size':&lt;10} {'Date':&lt;20} {'Hash':&lt;10} {'Parameters'}\")\n    output_lines.append(\"-\" * 80)\n\n    for info in cache_info:\n        output_lines.append(f\"{info['data_type']:&lt;15} {info['size']:&lt;10} {info['date']:&lt;20} \" f\"{info['hash']:&lt;10} {info['parameters']}\")\n\n    output_lines.append(\"-\" * 80)\n    output_lines.append(f\"Total cache entries: {len(cache_info)}\")\n\n    result = \"\\n\".join(output_lines)\n    print(result)\n</code></pre>"},{"location":"api/registration/","title":"Registration API Reference","text":""},{"location":"api/registration/#vrAnalysis.registration","title":"<code>registration</code>","text":""},{"location":"api/sessions/","title":"Sessions API Reference","text":""},{"location":"api/sessions/#vrAnalysis.sessions","title":"<code>sessions</code>","text":""},{"location":"api/vrAnalysis2/","title":"vrAnalysis API Reference","text":""},{"location":"api/vrAnalysis2/#vrAnalysis","title":"<code>vrAnalysis</code>","text":""},{"location":"examples/registration/","title":"Registration Workflow Example","text":"<p>This example demonstrates a complete registration workflow for processing VR session data.</p>"},{"location":"examples/registration/#basic-registration","title":"Basic Registration","text":"<pre><code>from vrAnalysis.registration import B2Registration\nfrom vrAnalysis.sessions.b2session import B2RegistrationOpts\n\n# Create registration options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,\n    redCellProcessing=True,\n    neuropilCoefficient=0.7,\n    tau=1.5,\n    fs=6\n)\n\n# Create and run registration\nregistration = B2Registration(\n    mouse_name=\"mouse001\",\n    date_string=\"2024-01-15\",\n    session_id=\"001\",\n    opts=opts\n)\n\nregistration.register()\n</code></pre>"},{"location":"examples/registration/#batch-registration","title":"Batch Registration","text":"<p>Register multiple sessions from the database:</p> <pre><code>from vrAnalysis.database import get_database\nfrom vrAnalysis.sessions.b2session import B2RegistrationOpts\n\n# Get database\ndb = get_database(\"vrSessions\")\n\n# Find sessions needing registration\nneeds_reg = db.needs_registration(mouseName=\"mouse001\")\n\n# Create options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,\n    redCellProcessing=True\n)\n\n# Register each session\nfor _, row in needs_reg.iterrows():\n    try:\n        registration = db.make_b2registration(row, opts)\n        registration.register()\n\n        # Update database\n        db.update_database_field(\"vrRegistration\", True, uSessionID=row[\"uSessionID\"])\n        print(f\"Successfully registered: {registration.session_print()}\")\n    except Exception as e:\n        print(f\"Error registering {row['uSessionID']}: {e}\")\n        db.update_database_field(\"vrRegistrationError\", True, uSessionID=row[\"uSessionID\"])\n</code></pre>"},{"location":"examples/registration/#custom-registration-options","title":"Custom Registration Options","text":"<p>Use different options for different sessions:</p> <pre><code># Standard registration\nstandard_opts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    oasis=True,\n    tau=1.5\n)\n\n# High-resolution registration\nhighres_opts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    oasis=True,\n    tau=0.8,  # Higher temporal resolution\n    fs=10     # Higher sampling rate\n)\n\n# Apply based on session criteria\nif session_date &gt;= \"2024-01-01\":\n    opts = highres_opts\nelse:\n    opts = standard_opts\n</code></pre>"},{"location":"examples/session_analysis/","title":"Session Analysis Example","text":"<p>This example demonstrates how to perform analysis on a single session.</p>"},{"location":"examples/session_analysis/#loading-and-processing-data","title":"Loading and Processing Data","text":"<pre><code>from vrAnalysis.sessions import create_b2session\nfrom vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Create session\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n\n# Load data\nsession.load_data()\n\n# Generate spike maps\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(\n    bin_size=5.0,\n    by_environment=True,\n    min_occupancy=0.1\n)\n\n# Access maps\nfor env_idx, env in enumerate(maps.environments):\n    occ_map = maps.occmap[env_idx]\n    spk_map = maps.spkmap[env_idx]\n\n    # Perform analysis on maps\n    # (analysis code here)\n</code></pre>"},{"location":"examples/session_analysis/#analyzing-place-cells","title":"Analyzing Place Cells","text":"<pre><code>from vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Generate spike maps and reliability\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(bin_size=5.0, by_environment=True)\nreliability = processor.get_reliability(envnum=0)  # Get reliability for environment 0\n\n# Access results\nreliability_scores = reliability.reliability  # Array of reliability scores per ROI\n</code></pre>"},{"location":"examples/session_analysis/#visualizing-results","title":"Visualizing Results","text":"<pre><code># Plot spike maps for cells with high reliability\n# (You'll need to implement plotting based on your visualization needs)\nimport matplotlib.pyplot as plt\n\n# Example: plot spike map for a single ROI\nroi_idx = 0\nif maps.by_environment:\n    spk_map = maps.spkmap[0][roi_idx]  # First environment, specific ROI\n    occ_map = maps.occmap[0]\nelse:\n    spk_map = maps.spkmap[roi_idx]\n    occ_map = maps.occmap\n\n# Create rate map (spikes per second)\nrate_map = spk_map / (occ_map + 1e-6)  # Avoid division by zero\n\nplt.imshow(rate_map, aspect='auto', origin='lower')\nplt.colorbar(label='Firing rate (Hz)')\nplt.title(f\"ROI {roi_idx}\")\nplt.show()\n</code></pre>"},{"location":"modules/analysis/","title":"Analysis","text":"<p>The <code>vrAnalysis.analysis</code> module provides tools for analyzing VR session data, including place cell analysis, reliability metrics, and plasticity measurements.</p>"},{"location":"modules/analysis/#analysis-modules","title":"Analysis Modules","text":""},{"location":"modules/analysis/#place-cell-reliability","title":"Place Cell Reliability","text":"<p>Analyze place cell reliability:</p> <pre><code>from vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Create processor and get reliability\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(bin_size=5.0, by_environment=True)\nreliability = processor.get_reliability(envnum=0)\n\n# Access reliability scores\nreliability_scores = reliability.reliability\n</code></pre>"},{"location":"modules/analysis/#changing-place-fields","title":"Changing Place Fields","text":"<p>Track how place fields change across sessions:</p> <pre><code>from vrAnalysis.multisession import MultiSessionSpkmaps\nfrom vrAnalysis.tracking import Tracker\n\n# Create multi-session object\ntracker = Tracker(\"mouse001\")\nmulti = MultiSessionSpkmaps(tracker)\n\n# Get maps for same environment across sessions\nenvnum = 0\nmaps_list = multi.get_env_maps(envnum)\n\n# Compare place fields between sessions\n# (implement comparison logic based on your needs)\n</code></pre>"},{"location":"modules/analysis/#tracked-plasticity","title":"Tracked Plasticity","text":"<p>Analyze plasticity in tracked cells:</p> <pre><code>from vrAnalysis.tracking import Tracker\n\n# Get tracked ROIs\ntracker = Tracker(\"mouse001\")\nidx_tracked, extras = tracker.get_tracked_idx()\n\n# Analyze changes in tracked cells\n# (implement analysis based on your needs)\n</code></pre>"},{"location":"modules/analysis/#common-analysis-workflows","title":"Common Analysis Workflows","text":""},{"location":"modules/analysis/#single-session-analysis","title":"Single Session Analysis","text":"<pre><code>from vrAnalysis.sessions import create_b2session\nfrom vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Load session\nsession = create_b2session(\"mouse001\", \"2024-01-15\", \"001\")\nsession.load_data()\n\n# Generate spike maps\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(bin_size=5.0)\n\n# Perform analysis\n# (analysis code here)\n</code></pre>"},{"location":"modules/analysis/#multi-session-analysis","title":"Multi-Session Analysis","text":"<pre><code>from vrAnalysis.multisession import MultiSessionSpkmaps\nfrom vrAnalysis.tracking import Tracker\n\n# Create tracker for a mouse\ntracker = Tracker(\"mouse001\")\n\n# Create multi-session object\nmulti = MultiSessionSpkmaps(tracker)\n\n# Perform cross-session analysis\n# (analysis code here)\n</code></pre>"},{"location":"modules/analysis/#see-also","title":"See Also","text":"<ul> <li>Multi-session Analysis for cross-session workflows</li> <li>Tracking Module for cell tracking</li> <li>Processors Module for data processing</li> </ul>"},{"location":"modules/database/","title":"Database Management","text":"<p>The <code>vrAnalysis.database</code> module provides classes and functions for managing VR session data in a database. It's designed to work with Microsoft Access databases (<code>.accdb</code> files) by default, but can be adapted to other SQL databases.</p>"},{"location":"modules/database/#core-classes","title":"Core Classes","text":""},{"location":"modules/database/#basedatabase","title":"BaseDatabase","text":"<p>The base class for database operations. Provides core functionality for connecting to databases, querying records, and updating data.</p> <p>Key Methods:</p> <ul> <li><code>get_table()</code>: Query records from the database table</li> <li><code>get_record()</code>: Retrieve a single record by unique identifiers</li> <li><code>update_database_field()</code>: Update field values for matching records</li> <li><code>add_record()</code>: Add new records to the database</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis.database import get_database\n\n# Get a database instance\ndb = get_database(\"vrMice\")\n\n# Query records\nmice = db.get_table()\n\n# Get a specific record (using unique identifier)\nmouse = db.get_record(\"mouse001\")  # Uses first unique field\n\n# Update a field\ndb.update_database_field(\"someField\", \"newValue\", mouseName=\"mouse001\")\n</code></pre>"},{"location":"modules/database/#sessiondatabase","title":"SessionDatabase","text":"<p>Specialized database class for managing VR sessions. Extends <code>BaseDatabase</code> with session-specific functionality.</p> <p>Key Methods:</p> <ul> <li><code>iter_sessions()</code>: Create B2Session objects from database records</li> <li><code>make_b2session()</code>: Create a B2Session from a database row</li> <li><code>make_b2registration()</code>: Create a B2Registration object for preprocessing</li> <li><code>needs_registration()</code>: Find sessions that need registration</li> <li><code>needs_s2p()</code>: Find sessions that need suite2p processing or QC</li> <li><code>check_s2p()</code>: Verify suite2p status consistency</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis.database import get_database\n\n# Get session database\ndb = get_database(\"vrSessions\")\n\n# Find sessions needing registration\nneeds_reg = db.needs_registration(mouseName=\"mouse001\")\n\n# Create session objects\nsessions = db.iter_sessions(mouseName=\"mouse001\", sessionQC=True)\n\n# Check suite2p status (if method exists)\n# db.check_s2p(with_database_update=True)\n</code></pre>"},{"location":"modules/database/#configuration","title":"Configuration","text":"<p>Database configuration is managed through the <code>get_database_metadata()</code> function. You'll need to edit this function to match your database setup:</p> <pre><code>def get_database_metadata(db_name: str) -&gt; dict:\n    dbdict = {\n        \"vrSessions\": {\n            \"db_path\": r\"C:\\path\\to\\your\\database\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"sessiondb\",\n            \"uid\": \"uSessionID\",\n            \"backup_path\": r\"D:\\backup\\path\",\n            \"unique_fields\": [(\"mouseName\", str), (\"sessionDate\", datetime), (\"sessionID\", int)],\n            \"default_conditions\": {\"sessionQC\": True},\n            \"constructor\": SessionDatabase,\n        },\n        # ... other databases\n    }\n    return dbdict[db_name]\n</code></pre>"},{"location":"modules/database/#querying-data","title":"Querying Data","text":"<p>The <code>get_table()</code> method supports flexible querying:</p> <pre><code># Simple equality\ndf = db.get_table(mouseName=\"mouse001\")\n\n# Comparison operators\ndf = db.get_table(sessionID=(5, \"&gt;\"))  # sessionID &gt; 5\n\n# Multiple conditions (AND logic)\ndf = db.get_table(mouseName=\"mouse001\", imaging=True)\n\n# Disable default conditions\ndf = db.get_table(use_default=False, mouseName=\"mouse001\")\n</code></pre>"},{"location":"modules/database/#adding-records","title":"Adding Records","text":"<p>Use the GUI to add new records:</p> <pre><code>from vrAnalysis.uilib.add_entry_gui import add_entry_gui\n\n# Open GUI for adding entries\nadd_entry_gui(\"vrSessions\")\n</code></pre> <p>Or programmatically:</p> <pre><code># Create insert statement\ncolumns = [\"mouseName\", \"sessionDate\", \"sessionID\", ...]\nvalues = [\"mouse001\", datetime(2024, 1, 15), 1, ...]\ninsert_stmt = f\"INSERT INTO {db.table_name} ({', '.join(columns)}) VALUES ({', '.join(['?'] * len(columns))})\"\n\n# Add record\ndb.add_record(insert_stmt, columns, values)\n</code></pre>"},{"location":"modules/database/#database-backup","title":"Database Backup","text":"<p>Automatically backup your database:</p> <pre><code>db.save_backup()\n</code></pre>"},{"location":"modules/database/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide for basic usage</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/helpers/","title":"Helpers","text":"<p>The <code>vrAnalysis.helpers</code> module provides utility functions for common operations.</p>"},{"location":"modules/helpers/#helper-modules","title":"Helper Modules","text":""},{"location":"modules/helpers/#plotting","title":"Plotting","text":"<p>Utilities for creating plots:</p> <pre><code>from vrAnalysis.helpers.plotting import plot_spike_map, plot_traces\n\n# Plot spike map\nplot_spike_map(spike_map, occupancy_map)\n\n# Plot calcium traces\nplot_traces(traces, time_axis)\n</code></pre>"},{"location":"modules/helpers/#signals","title":"Signals","text":"<p>Signal processing utilities:</p> <pre><code>from vrAnalysis.helpers.signals import smooth, normalize\n\n# Smooth signal\nsmoothed = smooth(signal, window_size=5)\n\n# Normalize signal\nnormalized = normalize(signal)\n</code></pre>"},{"location":"modules/helpers/#indexing","title":"Indexing","text":"<p>Indexing utilities:</p> <pre><code>from vrAnalysis.helpers.indexing import get_plane_indices\n\n# Get indices for specific plane\nindices = get_plane_indices(session, plane=0)\n</code></pre>"},{"location":"modules/helpers/#vr-support","title":"VR Support","text":"<p>VR-specific utilities:</p> <pre><code>from vrAnalysis.helpers.vrsupport import get_environment_transitions\n\n# Get environment transition times\ntransitions = get_environment_transitions(behavior)\n</code></pre>"},{"location":"modules/helpers/#see-also","title":"See Also","text":"<ul> <li>Individual helper modules for specific functionality</li> <li>API Reference for complete function listings</li> </ul>"},{"location":"modules/multisession/","title":"Multi-Session Analysis","text":"<p>The <code>vrAnalysis.multisession</code> module provides tools for analyzing data across multiple sessions, enabling population-level and longitudinal analyses.</p>"},{"location":"modules/multisession/#multisessionspkmaps-class","title":"MultiSessionSpkmaps Class","text":"<p>The <code>MultiSessionSpkmaps</code> class manages multiple sessions and provides cross-session spike map analysis capabilities.</p> <pre><code>from vrAnalysis.multisession import MultiSessionSpkmaps\nfrom vrAnalysis.tracking import Tracker\n\n# Create tracker for a mouse\ntracker = Tracker(\"mouse001\")\n\n# Create multi-session object\nmulti = MultiSessionSpkmaps(tracker)\n\n# Access processors (one per session)\nprocessors = multi.processors\n\n# Access sessions\nsessions = tracker.sessions\n</code></pre>"},{"location":"modules/multisession/#cross-session-analysis","title":"Cross-Session Analysis","text":"<p>Analyze data across sessions:</p> <pre><code># Get spike maps for a specific environment across sessions\nenvnum = 0\nmaps_list = multi.get_env_maps(envnum)\n\n# Get reliability across sessions\nreliability = multi.get_reliability(envnum)\n\n# Get tracked ROIs\nidx_tracked, extras = tracker.get_tracked_idx()\n</code></pre>"},{"location":"modules/multisession/#see-also","title":"See Also","text":"<ul> <li>Tracking Module for cell tracking</li> <li>Analysis Module for analysis tools</li> </ul>"},{"location":"modules/processors/","title":"Processors","text":"<p>The <code>vrAnalysis.processors</code> module provides data processing pipelines that transform session data into analysis-ready formats.</p>"},{"location":"modules/processors/#spike-map-processor","title":"Spike Map Processor","text":"<p>The <code>SpkmapProcessor</code> creates spatial representations of neural activity.</p>"},{"location":"modules/processors/#maps-class","title":"Maps Class","text":"<p>The <code>Maps</code> dataclass contains occupancy, speed, and spike maps:</p> <ul> <li><code>occmap</code>: Occupancy map (time spent in each spatial bin)</li> <li><code>speedmap</code>: Speed map (average speed in each spatial bin)</li> <li><code>spkmap</code>: Spike map (spike count in each spatial bin)</li> <li><code>by_environment</code>: Whether maps are separated by environment</li> <li><code>rois_first</code>: Whether ROI dimension comes first in spkmap</li> </ul>"},{"location":"modules/processors/#processing-spike-maps","title":"Processing Spike Maps","text":"<pre><code>from vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Create processor\nprocessor = SpkmapProcessor(session)\n\n# Process maps\nmaps = processor.process(\n    bin_size=5.0,  # 5 cm bins\n    by_environment=True,\n    rois_first=True,\n    min_occupancy=0.1  # Minimum occupancy threshold\n)\n\n# Access maps\nif maps.by_environment:\n    # Maps are lists, one per environment\n    for env_idx, env in enumerate(maps.environments):\n        occ = maps.occmap[env_idx]\n        spk = maps.spkmap[env_idx]\n        speed = maps.speedmap[env_idx]\nelse:\n    # Maps are single arrays\n    occ = maps.occmap\n    spk = maps.spkmap\n    speed = maps.speedmap\n</code></pre>"},{"location":"modules/processors/#map-averaging","title":"Map Averaging","text":"<p>Average maps across ROIs or sessions:</p> <pre><code># Average across ROIs\naveraged = maps.average_rois()\n\n# Average across environments\naveraged = maps.average_environments()\n</code></pre>"},{"location":"modules/processors/#map-visualization","title":"Map Visualization","text":"<p>Maps can be visualized using helper functions:</p> <pre><code>from vrAnalysis.helpers.plotting import plot_spike_map\n\n# Plot spike map for a single ROI\nplot_spike_map(\n    maps.spkmap[roi_idx],\n    maps.occmap,\n    title=f\"ROI {roi_idx}\"\n)\n</code></pre>"},{"location":"modules/processors/#processing-options","title":"Processing Options","text":""},{"location":"modules/processors/#bin-size","title":"Bin Size","text":"<p>Control spatial resolution:</p> <pre><code># Fine bins (2 cm)\nfine_maps = processor.process(bin_size=2.0)\n\n# Coarse bins (10 cm)\ncoarse_maps = processor.process(bin_size=10.0)\n</code></pre>"},{"location":"modules/processors/#environment-separation","title":"Environment Separation","text":"<p>Separate maps by environment:</p> <pre><code># Separate by environment\nby_env = processor.process(by_environment=True)\n\n# Combined across environments\ncombined = processor.process(by_environment=False)\n</code></pre>"},{"location":"modules/processors/#occupancy-filtering","title":"Occupancy Filtering","text":"<p>Filter out low-occupancy bins:</p> <pre><code># Only include bins with &gt; 0.1 seconds occupancy\nfiltered = processor.process(min_occupancy=0.1)\n</code></pre>"},{"location":"modules/processors/#caching","title":"Caching","text":"<p>Processors cache results to speed up repeated operations:</p> <pre><code># First call processes data\nmaps1 = processor.process(bin_size=5.0)\n\n# Second call uses cache\nmaps2 = processor.process(bin_size=5.0)  # Fast!\n</code></pre>"},{"location":"modules/processors/#custom-processors","title":"Custom Processors","text":"<p>You can create custom processors by extending the base processor class:</p> <pre><code>from vrAnalysis.processors.spkmaps import SpkmapProcessor\n\nclass CustomProcessor(SpkmapProcessor):\n    def process_custom(self, **kwargs):\n        # Custom processing logic\n        maps = self.process(**kwargs)\n        # Additional processing\n        return processed_maps\n</code></pre>"},{"location":"modules/processors/#see-also","title":"See Also","text":"<ul> <li>Sessions Module for session data</li> <li>Analysis Module for analysis workflows</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/registration/","title":"Registration","text":"<p>The <code>vrAnalysis.registration</code> module handles preprocessing and registration of experimental data. Registration aligns behavioral and imaging data, runs deconvolution, and prepares data for analysis.</p>"},{"location":"modules/registration/#core-classes","title":"Core Classes","text":""},{"location":"modules/registration/#b2registration","title":"B2Registration","text":"<p>Extends <code>B2Session</code> with registration workflows. Handles the complete preprocessing pipeline.</p> <p>Key Methods:</p> <ul> <li><code>register()</code>: Run the complete registration workflow</li> <li><code>register_behavior()</code>: Process and register behavioral data</li> <li><code>register_imaging()</code>: Process and register imaging data</li> <li><code>register_oasis()</code>: Run OASIS deconvolution</li> <li><code>register_redcell()</code>: Process red cell annotations</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis.registration import B2Registration\nfrom vrAnalysis.sessions.b2session import B2RegistrationOpts\n\n# Create registration options\nopts = B2RegistrationOpts(\n    vrBehaviorVersion=1,\n    imaging=True,\n    oasis=True,\n    redCellProcessing=True,\n    neuropilCoefficient=0.7,\n    tau=1.5,\n    fs=6\n)\n\n# Create registration object\nregistration = B2Registration(\n    mouse_name=\"mouse001\",\n    date_string=\"2024-01-15\",\n    session_id=\"001\",\n    opts=opts\n)\n\n# Run registration\nregistration.register()\n</code></pre>"},{"location":"modules/registration/#b2registrationopts","title":"B2RegistrationOpts","text":"<p>Dataclass for configuring registration options.</p> <p>Parameters:</p> <ul> <li><code>vrBehaviorVersion</code>: Version of vrControl software used (1 or 2)</li> <li><code>facecam</code>: Whether to process facecam data</li> <li><code>imaging</code>: Whether to process imaging data</li> <li><code>oasis</code>: Whether to run OASIS deconvolution</li> <li><code>redCellProcessing</code>: Whether to process red cell annotations</li> <li><code>clearOne</code>: Whether to clear One files</li> <li><code>neuropilCoefficient</code>: Coefficient for neuropil subtraction (default: 0.7)</li> <li><code>tau</code>: OASIS tau parameter (default: 1.5)</li> <li><code>fs</code>: Sampling frequency in Hz (default: 6)</li> </ul>"},{"location":"modules/registration/#registration-workflow","title":"Registration Workflow","text":"<p>The registration process includes:</p> <ol> <li>Behavior Registration: Load and process behavioral data from Timeline files</li> <li>Imaging Registration: Load suite2p outputs and process imaging data</li> <li>OASIS Deconvolution: Deconvolve calcium traces to get spike trains</li> <li>Red Cell Processing: Process red cell classifier results</li> <li>Data Alignment: Align behavioral and imaging data in time</li> </ol>"},{"location":"modules/registration/#behavior-processing","title":"Behavior Processing","text":"<p>Behavior processing handles different versions of vrControl:</p> <pre><code>from vrAnalysis.registration.behavior import register_behavior\n\n# Process behavior for version 1\nregistration = register_behavior(registration, behavior_type=1)\n\n# Process behavior for version 2 (CR hippocampus)\nregistration = register_behavior(registration, behavior_type=2)\n</code></pre> <p>Different behavior versions may require different processing: - Position tracking - Reward delivery - Lick detection - Environment transitions</p>"},{"location":"modules/registration/#oasis-deconvolution","title":"OASIS Deconvolution","text":"<p>OASIS deconvolution converts calcium traces to spike trains:</p> <pre><code>from vrAnalysis.registration.oasis import oasis_deconvolution\n\n# Run OASIS\ndeconvolved = oasis_deconvolution(\n    traces,\n    tau=1.5,\n    fs=6\n)\n</code></pre>"},{"location":"modules/registration/#red-cell-processing","title":"Red Cell Processing","text":"<p>Red cell processing is automatically handled during registration when <code>redCellProcessing=True</code> in the options. The <code>RedCellProcessing</code> class provides methods for computing red cell features:</p> <pre><code>from vrAnalysis.registration.redcell import RedCellProcessing\n\n# Red cell processing is typically done during registration\n# But you can also use it manually:\nred_cell = RedCellProcessing(registration)\n\n# Compute red cell features\ndot_product = red_cell.compute_dot()\ncorr_coeff = red_cell.compute_corr()\nphase_corr = red_cell.cropped_phase_correlation()[3]\n\n# Update red cell index based on cutoffs\nred_cell.update_red_idx(\n    s2p_cutoff=[0.5, np.inf],\n    dot_product_cutoff=[0.3, np.inf],\n    corr_coef_cutoff=[0.4, np.inf]\n)\n</code></pre>"},{"location":"modules/registration/#database-integration","title":"Database Integration","text":"<p>Registration status is tracked in the database:</p> <pre><code>from vrAnalysis.database import get_database\n\n# Get database\ndb = get_database(\"vrSessions\")\n\n# Find sessions needing registration\nneeds_reg = db.needs_registration(mouseName=\"mouse001\")\n\n# Run registration for each\nfor _, row in needs_reg.iterrows():\n    registration = db.make_b2registration(row, opts)\n    registration.register()\n\n    # Update database\n    db.update_database_field(\"vrRegistration\", True, uSessionID=row[\"uSessionID\"])\n</code></pre>"},{"location":"modules/registration/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide for basic usage</li> <li>Sessions Module for session management</li> <li>Examples for detailed workflows</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/sessions/","title":"Sessions","text":"<p>The <code>vrAnalysis.sessions</code> module provides classes for loading and managing VR session data. Sessions are the core objects that represent individual experimental runs.</p>"},{"location":"modules/sessions/#core-classes","title":"Core Classes","text":""},{"location":"modules/sessions/#b2session","title":"B2Session","text":"<p>The main session class that loads and provides access to experimental data.</p> <p>Key Attributes:</p> <ul> <li><code>mouse_name</code>: Mouse identifier</li> <li><code>date</code>: Session date</li> <li><code>session_id</code>: Session identifier</li> <li><code>params</code>: Session parameters (B2SessionParams)</li> </ul> <p>Key Methods:</p> <ul> <li><code>load_data()</code>: Load behavioral and imaging data</li> <li><code>get_spks()</code>: Get spike data</li> <li><code>get_behavior()</code>: Get behavioral data</li> <li><code>session_print()</code>: Print session information</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis.sessions import create_b2session\n\n# Create session with default parameters\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n\n# Load data\nsession.load_data()\n\n# Access spike data\nspks = session.get_spks()\n\n# Access behavioral data\nbehavior = session.get_behavior()\n</code></pre>"},{"location":"modules/sessions/#b2sessionparams","title":"B2SessionParams","text":"<p>Dataclass for configuring how session data is loaded.</p> <p>Parameters:</p> <ul> <li><code>spks_type</code>: Type of spike data to load (e.g., \"significant\")</li> <li><code>keep_planes</code>: List of plane indices to keep (None = all)</li> <li><code>good_labels</code>: List of ROI labels to keep (e.g., [\"c\", \"d\"])</li> <li><code>fraction_filled_threshold</code>: Threshold for ROI fraction filled</li> <li><code>footprint_size_threshold</code>: Threshold for ROI footprint size</li> <li><code>exclude_silent_rois</code>: Whether to exclude ROIs with no activity</li> <li><code>neuropil_coefficient</code>: Neuropil subtraction coefficient</li> <li><code>exclude_redundant_rois</code>: Whether to exclude redundant ROIs</li> </ul> <p>Example:</p> <pre><code>from vrAnalysis.sessions import create_b2session, B2SessionParams\n\n# Create custom parameters\nparams = B2SessionParams(\n    spks_type=\"significant\",\n    keep_planes=[0, 1, 2],\n    good_labels=[\"c\", \"d\"],\n    exclude_silent_rois=True\n)\n\n# Create session with custom parameters\nsession = create_b2session(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\",\n    params=params\n)\n</code></pre>"},{"location":"modules/sessions/#session-data-structure","title":"Session Data Structure","text":"<p>Sessions provide access to:</p>"},{"location":"modules/sessions/#behavioral-data","title":"Behavioral Data","text":"<ul> <li>Position tracking</li> <li>Velocity</li> <li>Rewards</li> <li>Licks</li> <li>Environment information</li> </ul>"},{"location":"modules/sessions/#imaging-data","title":"Imaging Data","text":"<ul> <li>Calcium traces (F, Fneu, etc.)</li> <li>Spike data (deconvolved or raw)</li> <li>ROI information</li> <li>Neuropil signals</li> <li>Suite2p statistics</li> </ul>"},{"location":"modules/sessions/#metadata","title":"Metadata","text":"<ul> <li>Session information</li> <li>Processing status</li> <li>Quality control flags</li> </ul>"},{"location":"modules/sessions/#loading-data","title":"Loading Data","text":"<p>Data is loaded lazily by default. Access data properties to trigger loading:</p> <pre><code># Data is loaded when accessed\nspks = session.spks  # Loads spike data\nbehavior = session.behavior  # Loads behavioral data\n</code></pre>"},{"location":"modules/sessions/#session-parameters","title":"Session Parameters","text":"<p>Session parameters control data filtering and processing:</p> <pre><code># Update parameters\nsession.params.update(\n    good_labels=[\"c\"],\n    exclude_silent_rois=True\n)\n\n# Reload data with new parameters\nsession.load_data()\n</code></pre>"},{"location":"modules/sessions/#file-paths","title":"File Paths","text":"<p>Sessions automatically construct paths to data files:</p> <pre><code># Suite2p path\ns2p_path = session.s2p_path\n\n# Timeline path\ntimeline_path = session.timeline_path\n\n# Session root\nsession_root = session.session_root\n</code></pre>"},{"location":"modules/sessions/#see-also","title":"See Also","text":"<ul> <li>Quickstart Guide for basic usage</li> <li>Registration Module for preprocessing workflows</li> <li>API Reference for complete function signatures</li> </ul>"},{"location":"modules/tracking/","title":"Tracking","text":"<p>The <code>vrAnalysis.tracking</code> module provides functionality for tracking cells across multiple sessions, enabling longitudinal analysis of individual neurons.</p>"},{"location":"modules/tracking/#cell-tracking","title":"Cell Tracking","text":"<p>Track the same cells across sessions:</p> <pre><code>from vrAnalysis.tracking import Tracker\n\n# Create tracker for a mouse (automatically loads all tracked sessions)\ntracker = Tracker(\"mouse001\")\n\n# Get tracked ROIs across specific sessions\nidx_tracked, extras = tracker.get_tracked_idx(\n    idx_ses=[0, 1],  # Track between first two sessions\n    use_session_filters=True,\n    keep_method=\"any\"  # Keep ROI if valid in any session, or \"all\" for all sessions\n)\n\n# idx_tracked is a (num_sessions, num_tracked_rois) array\n# Each column represents a tracked ROI across sessions\nfor roi_idx in range(idx_tracked.shape[1]):\n    session1_roi = idx_tracked[0, roi_idx]\n    session2_roi = idx_tracked[1, roi_idx]\n    print(f\"ROI {session1_roi} in session 1 matches ROI {session2_roi} in session 2\")\n\n# Access tracking metadata\ncluster_ids = extras[\"cluster_ids\"]\nsample_silhouettes = extras[\"sample_silhouettes\"]\ncluster_silhouettes = extras[\"cluster_silhouettes\"]\n</code></pre>"},{"location":"modules/tracking/#tracking-methods","title":"Tracking Methods","text":"<p>Tracking uses ROICaT clustering to match ROIs across sessions:</p> <pre><code># Get cluster index for a specific cluster ID\ncluster_id = 5\ncluster_idx = tracker.get_cluster_idx(cluster_id)\n# Returns array with ROI index in each session, or -1 if not present\n\n# Access tracking files directly\nlabels = tracker.labels  # List of label arrays, one per session\nsample_silhouettes = tracker.sample_silhouettes  # Sample silhouettes per session\ncluster_silhouettes = tracker.cluster_silhouettes  # Cluster silhouettes (shared)\n</code></pre>"},{"location":"modules/tracking/#multi-session-tracking","title":"Multi-Session Tracking","text":"<p>Track cells across multiple sessions using MultiSessionSpkmaps:</p> <pre><code>from vrAnalysis.multisession import MultiSessionSpkmaps\nfrom vrAnalysis.tracking import Tracker\n\n# Create tracker\ntracker = Tracker(\"mouse001\")\n\n# Create multi-session object for spike map analysis\nmulti = MultiSessionSpkmaps(tracker)\n\n# Get tracked ROIs across all sessions\nidx_tracked, extras = tracker.get_tracked_idx(\n    idx_ses=None  # Use all sessions\n)\n\n# Access tracking chains\nnum_tracked = idx_tracked.shape[1]\nprint(f\"Found {num_tracked} tracked ROIs across {len(tracker.sessions)} sessions\")\n</code></pre>"},{"location":"modules/tracking/#see-also","title":"See Also","text":"<ul> <li>Multi-session Analysis for cross-session workflows</li> <li>Analysis Module for tracked cell analysis</li> </ul>"}]}