{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"vrAnalysis Documentation","text":"<p>Welcome to the documentation for vrAnalysis, which is the Python codebase I use for processing and analyzing virtual reality (VR) behavioral and imaging experiments. This package is not \"software\" that works out of the box, it's simply a collection of code I've written to analyze my data. I hope I designed it well enough that it can be used by others, which is why I've made this documentation system. So if it isn't working for you, that might be expected! There's a lot of powerful stuff in here, so hopefully it's useful, and if you really think something in here is useful to you but you can't figure it out, let me know and I'll help you get it working.</p> <p>Also: if you are reading the documentation and think I could improve it in some way, please tell me. Even if I don't have time to make the changes right away, I appreciate the feedback and will try eventually.</p>"},{"location":"#overview","title":"Overview","text":"<p>vrAnalysis is designed to work with:</p> <ul> <li>Behavioral data from vrControl experiments</li> <li>Imaging data processed with suite2p</li> <li>Database management using Microsoft Access databases (or other SQL databases with minimal modifications)</li> <li>Session tracking following the Alyx directory structure</li> <li>ROI Tracking using ROICaT</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<p>The goal of the package (aka my coding philosophy) is to make it easy to analyze data from VR experiments. Analysis is sometimes frustrating because it's slow, so I take the time to write good code that is efficient and has clear and simple interfaces. From my perspective, it's much better for a notebook (ipynb) file to be a simple clear and obvious example of how to use the code, rather than a complex hack of a bunch of things that eventually makes plots. Becuase of that, the code is sometimes implemented in complex ways which I thought were necessary to make the interface simple and clear and fast. </p> <p>Here are some of the key features that it makes possible:</p> <ul> <li>Database Management: Track and manage VR session data with an easy-to-use database interface</li> <li>Session Registration: Automated preprocessing of behavioral and imaging data</li> <li>Data Processing: Generate spike maps, occupancy maps, and other spatial representations</li> <li>Cell Tracking: Track cells across sessions for longitudinal analysis</li> <li>Multi-session Analysis: Analyze data across groups of sessions</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Installation Guide - Get started with vrAnalysis</li> <li>Workflows - Learn how the package can be used</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If this documentation is not enough, there's a few other ways to get help. Firstly,  just send me a slack message or email me. If you can't do that open a GitHub issue. In fact, it's better to open a GitHub issue anyway! Then future people with similar problems can benefit from finding the solution. </p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to vrAnalysis!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and clone the repository</li> <li>Create a conda environment:</li> </ol> <pre><code>conda env create -f environment.yml\nconda activate vrAnalysis\n</code></pre> <ol> <li>Install in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints where possible</li> <li>Write docstrings in NumPy format</li> <li>Keep line length to 150 characters (per <code>pyproject.toml</code>)</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Add docstrings to all public functions and classes</li> <li>Use NumPy-style docstrings</li> <li>Update relevant documentation pages when adding features</li> <li>Include examples in docstrings where helpful</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure existing tests pass</li> <li>Test with real data when possible</li> </ul>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ul> <li>Create a new branch for your changes</li> <li>Write clear commit messages</li> <li>Update documentation as needed</li> <li>Ensure all tests pass before submitting</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>If you have questions about contributing, please open an issue on GitHub.</p>"},{"location":"installation/","title":"Installation","text":"<p>The code is designed for developers. To install, first clone the repository and then create a conda environment called vrAnalysis. You can use the <code>environment.yml</code> file to create the environment, but I'd actually recommend not doing this and instead making an empty environment (python &gt;= 3.9!!!!) and then installing the package locally with <code>pip install -e .</code>. </p>"},{"location":"installation/#basic-code-installation","title":"Basic Code Installation","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/landoskape/vrAnalysis\ncd vrAnalysis\n</code></pre> <ol> <li>Create a conda environment called vrAnalysis:</li> </ol> <pre><code>conda env create -n vrAnalysis\nconda activate vrAnalysis\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre> <p>Note: there are \"extra\" dependencies that will not be installed by this method. The options are:</p> <ul> <li>registration: includes packages that require special compilers<ul> <li>oasis-deconv</li> <li>cvxpy</li> </ul> </li> <li>gui: includes packages that are used for the GUI for manually adding data to the database<ul> <li>pyqt5</li> <li>pyqtgraph</li> <li>napari[all]</li> </ul> </li> <li>all: includes all of the above</li> </ul> <p>To install the extra dependencies, use one of the following commands:</p> <pre><code>pip install -e .[registration]\npip install -e .[gui]\npip install -e .[all]\n</code></pre>"},{"location":"installation/#other-dependencies","title":"Other dependencies","text":"<p>As an analysis package of experimental data, there are a few other things that need to happen before you can actually use the package. They primarily relate to setting up your database, your data directories, and some other preprocessing that needs to happen with other python packages (primarily <code>suite2p</code> and <code>ROICaT</code>).</p> <p>For a detailed guide on all of this, see the registration workflow page which walks you through the whole process. </p>"},{"location":"installation/#pytorch","title":"PyTorch","text":"<p>Some of the package uses pytorch. For reasons that escape me, meta has not found a good way to install pytorch inside other packages using <code>pip install</code> or <code>conda install</code>. My workaround is to make my environment as described above, install <code>vrAnalysis</code>, then install pytorch manually using the instructions on the pytorch website.</p> <p>If you try to use things that depend on pytorch and haven't done this, you'll get import errors. Sorry. </p> <p>Tip</p> <p>Make sure to install PyTorch with the correct CUDA version for your GPU, and verify that GPU acceleration is working properly after installation.</p>"},{"location":"installation/#configuration","title":"Configuration","text":"<p>After installation, you have to configure paths for your data:</p> <ol> <li>Data Directory: Set your local data path in <code>vrAnalysis/files.py</code>:</li> </ol> <pre><code>def local_data_path() -&gt; Path:\n    return Path(\"C:/path/to/your/data\")\n</code></pre> <ol> <li>Database Paths: Configure database paths in <code>vrAnalysis/database.py</code> using the <code>get_database_metadata()</code> function.</li> </ol>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#architecture","title":"Architecture","text":"<p>vrAnalysis is organized around a few core concepts:</p>"},{"location":"overview/#sessions","title":"Sessions","text":"<p>A session represents a single experimental run. It's the central object that enables accesss to data for an experiment. In general, all more complicated analyses are built on top of session objects - in other words, the interface enabled by session is the bottleneck to all analysis of the package. </p> <p>Each session contains:</p> <ul> <li>Behavioral data (position, velocity, rewards, etc.)</li> <li>Imaging data (calcium traces, ROIs, etc.)</li> <li>Metadata (mouse name, date, session ID, etc.)</li> </ul> <p>Sessions are represented by the <code>B2Session</code> class, which provides methods to load and access data. (<code>B2Session</code> is one example of a possible <code>SessionData</code> class, but since all of my work was done at B2,  this is really the only one that's used. The other ones are for converting external data into a format that can be used by this package, which are explained elsewhere).</p>"},{"location":"overview/#database","title":"Database","text":"<p>The database tracks all sessions and their metadata. It's a simple interface that allows you to query your database easily and efficiently, and also to perform batch operations on sessions in the database. It uses Microsoft Access (<code>.accdb</code>) files by default, but can be adapted to other SQL databases.</p> <p>The <code>SessionDatabase</code> class provides methods to:</p> <ul> <li>Query sessions based on criteria</li> <li>Add new sessions</li> <li>Update session metadata</li> <li>Track registration status and quality control flags</li> </ul>"},{"location":"overview/#processors","title":"Processors","text":"<p>Processors transform session data into analysis-ready formats. For example:</p> <ul> <li><code>SpkmapProcessor</code>: Creates spatial maps of neural activity</li> <li>I haven't built any others yet, maybe one day!</li> </ul>"},{"location":"overview/#tracking","title":"Tracking","text":"<p>Tracking identifies the same cells across multiple sessions. This enables longitudinal analysis of how individual cells change over time. This module is built with the <code>Tracker</code> class which is a nice wrapper around ROICaT tracking files. It will only work (or be relevant) if you've used ROICaT to track cells across sessions. </p>"},{"location":"overview/#registration","title":"Registration","text":"<p>Registration is the process of preprocessing raw experimental data. This includes:</p> <ul> <li>Loading behavioral data from Timeline files</li> <li>Processing imaging data from suite2p outputs</li> <li>Running OASIS deconvolution</li> <li>Processing red cell annotations</li> <li>Aligning data in time</li> </ul> <p>Registration creates standardized data structures that can be used for analysis.</p>"},{"location":"overview/#data-flow","title":"Data Flow","text":"<pre><code>Raw Data (Timeline, vrControl, suite2p)\n    \u2193\nRegistration (B2Registration)\n    \u2193\nSession Data (B2Session)\n    \u2193\nProcessing (Processors)\n    \u2193\nFigures!\n</code></pre>"},{"location":"overview/#directory-structure","title":"Directory Structure","text":"<p>Not sure where else to put this, but vrAnalysis expects data organized in an Alyx-style structure. This is the structure that is used by the alyx database and is generally a really nice way to organize sessions of data colleted on certain days from specific mice. </p> <pre><code>localData/ # configure this in the `local_data_path()` function of `vrAnalysis/files.py`\n\u251c\u2500\u2500 mouse001/ # mouse name\n\u2502   \u251c\u2500\u2500 2024-01-15/ # date of session \n\u2502   \u2502   \u251c\u2500\u2500 001/ # session ID\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 suite2p/ # where your suite2p output should go \n\u2502   \u2502   \u2502   \u251c\u2500\u2500 onedata/ # where onedata will be stored after registration\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ... # other files\n\u2502   \u2502   \u2514\u2500\u2500 002/ # session ID\n\u2502   \u2514\u2500\u2500 2024-01-16/ # date of session\n\u2514\u2500\u2500 mouse002/ # mouse name\n</code></pre>"},{"location":"api/b2session/","title":"B2 Session API Reference","text":"<p>B2Sessions are the core class for loading and managing VR session data. They are the primary <code>SessionData</code> object built specifically for data coming from the B2 rig. </p>"},{"location":"api/b2session/#main-classes-and-functions","title":"Main Classes and Functions","text":""},{"location":"api/b2session/#vrAnalysis.sessions.B2Session","title":"<code>B2Session</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SessionData</code></p> <p>B2Session represents a registered VR imaging session with suite2p and ROICaT data.</p> <p>This class extends SessionData to provide specialized functionality for loading and managing B2 format session data, including suite2p outputs, ROICaT classifier results, and behavioral data.</p> <p>Attributes:</p> Name Type Description <code>opts</code> <code>B2RegistrationOpts</code> <p>Registration options for the session.</p> <code>preprocessing</code> <code>list[str]</code> <p>List of preprocessing steps that were applied during registration.</p> <code>params</code> <code>B2SessionParams</code> <p>Parameters for configuring data loading and ROI filtering.</p> <code>spks_types</code> <code>tuple[str]</code> <p>Tuple of spks types to load.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>@dataclass\nclass B2Session(SessionData):\n    \"\"\"\n    B2Session represents a registered VR imaging session with suite2p and ROICaT data.\n\n    This class extends SessionData to provide specialized functionality for\n    loading and managing B2 format session data, including suite2p outputs,\n    ROICaT classifier results, and behavioral data.\n\n    Attributes\n    ----------\n    opts : B2RegistrationOpts\n        Registration options for the session.\n    preprocessing : list[str]\n        List of preprocessing steps that were applied during registration.\n    params : B2SessionParams\n        Parameters for configuring data loading and ROI filtering.\n    spks_types: tuple[str]\n        Tuple of spks types to load.\n    \"\"\"\n\n    opts: B2RegistrationOpts = field(default_factory=B2RegistrationOpts, repr=False, init=False)\n    preprocessing: list[str] = field(default_factory=list, repr=False, init=False)\n    params: B2SessionParams = field(default_factory=B2SessionParams, repr=False)\n    _for_registration: bool = field(default=False, repr=False, init=True)\n    spks_types: tuple[str, ...] = (\"oasis\", \"deconvolved\", \"raw\", \"neuropil\", \"significant\", \"corrected\", \"sigbase\", \"sigrebase\")\n\n    @classmethod\n    def create(\n        cls,\n        mouse_name: str,\n        date: str,\n        session_id: str,\n        params: \"B2SessionParams\" | Dict[str, Any] | None = None,\n        for_registration: bool = False,\n    ) -&gt; \"B2Session\":\n        \"\"\"Create a B2Session object and (optionally) specify the parameters\n\n        Parameters\n        ----------\n        mouse_name: str\n            The name of the mouse\n        date: str\n            The date of the session\n        session_id: str\n            The id of the session\n        params: B2SessionParams, dict, or None\n            The parameters to use for the session. If None, the default parameters will be used.\n            If a dictionary, it can contain the keys:\n                - spks_type: str (which kind of spks data to load)\n                - keep_planes: list[int] (which planes to keep)\n                - good_labels: list[str] (which labels to keep from the roicat classifier analysis)\n                - fraction_filled_threshold: float (threshold for the fraction of the ROI that is filled -- based on local concavity analysis)\n                - footprint_size_threshold: int (threshold for the size of the ROI)\n        for_registration: bool\n            Whether the session is being created for registration. If True, doesn't attempt to perform \"additional loading\" of the data.\n        \"\"\"\n        if params is None:\n            params = B2SessionParams()\n        elif isinstance(params, dict):\n            params = B2SessionParams.from_dict(params)\n        elif isinstance(params, B2SessionParams):\n            pass\n        else:\n            raise ValueError(f\"params must be a B2SessionParams object or a dictionary\")\n        return cls(mouse_name, date, session_id, params, for_registration)\n\n    @property\n    def s2p_path(self) -&gt; Path:\n        \"\"\"\n        Path to suite2p directory.\n\n        Returns\n        -------\n        Path\n            Path object pointing to the suite2p subdirectory within the\n            session data path.\n        \"\"\"\n        return self.data_path / \"suite2p\"\n\n    @property\n    def roicat_path(self) -&gt; Path:\n        \"\"\"\n        Path to roicat directory.\n\n        Returns\n        -------\n        Path\n            Path object pointing to the roicat subdirectory within the\n            session data path.\n        \"\"\"\n        return self.data_path / \"roicat\"\n\n    @property\n    def recipe_loaders(self) -&gt; dict:\n        \"\"\"\n        Dictionary of loaders for loading data from recipes.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping loader type strings to loader functions.\n            Available loaders: \"S2P\", \"roiPosition\".\n        \"\"\"\n        return {\"S2P\": self.load_s2p, \"roiPosition\": self.get_roi_position}\n\n    @property\n    def recipe_transforms(self) -&gt; dict:\n        \"\"\"\n        Dictionary of transforms for applying to data when loading recipes.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping transform names to transform functions.\n            Available transforms: \"transpose\", \"idx_column1\".\n        \"\"\"\n        return {\"transpose\": lambda x: x.T, \"idx_column1\": lambda x: x[:, 1]}\n\n    def _load_spks(self, spks_type: str = None) -&gt; np.ndarray:\n        \"\"\"\n        Load spike data of the specified type.\n\n        Parameters\n        ----------\n        spks_type : str, optional\n            Type of spike data to load. If None, uses params.spks_type.\n            Options: \"oasis\", \"deconvolved\", \"raw\", \"neuropil\", \"significant\",\n            \"corrected\", \"sigbase\", \"sigrebase\".\n\n        Returns\n        -------\n        np.ndarray\n            Spike data array. Shape is (n_frames, n_rois) for most types.\n\n        Raises\n        ------\n        ValueError\n            If spks_type is not recognized.\n        \"\"\"\n        if spks_type == \"oasis\":\n            return self.loadone(\"mpci.roiActivityDeconvolvedOasis\")\n        elif spks_type == \"deconvolved\":\n            return self.loadone(\"mpci.roiActivityDeconvolved\")\n        elif spks_type == \"raw\":\n            return self.loadone(\"mpci.roiActivityF\")\n        elif spks_type == \"neuropil\":\n            return self.loadone(\"mpci.roiNeuropilActivityF\")\n        elif spks_type == \"significant\":\n            return self.loadone(\"mpci.roiSignificantFluorescence\", sparse=True, keep_sparse=False)\n        elif spks_type == \"sigbase\":\n            return self.loadone(\"mpci.roiSignificantFluorescenceBase\", sparse=True, keep_sparse=False)\n        elif spks_type == \"sigrebase\":\n            return self.loadone(\"mpci.roiSignificantFluorescenceRebase\", sparse=True, keep_sparse=False)\n        elif spks_type == \"corrected\":\n            return self.loadfcorr().T\n        else:\n            raise ValueError(f\"spks_type {spks_type} not recognized\")\n\n    def _are_spks_zero_baseline(self, spks_type: str) -&gt; bool:\n        \"\"\"\n        Check if the spike data type has zero baseline (nonnegative values).\n\n        Parameters\n        ----------\n        spks_type : str\n            Type of spike data to check.\n\n        Returns\n        -------\n        bool\n            True if the spks_type has zero baseline (e.g., deconvolved,\n            significant), False otherwise (e.g., raw fluorescence).\n\n        Raises\n        ------\n        ValueError\n            If spks_type is not recognized.\n        \"\"\"\n        if spks_type == \"oasis\":\n            return True\n        elif spks_type == \"deconvolved\":\n            return True\n        elif spks_type == \"raw\":\n            return False\n        elif spks_type == \"neuropil\":\n            return False\n        elif spks_type == \"significant\":\n            return True\n        elif spks_type == \"sigbase\":\n            return True\n        elif spks_type == \"sigrebase\":\n            return True\n        elif spks_type == \"corrected\":\n            return False\n        else:\n            raise ValueError(f\"spks_type {spks_type} not recognized\")\n\n    def get_spks(self, spks_type: Optional[str] = None) -&gt; np.ndarray:\n        \"\"\"\n        Get spike data for the session.\n\n        Parameters\n        ----------\n        spks_type : str, optional\n            Type of spike data to load. If None, uses params.spks_type.\n            Default is None.\n\n        Returns\n        -------\n        np.ndarray\n            Spike data array of the specified type.\n        \"\"\"\n        spks_type = spks_type or self.params.spks_type\n        return self._load_spks(spks_type)\n\n    @property\n    def spks(self) -&gt; np.ndarray:\n        \"\"\"\n        Neural spike data for the session.\n\n        Returns\n        -------\n        np.ndarray\n            Spike data array using the configured spks_type from params.\n        \"\"\"\n        return self.get_spks(self.params.spks_type)\n\n    @property\n    def spks_type(self) -&gt; str:\n        \"\"\"\n        Current spike data type.\n\n        Returns\n        -------\n        str\n            The spks_type parameter value.\n        \"\"\"\n        return self.params.spks_type\n\n    @property\n    def zero_baseline_spks(self) -&gt; bool:\n        \"\"\"\n        Whether the current spike data type has zero baseline.\n\n        Returns\n        -------\n        bool\n            True if the current spks_type has zero baseline, False otherwise.\n        \"\"\"\n        return self._are_spks_zero_baseline(self.params.spks_type)\n\n    @property\n    def timestamps(self) -&gt; np.ndarray:\n        \"\"\"\n        Imaging timestamps.\n\n        Returns\n        -------\n        np.ndarray\n            Array of timestamps for each imaging frame.\n        \"\"\"\n        return self.loadone(\"mpci.times\")\n\n    @property\n    def env_length(self) -&gt; np.ndarray:\n        \"\"\"\n        Environment length for each trial.\n\n        Returns\n        -------\n        np.ndarray\n            Array of environment lengths (room lengths) for each trial.\n\n        Notes\n        -----\n        Part of SessionToSpkmapProtocol.\n        \"\"\"\n        return self.loadone(\"trials.roomLength\")\n\n    @property\n    def positions(self) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Return the position of the mouse during the VR experiment and timestamps.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n            Tuple containing:\n            - timestamps: Array of timestamps for each position sample\n            - position: Array of positions (typically 1D position along track)\n            - trial_numbers: Array of trial numbers for each position sample\n            - idx_behave_to_frame: Array mapping behavioral samples to imaging frames\n\n        Notes\n        -----\n        Part of SessionToSpkmapProtocol.\n        \"\"\"\n        timestamps = self.loadone(\"positionTracking.times\")\n        position = self.loadone(\"positionTracking.position\")\n        idx_behave_to_frame = self.loadone(\"positionTracking.mpci\")\n        trial_start_index = self.loadone(\"trials.positionTracking\")\n        num_samples = len(position)\n        trial_numbers = np.arange(len(trial_start_index))\n        trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n        trial_numbers = np.repeat(trial_numbers, trial_lengths)\n        return timestamps, position, trial_numbers, idx_behave_to_frame\n\n    @property\n    def trial_environment(self) -&gt; np.ndarray:\n        \"\"\"\n        Environment index for each trial.\n\n        Returns\n        -------\n        np.ndarray\n            Array of environment indices for each trial.\n        \"\"\"\n        return self.loadone(\"trials.environmentIndex\")\n\n    @property\n    def environments(self) -&gt; np.ndarray:\n        \"\"\"\n        Unique environments used in the session.\n\n        Returns\n        -------\n        np.ndarray\n            Array of unique environment indices present in the session.\n        \"\"\"\n        return np.unique(self.trial_environment)\n\n    @property\n    def num_trials(self) -&gt; int:\n        \"\"\"\n        Number of trials in the session.\n\n        Returns\n        -------\n        int\n            Total number of trials in the session.\n\n        Notes\n        -----\n        Part of SessionToSpkmapProtocol.\n        \"\"\"\n        return self.get_value(\"numTrials\")\n\n    @property\n    def idx_rois(self) -&gt; np.ndarray:\n        \"\"\"\n        Boolean indices of ROIs to load based on filtering criteria.\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array of shape (n_rois,) indicating which ROIs pass all\n            filtering criteria (plane, label, fill fraction, footprint size,\n            activity, redundancy).\n        \"\"\"\n        num_rois = self.get_value(\"numROIs\")\n        idx_rois = np.ones(num_rois, dtype=bool)\n\n        # Filter ROIs by which plane they are in\n        if self.params.keep_planes is not None:\n            idx_rois &amp;= self.valid_plane_idx()\n\n        # Filter ROIs by the results of the ROICaT classifier analysis\n        if self.roicat_classifier is not None and (\n            self.params.good_label_idx is not None\n            or self.params.fraction_filled_threshold is not None\n            or self.params.footprint_size_threshold is not None\n        ):\n            valid_label, valid_fill_fraction, valid_footprint_size = self.valid_mask_idx()\n\n            if valid_label is not None:\n                idx_rois &amp;= valid_label\n\n            if valid_fill_fraction is not None:\n                idx_rois &amp;= valid_fill_fraction\n\n            if valid_footprint_size is not None:\n                idx_rois &amp;= valid_footprint_size\n\n        if self.params.exclude_silent_rois:\n            idx_rois &amp;= self.valid_activity_idx()\n\n        if self.params.exclude_redundant_rois:\n            idx_rois &amp;= self.valid_redundancy_idx()\n\n        return idx_rois\n\n    @property\n    def env_stats(self) -&gt; dict:\n        \"\"\"\n        Get the environment stats for the session.\n        \"\"\"\n        return {env: np.sum(self.trial_environment == env) for env in self.environments}\n\n    def valid_plane_idx(self) -&gt; np.ndarray:\n        \"\"\"\n        Boolean indices of ROIs in the specified planes.\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array indicating which ROIs are in the planes specified\n            by params.keep_planes. If keep_planes is None, all ROIs are valid.\n        \"\"\"\n        if self.params.keep_planes is not None:\n            plane_idx = self.get_plane_idx()\n            return np.isin(plane_idx, self.params.keep_planes)\n        else:\n            return np.ones(self.get_value(\"numROIs\"), dtype=bool)\n\n    def valid_mask_idx(self) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Filter ROIs by the results of the ROICaT classifier analysis.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray, np.ndarray]\n            Tuple containing three boolean arrays:\n            - valid_label: ROIs with acceptable classifier labels\n            - valid_fill_fraction: ROIs above fraction_filled_threshold\n            - valid_footprint_size: ROIs above footprint_size_threshold\n\n        Notes\n        -----\n        If ROICaT classifier is not available or thresholds are not set,\n        all arrays will be True (all ROIs valid).\n        \"\"\"\n        if self.roicat_classifier is not None and (\n            self.params.good_label_idx is not None\n            or self.params.fraction_filled_threshold is not None\n            or self.params.footprint_size_threshold is not None\n        ):\n            class_predictions = self.roicat_classifier[\"class_predictions\"]\n            fill_fraction = self.roicat_classifier[\"fill_fraction\"]\n            footprint_size = self.roicat_classifier[\"footprint_size\"]\n\n            if self.params.good_label_idx is not None:\n                valid_label = np.isin(class_predictions, self.params.good_label_idx)\n            else:\n                valid_label = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n\n            if self.params.fraction_filled_threshold is not None:\n                valid_fill_fraction = fill_fraction &gt; self.params.fraction_filled_threshold\n            else:\n                valid_fill_fraction = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n\n            if self.params.footprint_size_threshold is not None:\n                valid_footprint_size = footprint_size &gt; self.params.footprint_size_threshold\n            else:\n                valid_footprint_size = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n        else:\n            valid_label = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n            valid_fill_fraction = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n            valid_footprint_size = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n\n        return valid_label, valid_fill_fraction, valid_footprint_size\n\n    def valid_activity_idx(self) -&gt; np.ndarray:\n        \"\"\"\n        Filter ROIs by activity (non-silent ROIs).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array indicating which ROIs have non-zero variance.\n            If exclude_silent_rois is False, all ROIs are valid.\n        \"\"\"\n        if self.params.exclude_silent_rois:\n            valid_activity = ss.var(self.spks, axis=0) != 0\n        else:\n            valid_activity = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n        return valid_activity\n\n    def valid_redundancy_idx(self) -&gt; np.ndarray:\n        \"\"\"\n        Filter ROIs by redundancy (non-redundant ROIs).\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array indicating which ROIs are not redundant.\n            If exclude_redundant_rois is False, all ROIs are valid.\n        \"\"\"\n        if self.params.exclude_redundant_rois:\n            valid_redundancy = ~self.loadone(\"mpciROIs.redundant\")\n        else:\n            valid_redundancy = np.ones(self.get_value(\"numROIs\"), dtype=bool)\n        return valid_redundancy\n\n    def get_validity_indices(self) -&gt; dict[str, np.ndarray]:\n        \"\"\"\n        Get all validity indices for ROI filtering.\n\n        Returns\n        -------\n        dict[str, np.ndarray]\n            Dictionary containing boolean arrays for each filtering criterion:\n            - \"plane_idx\": Valid planes\n            - \"mask_idx\": Valid labels\n            - \"fill_fraction_idx\": Valid fill fractions\n            - \"footprint_size_idx\": Valid footprint sizes\n            - \"activity_idx\": Valid activity (non-silent)\n            - \"redundancy_idx\": Valid redundancy (non-redundant)\n        \"\"\"\n        valid_plane = self.valid_plane_idx()\n        valid_label, valid_fill_fraction, valid_footprint_size = self.valid_mask_idx()\n        valid_activity = self.valid_activity_idx()\n        valid_redundancy = self.valid_redundancy_idx()\n        return {\n            \"plane_idx\": valid_plane,\n            \"mask_idx\": valid_label,\n            \"fill_fraction_idx\": valid_fill_fraction,\n            \"footprint_size_idx\": valid_footprint_size,\n            \"activity_idx\": valid_activity,\n            \"redundancy_idx\": valid_redundancy,\n        }\n\n    def get_red_idx(self) -&gt; np.ndarray:\n        \"\"\"\n        Get the indices of the red ROIs.\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array indicating which ROIs are red cells.\n\n        Notes\n        -----\n        redCellIdxCoherent is a consolidated red cell index array that uses\n        tracking information to determine which cells are red in a coherent\n        manner. The roicat_support.tracking module builds this array. When not\n        available, uses the standard redCellIdx array (session wasn't tracked).\n        \"\"\"\n        if \"mpciROIs.redCellIdxCoherent\" in self.print_saved_one():\n            return self.loadone(\"mpciROIs.redCellIdxCoherent\")\n        else:\n            return self.loadone(\"mpciROIs.redCellIdx\")\n\n    def update_params(self, **kwargs) -&gt; None:\n        \"\"\"\n        Update the parameters for the session.\n\n        Parameters\n        ----------\n        **kwargs\n            The parameters to update, can be any of the parameters in\n            B2SessionParams (except for good_label_idx, which is set automatically).\n            Including:\n                - spks_type: str\n                - keep_planes: list[int]\n                - good_labels: list[str]\n                - fraction_filled_threshold: float\n                - footprint_size_threshold: int\n                - exclude_silent_rois: bool\n                - neuropil_coefficient: float\n                - exclude_redundant_rois: bool\n        \"\"\"\n        self.params.update(**kwargs)\n\n    def _init_data_path(self) -&gt; Path:\n        \"\"\"\n        Set the data path for the session.\n\n        Returns\n        -------\n        Path\n            Path to the session data directory: local_data_path / mouse_name / date / session_id.\n        \"\"\"\n        return local_data_path() / self.mouse_name / self.date / self.session_id\n\n    def _additional_loading(self) -&gt; None:\n        \"\"\"\n        Load registered experiment data.\n\n        This method loads session configuration files, preprocessing steps,\n        stored values, and ROICaT classifier results if available.\n\n        Raises\n        ------\n        ValueError\n            If session JSON files are not found (session not registered).\n        \"\"\"\n        if self._for_registration:\n            return None\n\n        super()._additional_loading()\n        if not (self.data_path / \"vrExperimentOptions.json\").exists():\n            raise ValueError(\"session json files were not found! you need to register the session first.\")\n\n        # Load options and preprocessing steps\n        self.opts = B2RegistrationOpts(**json.load(open(self.data_path / \"vrExperimentOptions.json\")))\n        self.preprocessing = json.load(open(self.data_path / \"vrExperimentPreprocessing.json\"))\n\n        # Load stored values\n        values = json.load(open(self.data_path / \"vrExperimentValues.json\"))\n        for key, val in values.items():\n            self.set_value(key, val)\n\n        # Also load ROICaT Classifier Results if they exist\n        results_path = get_classifier_results_path(self)\n        if results_path.exists():\n            self.roicat_classifier = joblib.load(results_path)\n        else:\n            self.roicat_classifier = None\n\n    def save_session_prms(self) -&gt; None:\n        \"\"\"\n        Save registered session parameters to JSON files.\n\n        Saves opts, preprocessing steps, and values to their respective\n        JSON files in the session data directory.\n        \"\"\"\n        with open(self.data_path / \"vrExperimentOptions.json\", \"w\") as file:\n            json.dump(vars(self.opts), file, ensure_ascii=False)\n        with open(self.data_path / \"vrExperimentPreprocessing.json\", \"w\") as file:\n            json.dump(self.preprocessing, file, ensure_ascii=False)\n        with open(self.data_path / \"vrExperimentValues.json\", \"w\") as file:\n            json.dump(vars(self.values), file, ensure_ascii=False, cls=NumpyEncoder)\n\n    # =============================================================================\n    # Suite2p loading functions\n    # =============================================================================\n    def loadfcorr(self, mean_adjusted: bool = True, try_from_one: bool = True) -&gt; np.ndarray:\n        \"\"\"\n        Load corrected fluorescence data.\n\n        Corrected fluorescence is computed as F - neuropil_coefficient * (Fneu - meanFneu).\n        This requires a special loading function because it isn't saved directly.\n\n        Parameters\n        ----------\n        mean_adjusted : bool, optional\n            If True, subtract mean neuropil activity per ROI. Default is True.\n        try_from_one : bool, optional\n            If True, try loading from onedata first, otherwise load from suite2p.\n            Default is True.\n\n        Returns\n        -------\n        np.ndarray\n            Corrected fluorescence array of shape (n_rois, n_frames).\n        \"\"\"\n        if try_from_one:\n            F = self.loadone(\"mpci.roiActivityF\").T\n            Fneu = self.loadone(\"mpci.roiNeuropilActivityF\").T\n        else:\n            F = self.load_s2p(\"F\")\n            Fneu = self.load_s2p(\"Fneu\")\n        meanFneu = np.mean(Fneu, axis=1, keepdims=True) if mean_adjusted else np.zeros((np.sum(self.get_value(\"roiPerPlane\")), 1))\n        neuropil_coefficient = self.params.neuropil_coefficient or self.opts.neuropilCoefficient\n        return F - neuropil_coefficient * (Fneu - meanFneu)\n\n    def load_s2p(self, varName: str, concatenate: bool = True) -&gt; Union[np.ndarray, list]:\n        \"\"\"\n        Load suite2p variable from suite2p folders.\n\n        Parameters\n        ----------\n        varName : str\n            Name of the variable to load (e.g., \"F\", \"Fneu\", \"spks\", \"ops\").\n            Must be in the available variables list.\n        concatenate : bool, optional\n            If True, concatenate data across planes. If False, return list\n            of arrays per plane. For \"ops\", concatenate is always False.\n            Default is True.\n\n        Returns\n        -------\n        np.ndarray or list\n            If concatenate=True, returns concatenated array across all planes.\n            If concatenate=False or varName=\"ops\", returns list of arrays\n            (one per plane).\n\n        Raises\n        ------\n        AssertionError\n            If varName is not available in the suite2p folders.\n        \"\"\"\n        assert varName in self.get_value(\"available\"), f\"{varName} is not available in the suite2p folders for {self.session_print()}\"\n        frame_vars = [\"F\", \"F_chan2\", \"Fneu\", \"Fneu_chan2\", \"spks\"]\n\n        if varName == \"ops\":\n            concatenate = False\n\n        var = [np.load(self.s2p_path / planeName / f\"{varName}.npy\", allow_pickle=True) for planeName in self.get_value(\"planeNames\")]\n        if varName == \"ops\":\n            var = [cvar.item() for cvar in var]\n            return var\n\n        if concatenate:\n            # if concatenation is requested, then concatenate each plane across the ROIs axis so we have just one ndarray of shape: (allROIs, allFrames)\n            if varName in frame_vars:\n                var = [v[:, : self.get_value(\"numFrames\")] for v in var]  # trim if necesary so each plane has the same number of frames\n            var = np.concatenate(var, axis=0)\n\n        return var\n\n    def get_plane_idx(self) -&gt; np.ndarray:\n        \"\"\"\n        Return the plane index for each ROI (concatenated across planes).\n\n        Returns\n        -------\n        np.ndarray\n            Array of plane indices for each ROI, with shape (n_rois,).\n            Values are uint8.\n        \"\"\"\n        planeIDs = self.get_value(\"planeIDs\")\n        roiPerPlane = self.get_value(\"roiPerPlane\")\n        return np.repeat(planeIDs, roiPerPlane).astype(np.uint8)\n\n    def get_roi_position(self, mode=\"weightedmean\"):\n        \"\"\"Return the x &amp; y positions and plane index for all ROIs.\n\n        Parameters\n        ----------\n        mode : str, optional\n            Method for calculating the position of the ROI, by default \"weightedmean\"\n            but can also use median which ignores the intensity (lam) values.\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (nROIs, 3) with columns: x-position, y-position, planeIdx\n        \"\"\"\n        planeIdx = self.get_plane_idx()\n        stat = self.load_s2p(\"stat\")\n        lam = [s[\"lam\"] for s in stat]\n        ypix = [s[\"ypix\"] for s in stat]\n        xpix = [s[\"xpix\"] for s in stat]\n        if mode == \"weightedmean\":\n            yc = np.array([np.sum(l * y) / np.sum(l) for l, y in zip(lam, ypix)])\n            xc = np.array([np.sum(l * x) / np.sum(l) for l, x in zip(lam, xpix)])\n        elif mode == \"median\":\n            yc = np.array([np.median(y) for y in ypix])\n            xc = np.array([np.median(x) for x in xpix])\n        stack_position = np.stack((xc, yc, planeIdx)).T\n        return stack_position\n\n    #\n    # =============================================================================\n    # Behavior processing functions\n    # =============================================================================\n    def get_behave_trial_idx(self, trial_start_frame: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Get the trial index for each behavioral sample.\n\n        Parameters\n        ----------\n        trial_start_frame : np.ndarray\n            Array of frame indices where each trial starts.\n\n        Returns\n        -------\n        np.ndarray\n            Array of trial indices for each behavioral sample, with shape\n            (n_behave_samples,). Values are uint64.\n        \"\"\"\n        nspt = np.array([*np.diff(trial_start_frame), self.get_value(\"numBehaveTimestamps\") - trial_start_frame[-1]])\n        return np.concatenate([tidx * np.ones(ns) for (tidx, ns) in enumerate(nspt)]).astype(np.uint64)\n\n    def group_behave_by_trial(self, data: np.ndarray, trial_start_frame: np.ndarray) -&gt; list[np.ndarray]:\n        \"\"\"\n        Group behavioral data by trial.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Behavioral data array to group, with shape (n_behave_samples, ...).\n        trial_start_frame : np.ndarray\n            Array of frame indices where each trial starts.\n\n        Returns\n        -------\n        list[np.ndarray]\n            List of arrays, one per trial, containing the behavioral data\n            for that trial.\n        \"\"\"\n        trial_index = self.get_behave_trial_idx(trial_start_frame)\n        return [data[trial_index == tidx] for tidx in range(len(trial_start_frame))]\n\n    # =============================================================================\n    # Equality and Hashing\n    # =============================================================================\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"\n        Check if two sessions are equal.\n\n        Parameters\n        ----------\n        other : Any\n            Object to compare with.\n\n        Returns\n        -------\n        bool\n            True if other is a B2Session with the same session_name, False otherwise.\n        \"\"\"\n        if not isinstance(other, B2Session):\n            return False\n        if hash(self) == hash(other):\n            return True\n        else:\n            return False\n\n    def __hash__(self) -&gt; int:\n        \"\"\"\n        Hash the session based on session name.\n\n        Returns\n        -------\n        int\n            Hash value based on the session_name tuple.\n        \"\"\"\n        return hash(self.session_name)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Custom repr that excludes spks_types class variable.\n        \"\"\"\n        return f\"B2Session(mouse_name='{self.mouse_name}', date='{self.date}', session_id='{self.session_id}', spks_type='{self.params.spks_type}')\"\n\n    def to_old_session(self):\n        \"\"\"\n        Convert B2Session to old vrExperiment format for compatibility with legacy code.\n\n        This method creates a vrExperiment object that can be used with legacy analysis\n        code (e.g., placeCellSingleSession). The old format requires JSON files to exist\n        from the old registration system. If these files don't exist, this will raise\n        an AssertionError.\n\n        Returns\n        -------\n        vrExperiment\n            A vrExperiment object that wraps this B2Session, providing the old API\n            interface for compatibility with legacy analysis code.\n\n        Raises\n        ------\n        AssertionError\n            If the session folder doesn't exist or if the required JSON files from\n            the old registration system are not found.\n        \"\"\"\n        from _old_vrAnalysis.session import vrExperiment\n\n        # Create a vrExperiment object using the 3-string constructor\n        # This will load from JSON files (vrExperimentOptions.json, etc.)\n        # which must exist from the old registration system\n        vrexp = vrExperiment(self.mouse_name, self.date, self.session_id)\n\n        return vrexp\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.spks","title":"<code>spks</code>  <code>property</code>","text":"<p>Neural spike data for the session.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Spike data array using the configured spks_type from params.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.timestamps","title":"<code>timestamps</code>  <code>property</code>","text":"<p>Imaging timestamps.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of timestamps for each imaging frame.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.positions","title":"<code>positions</code>  <code>property</code>","text":"<p>Return the position of the mouse during the VR experiment and timestamps.</p> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>Tuple containing: - timestamps: Array of timestamps for each position sample - position: Array of positions (typically 1D position along track) - trial_numbers: Array of trial numbers for each position sample - idx_behave_to_frame: Array mapping behavioral samples to imaging frames</p> Notes <p>Part of SessionToSpkmapProtocol.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.trial_environment","title":"<code>trial_environment</code>  <code>property</code>","text":"<p>Environment index for each trial.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of environment indices for each trial.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.environments","title":"<code>environments</code>  <code>property</code>","text":"<p>Unique environments used in the session.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of unique environment indices present in the session.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.num_trials","title":"<code>num_trials</code>  <code>property</code>","text":"<p>Number of trials in the session.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total number of trials in the session.</p> Notes <p>Part of SessionToSpkmapProtocol.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.idx_rois","title":"<code>idx_rois</code>  <code>property</code>","text":"<p>Boolean indices of ROIs to load based on filtering criteria.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array of shape (n_rois,) indicating which ROIs pass all filtering criteria (plane, label, fill fraction, footprint size, activity, redundancy).</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.__init__","title":"<code>__init__(mouse_name, date, session_id=None, params=B2SessionParams(), _for_registration=False, spks_types=('oasis', 'deconvolved', 'raw', 'neuropil', 'significant', 'corrected', 'sigbase', 'sigrebase'))</code>","text":""},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.get_spks","title":"<code>get_spks(spks_type=None)</code>","text":"<p>Get spike data for the session.</p> <p>Parameters:</p> Name Type Description Default <code>spks_type</code> <code>str</code> <p>Type of spike data to load. If None, uses params.spks_type. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Spike data array of the specified type.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def get_spks(self, spks_type: Optional[str] = None) -&gt; np.ndarray:\n    \"\"\"\n    Get spike data for the session.\n\n    Parameters\n    ----------\n    spks_type : str, optional\n        Type of spike data to load. If None, uses params.spks_type.\n        Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        Spike data array of the specified type.\n    \"\"\"\n    spks_type = spks_type or self.params.spks_type\n    return self._load_spks(spks_type)\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.update_params","title":"<code>update_params(**kwargs)</code>","text":"<p>Update the parameters for the session.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The parameters to update, can be any of the parameters in B2SessionParams (except for good_label_idx, which is set automatically). Including:     - spks_type: str     - keep_planes: list[int]     - good_labels: list[str]     - fraction_filled_threshold: float     - footprint_size_threshold: int     - exclude_silent_rois: bool     - neuropil_coefficient: float     - exclude_redundant_rois: bool</p> <code>{}</code> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def update_params(self, **kwargs) -&gt; None:\n    \"\"\"\n    Update the parameters for the session.\n\n    Parameters\n    ----------\n    **kwargs\n        The parameters to update, can be any of the parameters in\n        B2SessionParams (except for good_label_idx, which is set automatically).\n        Including:\n            - spks_type: str\n            - keep_planes: list[int]\n            - good_labels: list[str]\n            - fraction_filled_threshold: float\n            - footprint_size_threshold: int\n            - exclude_silent_rois: bool\n            - neuropil_coefficient: float\n            - exclude_redundant_rois: bool\n    \"\"\"\n    self.params.update(**kwargs)\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.load_s2p","title":"<code>load_s2p(varName, concatenate=True)</code>","text":"<p>Load suite2p variable from suite2p folders.</p> <p>Parameters:</p> Name Type Description Default <code>varName</code> <code>str</code> <p>Name of the variable to load (e.g., \"F\", \"Fneu\", \"spks\", \"ops\"). Must be in the available variables list.</p> required <code>concatenate</code> <code>bool</code> <p>If True, concatenate data across planes. If False, return list of arrays per plane. For \"ops\", concatenate is always False. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray or list</code> <p>If concatenate=True, returns concatenated array across all planes. If concatenate=False or varName=\"ops\", returns list of arrays (one per plane).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If varName is not available in the suite2p folders.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def load_s2p(self, varName: str, concatenate: bool = True) -&gt; Union[np.ndarray, list]:\n    \"\"\"\n    Load suite2p variable from suite2p folders.\n\n    Parameters\n    ----------\n    varName : str\n        Name of the variable to load (e.g., \"F\", \"Fneu\", \"spks\", \"ops\").\n        Must be in the available variables list.\n    concatenate : bool, optional\n        If True, concatenate data across planes. If False, return list\n        of arrays per plane. For \"ops\", concatenate is always False.\n        Default is True.\n\n    Returns\n    -------\n    np.ndarray or list\n        If concatenate=True, returns concatenated array across all planes.\n        If concatenate=False or varName=\"ops\", returns list of arrays\n        (one per plane).\n\n    Raises\n    ------\n    AssertionError\n        If varName is not available in the suite2p folders.\n    \"\"\"\n    assert varName in self.get_value(\"available\"), f\"{varName} is not available in the suite2p folders for {self.session_print()}\"\n    frame_vars = [\"F\", \"F_chan2\", \"Fneu\", \"Fneu_chan2\", \"spks\"]\n\n    if varName == \"ops\":\n        concatenate = False\n\n    var = [np.load(self.s2p_path / planeName / f\"{varName}.npy\", allow_pickle=True) for planeName in self.get_value(\"planeNames\")]\n    if varName == \"ops\":\n        var = [cvar.item() for cvar in var]\n        return var\n\n    if concatenate:\n        # if concatenation is requested, then concatenate each plane across the ROIs axis so we have just one ndarray of shape: (allROIs, allFrames)\n        if varName in frame_vars:\n            var = [v[:, : self.get_value(\"numFrames\")] for v in var]  # trim if necesary so each plane has the same number of frames\n        var = np.concatenate(var, axis=0)\n\n    return var\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.get_roi_position","title":"<code>get_roi_position(mode='weightedmean')</code>","text":"<p>Return the x &amp; y positions and plane index for all ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Method for calculating the position of the ROI, by default \"weightedmean\" but can also use median which ignores the intensity (lam) values.</p> <code>'weightedmean'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (nROIs, 3) with columns: x-position, y-position, planeIdx</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def get_roi_position(self, mode=\"weightedmean\"):\n    \"\"\"Return the x &amp; y positions and plane index for all ROIs.\n\n    Parameters\n    ----------\n    mode : str, optional\n        Method for calculating the position of the ROI, by default \"weightedmean\"\n        but can also use median which ignores the intensity (lam) values.\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (nROIs, 3) with columns: x-position, y-position, planeIdx\n    \"\"\"\n    planeIdx = self.get_plane_idx()\n    stat = self.load_s2p(\"stat\")\n    lam = [s[\"lam\"] for s in stat]\n    ypix = [s[\"ypix\"] for s in stat]\n    xpix = [s[\"xpix\"] for s in stat]\n    if mode == \"weightedmean\":\n        yc = np.array([np.sum(l * y) / np.sum(l) for l, y in zip(lam, ypix)])\n        xc = np.array([np.sum(l * x) / np.sum(l) for l, x in zip(lam, xpix)])\n    elif mode == \"median\":\n        yc = np.array([np.median(y) for y in ypix])\n        xc = np.array([np.median(x) for x in xpix])\n    stack_position = np.stack((xc, yc, planeIdx)).T\n    return stack_position\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.get_validity_indices","title":"<code>get_validity_indices()</code>","text":"<p>Get all validity indices for ROI filtering.</p> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>Dictionary containing boolean arrays for each filtering criterion: - \"plane_idx\": Valid planes - \"mask_idx\": Valid labels - \"fill_fraction_idx\": Valid fill fractions - \"footprint_size_idx\": Valid footprint sizes - \"activity_idx\": Valid activity (non-silent) - \"redundancy_idx\": Valid redundancy (non-redundant)</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def get_validity_indices(self) -&gt; dict[str, np.ndarray]:\n    \"\"\"\n    Get all validity indices for ROI filtering.\n\n    Returns\n    -------\n    dict[str, np.ndarray]\n        Dictionary containing boolean arrays for each filtering criterion:\n        - \"plane_idx\": Valid planes\n        - \"mask_idx\": Valid labels\n        - \"fill_fraction_idx\": Valid fill fractions\n        - \"footprint_size_idx\": Valid footprint sizes\n        - \"activity_idx\": Valid activity (non-silent)\n        - \"redundancy_idx\": Valid redundancy (non-redundant)\n    \"\"\"\n    valid_plane = self.valid_plane_idx()\n    valid_label, valid_fill_fraction, valid_footprint_size = self.valid_mask_idx()\n    valid_activity = self.valid_activity_idx()\n    valid_redundancy = self.valid_redundancy_idx()\n    return {\n        \"plane_idx\": valid_plane,\n        \"mask_idx\": valid_label,\n        \"fill_fraction_idx\": valid_fill_fraction,\n        \"footprint_size_idx\": valid_footprint_size,\n        \"activity_idx\": valid_activity,\n        \"redundancy_idx\": valid_redundancy,\n    }\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2Session.get_red_idx","title":"<code>get_red_idx()</code>","text":"<p>Get the indices of the red ROIs.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array indicating which ROIs are red cells.</p> Notes <p>redCellIdxCoherent is a consolidated red cell index array that uses tracking information to determine which cells are red in a coherent manner. The roicat_support.tracking module builds this array. When not available, uses the standard redCellIdx array (session wasn't tracked).</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def get_red_idx(self) -&gt; np.ndarray:\n    \"\"\"\n    Get the indices of the red ROIs.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array indicating which ROIs are red cells.\n\n    Notes\n    -----\n    redCellIdxCoherent is a consolidated red cell index array that uses\n    tracking information to determine which cells are red in a coherent\n    manner. The roicat_support.tracking module builds this array. When not\n    available, uses the standard redCellIdx array (session wasn't tracked).\n    \"\"\"\n    if \"mpciROIs.redCellIdxCoherent\" in self.print_saved_one():\n        return self.loadone(\"mpciROIs.redCellIdxCoherent\")\n    else:\n        return self.loadone(\"mpciROIs.redCellIdx\")\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2SessionParams","title":"<code>B2SessionParams</code>  <code>dataclass</code>","text":"<p>Parameters for configuring B2Session data loading and filtering.</p> <p>Attributes:</p> Name Type Description <code>spks_type</code> <code>(str, optional)</code> <p>Type of spike data to load. Options: \"oasis\", \"deconvolved\", \"raw\", \"neuropil\", \"significant\", \"sigbase\", \"corrected\". Default is \"significant\".</p> <code>keep_planes</code> <code>(list[int], optional)</code> <p>List of plane indices to keep. If None, all planes are kept. Default is None.</p> <code>good_labels</code> <code>(list[str], optional)</code> <p>List of ROICaT classifier labels to keep. Default is [\"c\", \"d\"].</p> <code>fraction_filled_threshold</code> <code>(float, optional)</code> <p>Minimum fraction filled threshold for ROI filtering based on local concavity analysis. If None, no filtering is applied. Default is None.</p> <code>footprint_size_threshold</code> <code>(int, optional)</code> <p>Minimum footprint size threshold for ROI filtering. If None, no filtering is applied. Default is None.</p> <code>exclude_silent_rois</code> <code>(bool, optional)</code> <p>If True, exclude ROIs with zero variance. Default is True.</p> <code>neuropil_coefficient</code> <code>(float, optional)</code> <p>Coefficient for neuropil subtraction when computing corrected fluorescence. If None, uses value from B2RegistrationOpts. Default is None.</p> <code>exclude_redundant_rois</code> <code>(bool, optional)</code> <p>If True, exclude redundant ROIs based on clustering analysis. Default is True.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>@dataclass\nclass B2SessionParams:\n    \"\"\"\n    Parameters for configuring B2Session data loading and filtering.\n\n    Attributes\n    ----------\n    spks_type : str, optional\n        Type of spike data to load. Options: \"oasis\", \"deconvolved\", \"raw\",\n        \"neuropil\", \"significant\", \"sigbase\", \"corrected\". Default is \"significant\".\n    keep_planes : list[int], optional\n        List of plane indices to keep. If None, all planes are kept.\n        Default is None.\n    good_labels : list[str], optional\n        List of ROICaT classifier labels to keep. Default is [\"c\", \"d\"].\n    fraction_filled_threshold : float, optional\n        Minimum fraction filled threshold for ROI filtering based on local\n        concavity analysis. If None, no filtering is applied. Default is None.\n    footprint_size_threshold : int, optional\n        Minimum footprint size threshold for ROI filtering. If None, no\n        filtering is applied. Default is None.\n    exclude_silent_rois : bool, optional\n        If True, exclude ROIs with zero variance. Default is True.\n    neuropil_coefficient : float, optional\n        Coefficient for neuropil subtraction when computing corrected\n        fluorescence. If None, uses value from B2RegistrationOpts.\n        Default is None.\n    exclude_redundant_rois : bool, optional\n        If True, exclude redundant ROIs based on clustering analysis.\n        Default is True.\n    \"\"\"\n\n    spks_type: \"SpksTypes\" = \"significant\"\n    keep_planes: list[int] | None = None\n    good_labels: list[str] = field(default_factory=lambda: [\"c\", \"d\"])\n    fraction_filled_threshold: float | None = None\n    footprint_size_threshold: int | None = None\n    exclude_silent_rois: bool = True\n    neuropil_coefficient: float | None = None\n    exclude_redundant_rois: bool = True\n\n    @classmethod\n    def from_dict(cls, params: Dict[str, Any]) -&gt; \"B2SessionParams\":\n        \"\"\"\n        Create B2SessionParams from a dictionary.\n\n        Parameters\n        ----------\n        params : dict[str, Any]\n            Dictionary containing parameter values.\n\n        Returns\n        -------\n        B2SessionParams\n            B2SessionParams instance created from the dictionary.\n        \"\"\"\n        return cls(**params)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Post-initialization method to load classifier label mapping.\n\n        This method is called automatically after dataclass initialization.\n        It loads the ROICaT classifier to get the label_to_id mapping for\n        validating good_labels.\n        \"\"\"\n        classifier = load_classifier()\n        self._label_to_id = classifier[\"label_to_id\"]\n\n    def update(self, **kwargs) -&gt; None:\n        \"\"\"\n        Update the parameters for the session.\n\n        Parameters\n        ----------\n        **kwargs\n            Parameter names and values to update. Can include any parameter\n            from B2SessionParams. Special handling for \"good_labels\" which\n            validates against the classifier.\n        \"\"\"\n        for key, val in kwargs.items():\n            if key == \"good_labels\":\n                self.set_good_labels(val)\n            else:\n                setattr(self, key, val)\n\n    @property\n    def good_label_idx(self) -&gt; list[int] | None:\n        \"\"\"\n        Get the good label indices for the session.\n\n        Returns\n        -------\n        list[int] or None\n            List of label indices corresponding to good_labels, or None if\n            good_labels is None.\n        \"\"\"\n        if self.good_labels is None:\n            return None\n        else:\n            return [self._label_to_id[label] for label in self.good_labels]\n\n    def set_good_labels(self, good_labels: list[str] | None) -&gt; list[str]:\n        \"\"\"\n        Set the good labels for the session.\n\n        Parameters\n        ----------\n        good_labels : list[str] or None\n            List of ROICaT classifier labels to keep. If None, all labels\n            are kept.\n\n        Returns\n        -------\n        list[str]\n            The set good_labels list.\n\n        Raises\n        ------\n        ValueError\n            If any label in good_labels is not found in the classifier.\n        \"\"\"\n        if good_labels is None:\n            self.good_labels = None\n        else:\n            if any(label not in self._label_to_id for label in good_labels):\n                raise ValueError(f\"Not all labels in good_labels are found in the classifier: {good_labels}\")\n            self.good_labels = good_labels\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2SessionParams.good_label_idx","title":"<code>good_label_idx</code>  <code>property</code>","text":"<p>Get the good label indices for the session.</p> <p>Returns:</p> Type Description <code>list[int] or None</code> <p>List of label indices corresponding to good_labels, or None if good_labels is None.</p>"},{"location":"api/b2session/#vrAnalysis.sessions.B2SessionParams.from_dict","title":"<code>from_dict(params)</code>  <code>classmethod</code>","text":"<p>Create B2SessionParams from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, Any]</code> <p>Dictionary containing parameter values.</p> required <p>Returns:</p> Type Description <code>B2SessionParams</code> <p>B2SessionParams instance created from the dictionary.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>@classmethod\ndef from_dict(cls, params: Dict[str, Any]) -&gt; \"B2SessionParams\":\n    \"\"\"\n    Create B2SessionParams from a dictionary.\n\n    Parameters\n    ----------\n    params : dict[str, Any]\n        Dictionary containing parameter values.\n\n    Returns\n    -------\n    B2SessionParams\n        B2SessionParams instance created from the dictionary.\n    \"\"\"\n    return cls(**params)\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2SessionParams.update","title":"<code>update(**kwargs)</code>","text":"<p>Update the parameters for the session.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Parameter names and values to update. Can include any parameter from B2SessionParams. Special handling for \"good_labels\" which validates against the classifier.</p> <code>{}</code> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def update(self, **kwargs) -&gt; None:\n    \"\"\"\n    Update the parameters for the session.\n\n    Parameters\n    ----------\n    **kwargs\n        Parameter names and values to update. Can include any parameter\n        from B2SessionParams. Special handling for \"good_labels\" which\n        validates against the classifier.\n    \"\"\"\n    for key, val in kwargs.items():\n        if key == \"good_labels\":\n            self.set_good_labels(val)\n        else:\n            setattr(self, key, val)\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.B2SessionParams.set_good_labels","title":"<code>set_good_labels(good_labels)</code>","text":"<p>Set the good labels for the session.</p> <p>Parameters:</p> Name Type Description Default <code>good_labels</code> <code>list[str] or None</code> <p>List of ROICaT classifier labels to keep. If None, all labels are kept.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>The set good_labels list.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any label in good_labels is not found in the classifier.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>def set_good_labels(self, good_labels: list[str] | None) -&gt; list[str]:\n    \"\"\"\n    Set the good labels for the session.\n\n    Parameters\n    ----------\n    good_labels : list[str] or None\n        List of ROICaT classifier labels to keep. If None, all labels\n        are kept.\n\n    Returns\n    -------\n    list[str]\n        The set good_labels list.\n\n    Raises\n    ------\n    ValueError\n        If any label in good_labels is not found in the classifier.\n    \"\"\"\n    if good_labels is None:\n        self.good_labels = None\n    else:\n        if any(label not in self._label_to_id for label in good_labels):\n            raise ValueError(f\"Not all labels in good_labels are found in the classifier: {good_labels}\")\n        self.good_labels = good_labels\n</code></pre>"},{"location":"api/b2session/#vrAnalysis.sessions.b2session.B2RegistrationOpts","title":"<code>B2RegistrationOpts</code>  <code>dataclass</code>","text":"<p>Options for B2 session registration.</p> <p>Attributes:</p> Name Type Description <code>vrBehaviorVersion</code> <code>(int, optional)</code> <p>Version of VR behavior data format. Default is 1.</p> <code>facecam</code> <code>(bool, optional)</code> <p>Whether facecam data is included. Default is False.</p> <code>imaging</code> <code>(bool, optional)</code> <p>Whether imaging data is included. Default is True.</p> <code>oasis</code> <code>(bool, optional)</code> <p>Whether OASIS deconvolution was performed. Default is True.</p> <code>redCellProcessing</code> <code>(bool, optional)</code> <p>Whether red cell processing was performed. Default is True.</p> <code>clearOne</code> <code>(bool, optional)</code> <p>Whether to clear onedata during registration. Default is True.</p> <code>neuropilCoefficient</code> <code>(float, optional)</code> <p>Coefficient for neuropil subtraction. Default is 0.7.</p> <code>tau</code> <code>(float, optional)</code> <p>Time constant for deconvolution. Default is 1.5.</p> <code>fs</code> <code>(int, optional)</code> <p>Sampling frequency in Hz. Default is 6.</p> Source code in <code>vrAnalysis/sessions/b2session.py</code> <pre><code>@dataclass\nclass B2RegistrationOpts:\n    \"\"\"\n    Options for B2 session registration.\n\n    Attributes\n    ----------\n    vrBehaviorVersion : int, optional\n        Version of VR behavior data format. Default is 1.\n    facecam : bool, optional\n        Whether facecam data is included. Default is False.\n    imaging : bool, optional\n        Whether imaging data is included. Default is True.\n    oasis : bool, optional\n        Whether OASIS deconvolution was performed. Default is True.\n    redCellProcessing : bool, optional\n        Whether red cell processing was performed. Default is True.\n    clearOne : bool, optional\n        Whether to clear onedata during registration. Default is True.\n    neuropilCoefficient : float, optional\n        Coefficient for neuropil subtraction. Default is 0.7.\n    tau : float, optional\n        Time constant for deconvolution. Default is 1.5.\n    fs : int, optional\n        Sampling frequency in Hz. Default is 6.\n    \"\"\"\n\n    vrBehaviorVersion: int = 1\n    facecam: bool = False\n    imaging: bool = True\n    oasis: bool = True\n    redCellProcessing: bool = True\n    clearOne: bool = True\n    neuropilCoefficient: float = 0.7\n    tau: float = 1.5\n    fs: int = 6\n    moveRawData: bool = field(\n        default=False,\n        metadata={\n            \"deprecated\": True,\n            \"note\": \"For backwards compatibility only; this option is never used in current code.\",\n        },\n    )\n</code></pre>"},{"location":"api/database/","title":"Database API Reference","text":""},{"location":"api/database/#vrAnalysis.database","title":"<code>database</code>","text":"<p>Database management module for VR analysis sessions.</p> <p>This module provides classes and functions for interacting with Microsoft Access databases used to track VR session data. It includes base database functionality and specialized session database management with support for registration workflows, suite2p processing tracking, and quality control operations.</p>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase","title":"<code>BaseDatabase</code>","text":"Source code in <code>vrAnalysis/database.py</code> <pre><code>class BaseDatabase:\n    def __init__(self, db_name: str):\n        \"\"\"\n        Initialize a new database instance.\n\n        This constructor initializes a new instance of the BaseDatabase class. It sets the default\n        values for the table name, database name, and database path. It is built to work with\n        the Microsoft Access application; however, a few small changes can make it compatible with\n        other SQL-based database systems.\n\n        Parameters\n        ----------\n        db_name : str, required\n            The name of the database to access.\n\n        Example\n        -------\n        &gt;&gt;&gt; db = BaseDatabase('vrSessions')\n        &gt;&gt;&gt; print(vrdb.table_name)\n        'sessiondb'\n        &gt;&gt;&gt; print(vrdb.db_name)\n        'vrDatabase'\n\n        Notes\n        -----\n        - This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.\n        - If you are using this on a new system, then you should edit your path, database name, and default table in that function.\n        \"\"\"\n\n        metadata = get_database_metadata(db_name)\n        self.db_path = metadata[\"db_path\"]\n        self.db_name = metadata[\"db_name\"]\n        self.db_ext = metadata[\"db_ext\"]\n        self.table_name = metadata[\"table_name\"]\n        self.uid = metadata[\"uid\"]\n        self.backup_path = metadata[\"backup_path\"]\n        self.host_type = host_types[self.db_ext]\n        self.unique_fields = self.process_unique_fields(metadata[\"unique_fields\"])\n        self.default_conditions = metadata[\"default_conditions\"]\n\n    def process_unique_fields(self, fields: List[Union[str, Tuple[str, type]]]) -&gt; List[Tuple[str, type]]:\n        \"\"\"\n        Process and validate unique field definitions.\n\n        Converts unique field specifications into a standardized format where each\n        field is a tuple of (field_name, field_type). String fields can be specified\n        as just the name and will default to str type.\n\n        Parameters\n        ----------\n        fields : list\n            List of field specifications. Each can be:\n            - A string (field name, defaults to str type)\n            - A tuple of (field_name, type) where type is a Python type class\n\n        Returns\n        -------\n        list\n            List of tuples, each containing (field_name, field_type).\n\n        Raises\n        ------\n        ValueError\n            If a field specification is not a string or a valid (string, type) tuple.\n        \"\"\"\n        ufields = []\n        for f in fields:\n            if isinstance(f, tuple) and len(f) == 2 and type(f[1]) == type and isinstance(f[0], str):\n                ufields.append(f)\n            elif isinstance(f, str):\n                ufields.append((f, str))\n            else:\n                raise ValueError(f\"unique field {f} must be a string or a string-type tuple\")\n        return ufields\n\n    def get_dbfile(self) -&gt; Path:\n        \"\"\"\n        Get the full path to the database file.\n\n        Returns\n        -------\n        Path\n            Path object pointing to the database file.\n        \"\"\"\n        return Path(self.db_path) / (self.db_name + self.db_ext)\n\n    def save_backup(self, return_out: bool = False) -&gt; Optional[CompletedProcess]:\n        \"\"\"Save a backup of the database to the backup path specified in metadata.\n\n        Parameters\n        ----------\n        return_out : bool, optional\n            If True, return the output of the robocopy command.\n\n        Returns\n        -------\n        CompletedProcess or None\n            CompletedProcess object from the robocopy command (if return_out is True).\n            Returns None if return_out is False.\n        \"\"\"\n        source_path = self.db_path\n        target_path = self.backup_path\n        source_file = self.db_name + self.db_ext\n        robocopy_arguments = f\"robocopy {source_path} {target_path} {source_file}\"\n        outs = run(robocopy_arguments, capture_output=True, text=True)\n        if return_out:\n            return outs\n\n    def connect(self) -&gt; pyodbc.Connection:\n        \"\"\"\n        Establish a connection to the database.\n\n        Creates a pyodbc connection using the appropriate driver string based on\n        the database file extension. Currently configured for Microsoft Access\n        databases (.accdb, .mdb files).\n\n        Returns\n        -------\n        pyodbc.Connection\n            Database connection object.\n\n        Raises\n        ------\n        AssertionError\n            If the host type for the database extension is not supported.\n\n        Notes\n        -----\n        To support additional database types:\n        1. Determine the appropriate pyodbc driver string for your database\n        2. Add it to the driver_string dictionary in this method\n        3. Update the host_types dictionary at the module level to map your\n           file extension to the driver key\n\n        See https://www.connectionstrings.com/ for driver string examples.\n        \"\"\"\n        driver_string = {\"access\": r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\" + rf\"DBQ={self.get_dbfile()};\"}\n\n        # Make sure connections are possible for this hosttype\n        failure_message = (\n            f\"Requested host_type ({self.host_type}) is not available. The only ones that are coded are: {[k for k in driver_string.keys()]}\\n\\n\"\n            f\"For support with writing a driver string for a different host, use the fantastic website: https://www.connectionstrings.com/\"\n        )\n        assert self.host_type in driver_string, failure_message\n\n        # Return a connection to the database\n        return pyodbc.connect(driver_string[self.host_type])\n\n    @contextmanager\n    def open_cursor(self, commit_changes: bool = False) -&gt; Generator[pyodbc.Cursor, None, None]:\n        \"\"\"\n        Context manager to open a database cursor and manage connections.\n\n        This context manager provides a convenient way to open a cursor to the database,\n        perform database operations, and manage connections. It also allows you to\n        commit changes if needed.\n\n        Parameters\n        ----------\n        commit_changes : bool, optional\n            Whether to commit changes to the database. Default is False.\n\n        Yields\n        ------\n        pyodbc.Cursor\n            A database cursor for executing SQL queries.\n\n        Raises\n        ------\n        Exception\n            If an error occurs while connecting to the database.\n\n        Example\n        -------\n        Use the context manager to perform database operations:\n\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     cursor.execute(\"SELECT * FROM your_table\")\n\n        \"\"\"\n        try:\n            # Attempt to open a cursor to the database\n            conn = self.connect()\n            cursor = conn.cursor()\n            yield cursor\n        except Exception as ex:\n            print(f\"An exception occurred while trying to connect to {self.db_name}!\")\n            print(ex)\n            raise ex\n        else:\n            # if no exception was raised, commit changes\n            if commit_changes:\n                conn.commit()\n        finally:\n            # Always close the cursor and connection\n            cursor.close()\n            conn.close()\n\n    # == display meta data for database ==\n    def show_metadata(self) -&gt; None:\n        \"\"\"\n        Display metadata associated with the open database.\n\n        Prints information about the database location, name, table, unique ID field,\n        backup path, and default filtering conditions.\n        \"\"\"\n        print(f\"{self.host_type} database located at {self.db_path}\")\n        print(f\"Database name: {self.db_name}{self.db_ext}, table name: {self.table_name}, with uid: {self.uid}\")\n        if self.backup_path is not None:\n            print(f\"Backup path located at: {self.backup_path}\")\n        else:\n            print(f\"No backup path specified...\")\n        if self.default_conditions:\n            print(f\"Default database filters:\")\n            for key, val in self.default_conditions.items():\n                print(\"  \", self.construct_filter_string(key, self.process_filter_value(val)))\n        else:\n            print(f\"No default filters.\")\n\n    def table_column_info(self) -&gt; Tuple[List[str], List[str], List[bool]]:\n        \"\"\"\n        Retrieve the column names, data types, and nullable status of the table.\n\n        Returns\n        -------\n        tuple\n            A tuple containing three elements:\n            - A list of strings representing the column names of the table.\n            - A list of strings representing the data types of the table.\n            - A list of booleans representing the nullable status of the table.\n        \"\"\"\n        with self.open_cursor(commit_changes=False) as cursor:\n            query = f\"SELECT * FROM {self.table_name} WHERE 1=0\"\n            cursor.execute(query)\n            column_descriptions = cursor.description\n        column_name, data_type, _, _, _, _, nullable = map(list, zip(*column_descriptions))\n        return column_name, data_type, nullable\n\n    # == retrieve table data ==\n    def table_data(self) -&gt; Tuple[List[str], List[Tuple[Any, ...]]]:\n        \"\"\"\n        Retrieve data and field names from the specified table.\n\n        This method retrieves the field names and table elements from the table specified\n        in the `BaseDatabase` instance.\n\n        Returns\n        -------\n        tuple\n            A tuple containing two elements:\n            - A list of strings representing the field names of the table.\n            - A list of tuples representing the data rows of the table.\n        \"\"\"\n        with self.open_cursor() as cursor:\n            field_names = [col.column_name for col in cursor.columns(table=self.table_name)]\n            cursor.execute(f\"SELECT * FROM {self.table_name}\")\n            table_elements = cursor.fetchall()\n\n        return field_names, table_elements\n\n    def get_table(self, use_default: bool = True, **kw_conditions: Any) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieve data from table in database and return as dataframe with optional filtering.\n\n        This method retrieves all data from the primary table in the database specified in\n        BaseDatabase instance. It automatically filters the data using the defaultConditions\n        defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is\n        a conflict.\n\n        Parameters\n        ----------\n        use_default : bool, default=True\n            Use default conditions if true, if False ignore them\n        **kw_conditions : dict, optional\n            Additional filtering conditions as keyword arguments.\n            Each condition should match a column name in the table.\n            Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair.\n            The operation defaults to '==', but you can use anything that works as a df query.\n\n            Examples:\n                - Simple equality: ``imaging=True`` filters where imaging column equals True\n                - Comparison operators: ``sessionID=(5, '&gt;')`` filters where sessionID &gt; 5\n                - Multiple conditions: ``imaging=True, mouseName='ATL028'`` applies AND logic\n\n            Note: this is limited in the sense that empty data can't be identified with key:None.\n            (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())\n\n        Returns\n        -------\n        df : pandas dataframe\n            A dataframe containing the filtered data from the primary database table.\n\n        Example\n        -------\n        &gt;&gt;&gt; vrdb = YourDatabaseClass()\n        &gt;&gt;&gt; df = vrdb.get_table(imaging=True)\n        &gt;&gt;&gt; df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))\n        \"\"\"\n\n        field_names, table_data = self.table_data()\n        df = pd.DataFrame.from_records(table_data, columns=field_names)\n        conditions = copy(self.default_conditions) if use_default else {}\n        conditions.update(kw_conditions)\n        if conditions:\n            for key, val in conditions.items():\n                assert key in field_names, f\"{key} is not a column name in {self.table_name}\"\n                conditions[key] = self.process_filter_value(val)  # make sure it's a value/operation pair\n            query = \" &amp; \".join([self.construct_filter_string(key, val_op_tuple) for key, val_op_tuple in conditions.items()])\n            df = df.query(query)\n        return df\n\n    def process_filter_value(self, val: Union[Any, Tuple[Any, str]]) -&gt; Tuple[Any, str]:\n        \"\"\"\n        Ensure filter value has an operation associated with it.\n\n        Filters are passed to pandas DataFrame queries as {key}{operation}{value}.\n        This method ensures each value is a tuple of (value, operation), defaulting\n        to '==' if no operation is specified.\n\n        Parameters\n        ----------\n        val : any or tuple\n            Filter value. If a tuple, should be (value, operation). If not a tuple,\n            will be converted to (val, '==').\n\n        Returns\n        -------\n        tuple\n            Tuple of (value, operation) for use in DataFrame queries.\n        \"\"\"\n        if not isinstance(val, tuple):\n            val = (val, \"==\")\n        return val\n\n    def construct_filter_string(self, key: str, val_op_tuple: Tuple[Any, str]) -&gt; str:\n        \"\"\"\n        Construct a string to be used as a pandas DataFrame query expression.\n\n        Parameters\n        ----------\n        key : str\n            Column name to filter on.\n        val_op_tuple : tuple\n            Tuple of (value, operation) where operation is a comparison operator\n            (e.g., '==', '!=', '&gt;', '&lt;').\n\n        Returns\n        -------\n        str\n            Query string in the format `column_name`operator'value'.\n        \"\"\"\n        val, op = val_op_tuple\n        return f\"`{key}`{op}{val!r}\"\n\n    # == methods for adding records and updating information to the database ==\n    def create_update_statement(self, field: str, uid: Any) -&gt; str:\n        \"\"\"\n        Create an SQL UPDATE statement for a single field and record.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n        uid : any\n            Unique identifier value for the record to update.\n\n        Returns\n        -------\n        str\n            SQL UPDATE statement with parameter placeholder.\n\n        Example\n        -------\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)\n        \"\"\"\n        return f\"UPDATE {self.table_name} set {field} = ? WHERE {self.uid} = {uid}\"\n\n    def create_update_many_statement(self, field: str) -&gt; str:\n        \"\"\"\n        Create an SQL UPDATE statement for batch updating a single field.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n\n        Returns\n        -------\n        str\n            SQL UPDATE statement with parameter placeholders for value and uid.\n\n        Example\n        -------\n        &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n        ...     stmt = self.create_update_many_statement(\"fieldName\")\n        ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])\n        \"\"\"\n        return f\"UPDATE {self.table_name} set {field} = ? where {self.uid} = ?\"\n\n    def update_database_field(self, field: str, val: Any, **kw_conditions: Any) -&gt; None:\n        \"\"\"\n        Update a database field for all records matching specified conditions.\n\n        Parameters\n        ----------\n        field : str\n            Name of the field to update.\n        val : any\n            Value to set for the field.\n        **kw_conditions : dict, optional\n            Filtering conditions to identify records to update.\n            See get_table() documentation for filtering syntax.\n            Examples: ``mouseName='ATL028'``, ``imaging=True``\n\n        Raises\n        ------\n        AssertionError\n            If the specified field is not in the database table.\n        \"\"\"\n        assert field in self.table_data()[0], f\"Requested field ({field}) is not in table. Use 'self.table_data()[0]' to see available fields.\"\n        df = self.get_table(**kw_conditions)\n        update_statement = self.create_update_many_statement(field)\n        uids = df[self.uid].tolist()  # uids of all sessions requested\n        val_as_list = [val] * len(uids)\n        print(f\"Setting {field}={val} for all requested records...\")\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(update_statement, zip(val_as_list, uids))\n\n    # == method for adding a record to the database ==\n    def add_record(self, insert_statement: str, columns: List[str], values: List[Any]) -&gt; str:\n        \"\"\"\n        Add a single record to the database.\n\n        First checks if a record with matching unique field values already exists.\n        If so, prevents duplicate insertion and returns a message. Otherwise,\n        adds the new record to the database.\n\n        Parameters\n        ----------\n        insert_statement : str\n            SQL INSERT statement with parameter placeholders.\n        columns : list\n            List of column names matching the insert statement.\n        values : list\n            List of values to insert, corresponding to the columns.\n\n        Returns\n        -------\n        str\n            Success or duplicate record message.\n        \"\"\"\n        d = dict(zip(columns, values))\n        unique_values = [d[uf[0]] for uf in self.unique_fields]  # get values associated with unique fields\n        for ii, uv in enumerate(unique_values):\n            if isinstance(uv, date) or isinstance(uv, datetime):\n                # this is required for communicating with Access\n                unique_values[ii] = uv.strftime(\"%Y-%m-%d\")\n        unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n        if self.get_record(*unique_values, verbose=False) is not None:\n            print(f\"Record already exists for {unique_combo}\")\n            return f\"Record already exists for {unique_combo}\"\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(insert_statement, values)\n            print(f\"Successfully added new record for {unique_combo}\")\n        return \"Successfully added new record\"\n\n    def get_record(self, *unique_values: Any, verbose: bool = True) -&gt; Optional[pd.Series]:\n        \"\"\"\n        Retrieve single record from table in database and return as dataframe.\n\n        This method retrieves a single record(row) from the table in the database. The metadata for\n        each database defines a set of fields that comprise a unique set (each combination of values\n        for the unique fields is only represented once in the database).\n\n        Parameters\n        ----------\n        *unique_values: variable length list of values associated with the unique fields\n            - must be the same length as self.uniqueFields\n            - the second value of the uniqueField tuple (string by default) determines how\n              to query the unique value\n\n        Returns\n        -------\n        record : pandas Series\n\n        Example\n        -------\n        &gt;&gt;&gt; vrdb = YourDatabaseClass()\n        &gt;&gt;&gt; record = vrdb.get_record(*unique_conditions)\n        \"\"\"\n\n        # Check if correct values are provided\n        if len(unique_values) != len(self.unique_fields):\n            expected_list = \", \".join([uf[0] for uf in self.unique_fields])\n            raise ValueError(f\"{len(unique_values)} values provided but *get_record* is expecting values for: {expected_list}\")\n\n        # Get table and compare\n        df = self.get_table()\n        for uf, uv in zip(self.unique_fields, unique_values):\n            if uf[1] == str:\n                df = df[df[uf[0]] == uv]\n            elif uf[1] == datetime:\n                df = df[df[uf[0]].apply(lambda sd: sd.strftime(\"%Y-%m-%d\")) == uv]\n            elif uf[1] == int:\n                df = df[df[uf[0]] == int(uv)]\n            else:\n                raise ValueError(f\"uniqueField type ({uf[1]}) not recognized, add the appropriate query to this method!\")\n\n        if len(df) == 0:\n            if verbose:\n                unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n                print(f\"No session found under: {unique_combo}\")\n            return None\n\n        if len(df) &gt; 1:\n            unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n            raise ValueError(f\"Multiple sessions found under: {unique_combo}\")\n        return df.iloc[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.__init__","title":"<code>__init__(db_name)</code>","text":"<p>Initialize a new database instance.</p> <p>This constructor initializes a new instance of the BaseDatabase class. It sets the default values for the table name, database name, and database path. It is built to work with the Microsoft Access application; however, a few small changes can make it compatible with other SQL-based database systems.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>(str, required)</code> <p>The name of the database to access.</p> required Example <p>db = BaseDatabase('vrSessions') print(vrdb.table_name) 'sessiondb' print(vrdb.db_name) 'vrDatabase'</p> Notes <ul> <li>This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.</li> <li>If you are using this on a new system, then you should edit your path, database name, and default table in that function.</li> </ul> Source code in <code>vrAnalysis/database.py</code> <pre><code>def __init__(self, db_name: str):\n    \"\"\"\n    Initialize a new database instance.\n\n    This constructor initializes a new instance of the BaseDatabase class. It sets the default\n    values for the table name, database name, and database path. It is built to work with\n    the Microsoft Access application; however, a few small changes can make it compatible with\n    other SQL-based database systems.\n\n    Parameters\n    ----------\n    db_name : str, required\n        The name of the database to access.\n\n    Example\n    -------\n    &gt;&gt;&gt; db = BaseDatabase('vrSessions')\n    &gt;&gt;&gt; print(vrdb.table_name)\n    'sessiondb'\n    &gt;&gt;&gt; print(vrdb.db_name)\n    'vrDatabase'\n\n    Notes\n    -----\n    - This constructor uses a supporting function called get_database_metadata to get database metadata based on the db_name provided.\n    - If you are using this on a new system, then you should edit your path, database name, and default table in that function.\n    \"\"\"\n\n    metadata = get_database_metadata(db_name)\n    self.db_path = metadata[\"db_path\"]\n    self.db_name = metadata[\"db_name\"]\n    self.db_ext = metadata[\"db_ext\"]\n    self.table_name = metadata[\"table_name\"]\n    self.uid = metadata[\"uid\"]\n    self.backup_path = metadata[\"backup_path\"]\n    self.host_type = host_types[self.db_ext]\n    self.unique_fields = self.process_unique_fields(metadata[\"unique_fields\"])\n    self.default_conditions = metadata[\"default_conditions\"]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.add_record","title":"<code>add_record(insert_statement, columns, values)</code>","text":"<p>Add a single record to the database.</p> <p>First checks if a record with matching unique field values already exists. If so, prevents duplicate insertion and returns a message. Otherwise, adds the new record to the database.</p> <p>Parameters:</p> Name Type Description Default <code>insert_statement</code> <code>str</code> <p>SQL INSERT statement with parameter placeholders.</p> required <code>columns</code> <code>list</code> <p>List of column names matching the insert statement.</p> required <code>values</code> <code>list</code> <p>List of values to insert, corresponding to the columns.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Success or duplicate record message.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def add_record(self, insert_statement: str, columns: List[str], values: List[Any]) -&gt; str:\n    \"\"\"\n    Add a single record to the database.\n\n    First checks if a record with matching unique field values already exists.\n    If so, prevents duplicate insertion and returns a message. Otherwise,\n    adds the new record to the database.\n\n    Parameters\n    ----------\n    insert_statement : str\n        SQL INSERT statement with parameter placeholders.\n    columns : list\n        List of column names matching the insert statement.\n    values : list\n        List of values to insert, corresponding to the columns.\n\n    Returns\n    -------\n    str\n        Success or duplicate record message.\n    \"\"\"\n    d = dict(zip(columns, values))\n    unique_values = [d[uf[0]] for uf in self.unique_fields]  # get values associated with unique fields\n    for ii, uv in enumerate(unique_values):\n        if isinstance(uv, date) or isinstance(uv, datetime):\n            # this is required for communicating with Access\n            unique_values[ii] = uv.strftime(\"%Y-%m-%d\")\n    unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n    if self.get_record(*unique_values, verbose=False) is not None:\n        print(f\"Record already exists for {unique_combo}\")\n        return f\"Record already exists for {unique_combo}\"\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.execute(insert_statement, values)\n        print(f\"Successfully added new record for {unique_combo}\")\n    return \"Successfully added new record\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.connect","title":"<code>connect()</code>","text":"<p>Establish a connection to the database.</p> <p>Creates a pyodbc connection using the appropriate driver string based on the database file extension. Currently configured for Microsoft Access databases (.accdb, .mdb files).</p> <p>Returns:</p> Type Description <code>Connection</code> <p>Database connection object.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the host type for the database extension is not supported.</p> Notes <p>To support additional database types: 1. Determine the appropriate pyodbc driver string for your database 2. Add it to the driver_string dictionary in this method 3. Update the host_types dictionary at the module level to map your    file extension to the driver key</p> <p>See https://www.connectionstrings.com/ for driver string examples.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def connect(self) -&gt; pyodbc.Connection:\n    \"\"\"\n    Establish a connection to the database.\n\n    Creates a pyodbc connection using the appropriate driver string based on\n    the database file extension. Currently configured for Microsoft Access\n    databases (.accdb, .mdb files).\n\n    Returns\n    -------\n    pyodbc.Connection\n        Database connection object.\n\n    Raises\n    ------\n    AssertionError\n        If the host type for the database extension is not supported.\n\n    Notes\n    -----\n    To support additional database types:\n    1. Determine the appropriate pyodbc driver string for your database\n    2. Add it to the driver_string dictionary in this method\n    3. Update the host_types dictionary at the module level to map your\n       file extension to the driver key\n\n    See https://www.connectionstrings.com/ for driver string examples.\n    \"\"\"\n    driver_string = {\"access\": r\"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\" + rf\"DBQ={self.get_dbfile()};\"}\n\n    # Make sure connections are possible for this hosttype\n    failure_message = (\n        f\"Requested host_type ({self.host_type}) is not available. The only ones that are coded are: {[k for k in driver_string.keys()]}\\n\\n\"\n        f\"For support with writing a driver string for a different host, use the fantastic website: https://www.connectionstrings.com/\"\n    )\n    assert self.host_type in driver_string, failure_message\n\n    # Return a connection to the database\n    return pyodbc.connect(driver_string[self.host_type])\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.construct_filter_string","title":"<code>construct_filter_string(key, val_op_tuple)</code>","text":"<p>Construct a string to be used as a pandas DataFrame query expression.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Column name to filter on.</p> required <code>val_op_tuple</code> <code>tuple</code> <p>Tuple of (value, operation) where operation is a comparison operator (e.g., '==', '!=', '&gt;', '&lt;').</p> required <p>Returns:</p> Type Description <code>str</code> <p>Query string in the format <code>column_name</code>operator'value'.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def construct_filter_string(self, key: str, val_op_tuple: Tuple[Any, str]) -&gt; str:\n    \"\"\"\n    Construct a string to be used as a pandas DataFrame query expression.\n\n    Parameters\n    ----------\n    key : str\n        Column name to filter on.\n    val_op_tuple : tuple\n        Tuple of (value, operation) where operation is a comparison operator\n        (e.g., '==', '!=', '&gt;', '&lt;').\n\n    Returns\n    -------\n    str\n        Query string in the format `column_name`operator'value'.\n    \"\"\"\n    val, op = val_op_tuple\n    return f\"`{key}`{op}{val!r}\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.create_update_many_statement","title":"<code>create_update_many_statement(field)</code>","text":"<p>Create an SQL UPDATE statement for batch updating a single field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL UPDATE statement with parameter placeholders for value and uid.</p> Example <p>with self.open_cursor(commit_changes=True) as cursor: ...     stmt = self.create_update_many_statement(\"fieldName\") ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def create_update_many_statement(self, field: str) -&gt; str:\n    \"\"\"\n    Create an SQL UPDATE statement for batch updating a single field.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n\n    Returns\n    -------\n    str\n        SQL UPDATE statement with parameter placeholders for value and uid.\n\n    Example\n    -------\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     stmt = self.create_update_many_statement(\"fieldName\")\n    ...     cursor.executemany(stmt, [(val1, uid1), (val2, uid2), ...])\n    \"\"\"\n    return f\"UPDATE {self.table_name} set {field} = ? where {self.uid} = ?\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.create_update_statement","title":"<code>create_update_statement(field, uid)</code>","text":"<p>Create an SQL UPDATE statement for a single field and record.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <code>uid</code> <code>any</code> <p>Unique identifier value for the record to update.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL UPDATE statement with parameter placeholder.</p> Example <p>with self.open_cursor(commit_changes=True) as cursor: ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def create_update_statement(self, field: str, uid: Any) -&gt; str:\n    \"\"\"\n    Create an SQL UPDATE statement for a single field and record.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n    uid : any\n        Unique identifier value for the record to update.\n\n    Returns\n    -------\n    str\n        SQL UPDATE statement with parameter placeholder.\n\n    Example\n    -------\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     cursor.execute(self.create_update_statement(\"fieldName\", 123), value)\n    \"\"\"\n    return f\"UPDATE {self.table_name} set {field} = ? WHERE {self.uid} = {uid}\"\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.get_dbfile","title":"<code>get_dbfile()</code>","text":"<p>Get the full path to the database file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the database file.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_dbfile(self) -&gt; Path:\n    \"\"\"\n    Get the full path to the database file.\n\n    Returns\n    -------\n    Path\n        Path object pointing to the database file.\n    \"\"\"\n    return Path(self.db_path) / (self.db_name + self.db_ext)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.get_record","title":"<code>get_record(*unique_values, verbose=True)</code>","text":"<p>Retrieve single record from table in database and return as dataframe.</p> <p>This method retrieves a single record(row) from the table in the database. The metadata for each database defines a set of fields that comprise a unique set (each combination of values for the unique fields is only represented once in the database).</p> <p>Parameters:</p> Name Type Description Default <code>*unique_values</code> <code>Any</code> <ul> <li>must be the same length as self.uniqueFields</li> <li>the second value of the uniqueField tuple (string by default) determines how   to query the unique value</li> </ul> <code>()</code> <p>Returns:</p> Name Type Description <code>record</code> <code>pandas Series</code> Example <p>vrdb = YourDatabaseClass() record = vrdb.get_record(*unique_conditions)</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_record(self, *unique_values: Any, verbose: bool = True) -&gt; Optional[pd.Series]:\n    \"\"\"\n    Retrieve single record from table in database and return as dataframe.\n\n    This method retrieves a single record(row) from the table in the database. The metadata for\n    each database defines a set of fields that comprise a unique set (each combination of values\n    for the unique fields is only represented once in the database).\n\n    Parameters\n    ----------\n    *unique_values: variable length list of values associated with the unique fields\n        - must be the same length as self.uniqueFields\n        - the second value of the uniqueField tuple (string by default) determines how\n          to query the unique value\n\n    Returns\n    -------\n    record : pandas Series\n\n    Example\n    -------\n    &gt;&gt;&gt; vrdb = YourDatabaseClass()\n    &gt;&gt;&gt; record = vrdb.get_record(*unique_conditions)\n    \"\"\"\n\n    # Check if correct values are provided\n    if len(unique_values) != len(self.unique_fields):\n        expected_list = \", \".join([uf[0] for uf in self.unique_fields])\n        raise ValueError(f\"{len(unique_values)} values provided but *get_record* is expecting values for: {expected_list}\")\n\n    # Get table and compare\n    df = self.get_table()\n    for uf, uv in zip(self.unique_fields, unique_values):\n        if uf[1] == str:\n            df = df[df[uf[0]] == uv]\n        elif uf[1] == datetime:\n            df = df[df[uf[0]].apply(lambda sd: sd.strftime(\"%Y-%m-%d\")) == uv]\n        elif uf[1] == int:\n            df = df[df[uf[0]] == int(uv)]\n        else:\n            raise ValueError(f\"uniqueField type ({uf[1]}) not recognized, add the appropriate query to this method!\")\n\n    if len(df) == 0:\n        if verbose:\n            unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n            print(f\"No session found under: {unique_combo}\")\n        return None\n\n    if len(df) &gt; 1:\n        unique_combo = \", \".join([f\"{uf[0]}={uv}\" for uf, uv in zip(self.unique_fields, unique_values)])\n        raise ValueError(f\"Multiple sessions found under: {unique_combo}\")\n    return df.iloc[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.get_table","title":"<code>get_table(use_default=True, **kw_conditions)</code>","text":"<p>Retrieve data from table in database and return as dataframe with optional filtering.</p> <p>This method retrieves all data from the primary table in the database specified in BaseDatabase instance. It automatically filters the data using the defaultConditions defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is a conflict.</p> <p>Parameters:</p> Name Type Description Default <code>use_default</code> <code>bool</code> <p>Use default conditions if true, if False ignore them</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions as keyword arguments. Each condition should match a column name in the table. Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair. The operation defaults to '==', but you can use anything that works as a df query.</p> <p>Examples:     - Simple equality: <code>imaging=True</code> filters where imaging column equals True     - Comparison operators: <code>sessionID=(5, '&gt;')</code> filters where sessionID &gt; 5     - Multiple conditions: <code>imaging=True, mouseName='ATL028'</code> applies AND logic</p> <p>Note: this is limited in the sense that empty data can't be identified with key:None. (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas dataframe</code> <p>A dataframe containing the filtered data from the primary database table.</p> Example <p>vrdb = YourDatabaseClass() df = vrdb.get_table(imaging=True) df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_table(self, use_default: bool = True, **kw_conditions: Any) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieve data from table in database and return as dataframe with optional filtering.\n\n    This method retrieves all data from the primary table in the database specified in\n    BaseDatabase instance. It automatically filters the data using the defaultConditions\n    defined in the dbMetadata method. kw_conditions overwrite defaultConditions if there is\n    a conflict.\n\n    Parameters\n    ----------\n    use_default : bool, default=True\n        Use default conditions if true, if False ignore them\n    **kw_conditions : dict, optional\n        Additional filtering conditions as keyword arguments.\n        Each condition should match a column name in the table.\n        Value can either be a variable (e.g. 0 or 'ATL000'), or a (value, operation) pair.\n        The operation defaults to '==', but you can use anything that works as a df query.\n\n        Examples:\n            - Simple equality: ``imaging=True`` filters where imaging column equals True\n            - Comparison operators: ``sessionID=(5, '&gt;')`` filters where sessionID &gt; 5\n            - Multiple conditions: ``imaging=True, mouseName='ATL028'`` applies AND logic\n\n        Note: this is limited in the sense that empty data can't be identified with key:None.\n        (using the pd.isnull() is a valid work around, but needs to be coded outside of get_table())\n\n    Returns\n    -------\n    df : pandas dataframe\n        A dataframe containing the filtered data from the primary database table.\n\n    Example\n    -------\n    &gt;&gt;&gt; vrdb = YourDatabaseClass()\n    &gt;&gt;&gt; df = vrdb.get_table(imaging=True)\n    &gt;&gt;&gt; df = vrdb.get_table(mouseName='ATL028', sessionID=(5, '&gt;'))\n    \"\"\"\n\n    field_names, table_data = self.table_data()\n    df = pd.DataFrame.from_records(table_data, columns=field_names)\n    conditions = copy(self.default_conditions) if use_default else {}\n    conditions.update(kw_conditions)\n    if conditions:\n        for key, val in conditions.items():\n            assert key in field_names, f\"{key} is not a column name in {self.table_name}\"\n            conditions[key] = self.process_filter_value(val)  # make sure it's a value/operation pair\n        query = \" &amp; \".join([self.construct_filter_string(key, val_op_tuple) for key, val_op_tuple in conditions.items()])\n        df = df.query(query)\n    return df\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.open_cursor","title":"<code>open_cursor(commit_changes=False)</code>","text":"<p>Context manager to open a database cursor and manage connections.</p> <p>This context manager provides a convenient way to open a cursor to the database, perform database operations, and manage connections. It also allows you to commit changes if needed.</p> <p>Parameters:</p> Name Type Description Default <code>commit_changes</code> <code>bool</code> <p>Whether to commit changes to the database. Default is False.</p> <code>False</code> <p>Yields:</p> Type Description <code>Cursor</code> <p>A database cursor for executing SQL queries.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while connecting to the database.</p> Example <p>Use the context manager to perform database operations:</p> <p>with self.open_cursor(commit_changes=True) as cursor: ...     cursor.execute(\"SELECT * FROM your_table\")</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>@contextmanager\ndef open_cursor(self, commit_changes: bool = False) -&gt; Generator[pyodbc.Cursor, None, None]:\n    \"\"\"\n    Context manager to open a database cursor and manage connections.\n\n    This context manager provides a convenient way to open a cursor to the database,\n    perform database operations, and manage connections. It also allows you to\n    commit changes if needed.\n\n    Parameters\n    ----------\n    commit_changes : bool, optional\n        Whether to commit changes to the database. Default is False.\n\n    Yields\n    ------\n    pyodbc.Cursor\n        A database cursor for executing SQL queries.\n\n    Raises\n    ------\n    Exception\n        If an error occurs while connecting to the database.\n\n    Example\n    -------\n    Use the context manager to perform database operations:\n\n    &gt;&gt;&gt; with self.open_cursor(commit_changes=True) as cursor:\n    ...     cursor.execute(\"SELECT * FROM your_table\")\n\n    \"\"\"\n    try:\n        # Attempt to open a cursor to the database\n        conn = self.connect()\n        cursor = conn.cursor()\n        yield cursor\n    except Exception as ex:\n        print(f\"An exception occurred while trying to connect to {self.db_name}!\")\n        print(ex)\n        raise ex\n    else:\n        # if no exception was raised, commit changes\n        if commit_changes:\n            conn.commit()\n    finally:\n        # Always close the cursor and connection\n        cursor.close()\n        conn.close()\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.process_filter_value","title":"<code>process_filter_value(val)</code>","text":"<p>Ensure filter value has an operation associated with it.</p> <p>Filters are passed to pandas DataFrame queries as {key}{operation}{value}. This method ensures each value is a tuple of (value, operation), defaulting to '==' if no operation is specified.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>any or tuple</code> <p>Filter value. If a tuple, should be (value, operation). If not a tuple, will be converted to (val, '==').</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (value, operation) for use in DataFrame queries.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def process_filter_value(self, val: Union[Any, Tuple[Any, str]]) -&gt; Tuple[Any, str]:\n    \"\"\"\n    Ensure filter value has an operation associated with it.\n\n    Filters are passed to pandas DataFrame queries as {key}{operation}{value}.\n    This method ensures each value is a tuple of (value, operation), defaulting\n    to '==' if no operation is specified.\n\n    Parameters\n    ----------\n    val : any or tuple\n        Filter value. If a tuple, should be (value, operation). If not a tuple,\n        will be converted to (val, '==').\n\n    Returns\n    -------\n    tuple\n        Tuple of (value, operation) for use in DataFrame queries.\n    \"\"\"\n    if not isinstance(val, tuple):\n        val = (val, \"==\")\n    return val\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.process_unique_fields","title":"<code>process_unique_fields(fields)</code>","text":"<p>Process and validate unique field definitions.</p> <p>Converts unique field specifications into a standardized format where each field is a tuple of (field_name, field_type). String fields can be specified as just the name and will default to str type.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>List of field specifications. Each can be: - A string (field name, defaults to str type) - A tuple of (field_name, type) where type is a Python type class</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of tuples, each containing (field_name, field_type).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a field specification is not a string or a valid (string, type) tuple.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def process_unique_fields(self, fields: List[Union[str, Tuple[str, type]]]) -&gt; List[Tuple[str, type]]:\n    \"\"\"\n    Process and validate unique field definitions.\n\n    Converts unique field specifications into a standardized format where each\n    field is a tuple of (field_name, field_type). String fields can be specified\n    as just the name and will default to str type.\n\n    Parameters\n    ----------\n    fields : list\n        List of field specifications. Each can be:\n        - A string (field name, defaults to str type)\n        - A tuple of (field_name, type) where type is a Python type class\n\n    Returns\n    -------\n    list\n        List of tuples, each containing (field_name, field_type).\n\n    Raises\n    ------\n    ValueError\n        If a field specification is not a string or a valid (string, type) tuple.\n    \"\"\"\n    ufields = []\n    for f in fields:\n        if isinstance(f, tuple) and len(f) == 2 and type(f[1]) == type and isinstance(f[0], str):\n            ufields.append(f)\n        elif isinstance(f, str):\n            ufields.append((f, str))\n        else:\n            raise ValueError(f\"unique field {f} must be a string or a string-type tuple\")\n    return ufields\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.save_backup","title":"<code>save_backup(return_out=False)</code>","text":"<p>Save a backup of the database to the backup path specified in metadata.</p> <p>Parameters:</p> Name Type Description Default <code>return_out</code> <code>bool</code> <p>If True, return the output of the robocopy command.</p> <code>False</code> <p>Returns:</p> Type Description <code>CompletedProcess or None</code> <p>CompletedProcess object from the robocopy command (if return_out is True). Returns None if return_out is False.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def save_backup(self, return_out: bool = False) -&gt; Optional[CompletedProcess]:\n    \"\"\"Save a backup of the database to the backup path specified in metadata.\n\n    Parameters\n    ----------\n    return_out : bool, optional\n        If True, return the output of the robocopy command.\n\n    Returns\n    -------\n    CompletedProcess or None\n        CompletedProcess object from the robocopy command (if return_out is True).\n        Returns None if return_out is False.\n    \"\"\"\n    source_path = self.db_path\n    target_path = self.backup_path\n    source_file = self.db_name + self.db_ext\n    robocopy_arguments = f\"robocopy {source_path} {target_path} {source_file}\"\n    outs = run(robocopy_arguments, capture_output=True, text=True)\n    if return_out:\n        return outs\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.show_metadata","title":"<code>show_metadata()</code>","text":"<p>Display metadata associated with the open database.</p> <p>Prints information about the database location, name, table, unique ID field, backup path, and default filtering conditions.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def show_metadata(self) -&gt; None:\n    \"\"\"\n    Display metadata associated with the open database.\n\n    Prints information about the database location, name, table, unique ID field,\n    backup path, and default filtering conditions.\n    \"\"\"\n    print(f\"{self.host_type} database located at {self.db_path}\")\n    print(f\"Database name: {self.db_name}{self.db_ext}, table name: {self.table_name}, with uid: {self.uid}\")\n    if self.backup_path is not None:\n        print(f\"Backup path located at: {self.backup_path}\")\n    else:\n        print(f\"No backup path specified...\")\n    if self.default_conditions:\n        print(f\"Default database filters:\")\n        for key, val in self.default_conditions.items():\n            print(\"  \", self.construct_filter_string(key, self.process_filter_value(val)))\n    else:\n        print(f\"No default filters.\")\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.table_column_info","title":"<code>table_column_info()</code>","text":"<p>Retrieve the column names, data types, and nullable status of the table.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing three elements: - A list of strings representing the column names of the table. - A list of strings representing the data types of the table. - A list of booleans representing the nullable status of the table.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def table_column_info(self) -&gt; Tuple[List[str], List[str], List[bool]]:\n    \"\"\"\n    Retrieve the column names, data types, and nullable status of the table.\n\n    Returns\n    -------\n    tuple\n        A tuple containing three elements:\n        - A list of strings representing the column names of the table.\n        - A list of strings representing the data types of the table.\n        - A list of booleans representing the nullable status of the table.\n    \"\"\"\n    with self.open_cursor(commit_changes=False) as cursor:\n        query = f\"SELECT * FROM {self.table_name} WHERE 1=0\"\n        cursor.execute(query)\n        column_descriptions = cursor.description\n    column_name, data_type, _, _, _, _, nullable = map(list, zip(*column_descriptions))\n    return column_name, data_type, nullable\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.table_data","title":"<code>table_data()</code>","text":"<p>Retrieve data and field names from the specified table.</p> <p>This method retrieves the field names and table elements from the table specified in the <code>BaseDatabase</code> instance.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two elements: - A list of strings representing the field names of the table. - A list of tuples representing the data rows of the table.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def table_data(self) -&gt; Tuple[List[str], List[Tuple[Any, ...]]]:\n    \"\"\"\n    Retrieve data and field names from the specified table.\n\n    This method retrieves the field names and table elements from the table specified\n    in the `BaseDatabase` instance.\n\n    Returns\n    -------\n    tuple\n        A tuple containing two elements:\n        - A list of strings representing the field names of the table.\n        - A list of tuples representing the data rows of the table.\n    \"\"\"\n    with self.open_cursor() as cursor:\n        field_names = [col.column_name for col in cursor.columns(table=self.table_name)]\n        cursor.execute(f\"SELECT * FROM {self.table_name}\")\n        table_elements = cursor.fetchall()\n\n    return field_names, table_elements\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.BaseDatabase.update_database_field","title":"<code>update_database_field(field, val, **kw_conditions)</code>","text":"<p>Update a database field for all records matching specified conditions.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the field to update.</p> required <code>val</code> <code>any</code> <p>Value to set for the field.</p> required <code>**kw_conditions</code> <code>dict</code> <p>Filtering conditions to identify records to update. See get_table() documentation for filtering syntax. Examples: <code>mouseName='ATL028'</code>, <code>imaging=True</code></p> <code>{}</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the specified field is not in the database table.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def update_database_field(self, field: str, val: Any, **kw_conditions: Any) -&gt; None:\n    \"\"\"\n    Update a database field for all records matching specified conditions.\n\n    Parameters\n    ----------\n    field : str\n        Name of the field to update.\n    val : any\n        Value to set for the field.\n    **kw_conditions : dict, optional\n        Filtering conditions to identify records to update.\n        See get_table() documentation for filtering syntax.\n        Examples: ``mouseName='ATL028'``, ``imaging=True``\n\n    Raises\n    ------\n    AssertionError\n        If the specified field is not in the database table.\n    \"\"\"\n    assert field in self.table_data()[0], f\"Requested field ({field}) is not in table. Use 'self.table_data()[0]' to see available fields.\"\n    df = self.get_table(**kw_conditions)\n    update_statement = self.create_update_many_statement(field)\n    uids = df[self.uid].tolist()  # uids of all sessions requested\n    val_as_list = [val] * len(uids)\n    print(f\"Setting {field}={val} for all requested records...\")\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(update_statement, zip(val_as_list, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase","title":"<code>SessionDatabase</code>","text":"<p>               Bases: <code>BaseDatabase</code></p> <p>Database class for handling VR session data.</p> <p>Specialized database class that extends BaseDatabase with session-specific functionality, including methods for creating session objects, managing registration workflows, and handling quality control processes.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>class SessionDatabase(BaseDatabase):\n    \"\"\"\n    Database class for handling VR session data.\n\n    Specialized database class that extends BaseDatabase with session-specific\n    functionality, including methods for creating session objects, managing\n    registration workflows, and handling quality control processes.\n    \"\"\"\n\n    def iter_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; List[B2Session]:\n        \"\"\"Iterate over sessions matching conditions.\n\n        Parameters\n        ----------\n        session_params : dict, default={}\n            Additional parameters to pass to the session constructor when creating\n            B2Session objects. These are passed through to B2Session.create().\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n            Examples: ``mouseName='ATL028'``, ``imaging=True``, ``sessionID=(5, '&gt;')``\n\n        Returns\n        -------\n        sessions : list[B2Session]\n            List of sessions matching the conditions.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        sessions = []\n        for _, row in df.iterrows():\n            sessions.append(B2Session.create(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params))\n        return sessions\n\n    def gen_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; Generator[B2Session, None, None]:\n        \"\"\"Generate sessions matching conditions.\n\n        Parameters\n        ----------\n        session_params : dict, default={}\n            Additional parameters to pass to the session constructor when creating\n            B2Session objects. These are passed through to B2Session.create().\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        generator[B2Session]\n            Generator of sessions matching the conditions.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        for _, row in df.iterrows():\n            yield B2Session.create(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params)\n\n    def session_name(self, row: pd.Series) -&gt; Tuple[str, str, str]:\n        \"\"\"\n        Extract session identifiers from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n\n        Returns\n        -------\n        tuple\n            Tuple of (mouse_name, session_date, session_id) where session_date is\n            formatted as 'YYYY-MM-DD' and session_id is converted to string.\n        \"\"\"\n        mouse_name = row[\"mouseName\"]\n        session_date = row[\"sessionDate\"].strftime(\"%Y-%m-%d\")\n        session_id = str(row[\"sessionID\"])\n        return mouse_name, session_date, session_id\n\n    def make_b2session(self, row: pd.Series) -&gt; B2Session:\n        \"\"\"\n        Create a B2Session object from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n\n        Returns\n        -------\n        B2Session\n            Session object initialized with data from the record.\n        \"\"\"\n        mouse_name, session_date, session_id = self.session_name(row)\n        return B2Session.create(mouse_name, session_date, session_id)\n\n    def make_b2registration(self, row: pd.Series, opts: B2RegistrationOpts) -&gt; B2Registration:\n        \"\"\"\n        Create a B2Registration object from a database record.\n\n        Parameters\n        ----------\n        row : pandas.Series\n            Database record containing session information.\n        opts : B2RegistrationOpts\n            Registration options to use for the session.\n\n        Returns\n        -------\n        B2Registration\n            Registration object initialized with session data and options.\n        \"\"\"\n        mouse_name, session_date, session_id = self.session_name(row)\n        return B2Registration(mouse_name, session_date, session_id, opts)\n\n    # == helper functions for figuring out what needs work ==\n    def needs_registration(self, skip_errors: bool = True, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need registration preprocessing.\n\n        Parameters\n        ----------\n        skip_errors : bool, default=True\n            If True, exclude sessions that had registration errors.\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need registration.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        if skip_errors:\n            df = df[df[\"vrRegistrationError\"] == False]\n        df = df[df[\"vrRegistration\"] == False]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                session = self.make_b2session(row)\n                print(f\"Session needs registration: {session.session_print()}\")\n            return None\n\n    def update_s2p_date_time(self) -&gt; None:\n        \"\"\"\n        Update suite2p creation dates in the database based on file modification times.\n\n        For all sessions where suite2p processing is complete, finds the most recent\n        file modification time in the suite2p output directory and updates the\n        suite2pDate field in the database.\n        \"\"\"\n        df = self.get_table()\n        s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n        uids = s2p_done[self.uid].tolist()\n        s2p_creation_date = []\n        for idx, row in s2p_done.iterrows():\n            session = self.make_b2session(row)  # create vrSession to point to session folder\n            c_latest_mod = 0\n            for p in session.s2p_path.rglob(\"*\"):\n                if not (p.is_dir()):\n                    c_latest_mod = max(p.stat().st_mtime, c_latest_mod)\n            c_date_time = datetime.fromtimestamp(c_latest_mod)\n            s2p_creation_date.append(c_date_time)  # get suite2p path creation date\n\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(self.create_update_many_statement(\"suite2pDate\"), zip(s2p_creation_date, uids))\n\n    def needs_s2p(\n        self, needs_qc: bool = False, return_df: bool = True, print_targets: bool = True, **kw_conditions: Any\n    ) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need suite2p processing or quality control.\n\n        Parameters\n        ----------\n        needs_qc : bool, default=False\n            If False, returns/prints sessions that need suite2p processing.\n            If True, returns/prints sessions that need suite2p quality control.\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        print_targets : bool, default=True\n            If True and return_df=False, prints suite2p target information for sessions needing processing.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        if needs_qc:\n            df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"suite2pQC\"] == False)]\n        else:\n            df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                session = self.make_b2session(row)\n                if needs_qc:\n                    print(f\"Database indicates that suite2p has been run but not QC'd: {session.session_print()}\")\n                else:\n                    print(f\"Database indicates that suite2p has not been run: {session.session_print()}\")\n                    if print_targets:\n                        mouse_name, session_date, session_id = self.session_name(row)\n                        s2p_targets(mouse_name, session_date, session_id)\n                        print(\"\")\n            return None\n\n    def check_s2p(self, with_database_update: bool = False, return_check: bool = False) -&gt; Optional[bool]:\n        \"\"\"\n        Verify suite2p status consistency between database and file system.\n\n        Checks for discrepancies where:\n        - Database says suite2p is done but files don't exist\n        - Files exist but database says suite2p wasn't done\n\n        Parameters\n        ----------\n        with_database_update : bool, default=False\n            If True, automatically corrects database entries when discrepancies are found.\n        return_check : bool, default=False\n            If True, returns a boolean indicating whether any discrepancies were found.\n\n        Returns\n        -------\n        bool or None\n            If return_check is True, returns True if any discrepancies were found,\n            False otherwise. Returns None if return_check is False.\n        \"\"\"\n        df = self.get_table()\n\n        # Check sessions where database says suite2p is done but files don't exist\n        check_s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n        checked_not_done = check_s2p_done.apply(lambda row: not (self.make_b2session(row).s2p_path.exists()), axis=1)\n        not_actually_done = check_s2p_done[checked_not_done]\n\n        # Check sessions where files exist but database says suite2p wasn't done\n        check_s2p_needed = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n        checked_not_needed = check_s2p_needed.apply(lambda row: self.make_b2session(row).s2p_path.exists(), axis=1)\n        not_actually_needed = check_s2p_needed[checked_not_needed]\n\n        # Print database errors to workspace\n        for idx, row in not_actually_done.iterrows():\n            print(f\"Database said suite2p has been ran, but it actually hasn't: {self.make_b2session(row).session_print()}\")\n        for idx, row in not_actually_needed.iterrows():\n            print(f\"Database said suite2p didn't run, but it already did: {self.make_b2session(row).session_print()}\")\n\n        # If with_database_update is True, correct the database\n        if with_database_update:\n            for idx, row in not_actually_done.iterrows():\n                with self.open_cursor(commit_changes=True) as cursor:\n                    cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), False)\n\n            for idx, row in not_actually_needed.iterrows():\n                with self.open_cursor(commit_changes=True) as cursor:\n                    cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), True)\n\n        # If return_check is requested, return True if any records were invalid\n        if return_check:\n            return checked_not_done.any() or checked_not_needed.any()\n\n    # == for communicating with the database about red cell quality control ==\n    def update_red_cell_qc_date_time(self) -&gt; None:\n        \"\"\"\n        Update red cell QC dates in the database based on file modification times.\n\n        For all sessions where red cell QC is complete, finds the most recent\n        modification time of relevant red cell QC files and updates the\n        redCellQCDate field in the database.\n        \"\"\"\n        relevant_one_files = [\n            \"mpciROIs.redCellIdx.npy\",\n            \"mpciROIs.redCellManualAssignment.npy\",\n            \"parametersRed*\",  # wildcard because there are multiple possibilities\n        ]\n\n        df = self.get_table()\n        red_cell_qc_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"redCellQC\"] == True)]\n        uids = red_cell_qc_done[self.uid].tolist()\n        rc_edit_date = []\n        for idx, row in red_cell_qc_done.iterrows():\n            session = self.make_b2session(row)  # create vrSession to point to session folder\n            c_latest_mod = 0\n            for f in relevant_one_files:\n                for file in session.one_path.rglob(f):\n                    c_latest_mod = max(file.stat().st_mtime, c_latest_mod)\n            c_date_time = datetime.fromtimestamp(c_latest_mod)\n            rc_edit_date.append(c_date_time)  # get red cell QC file modification date\n\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.executemany(self.create_update_many_statement(\"redCellQCDate\"), zip(rc_edit_date, uids))\n\n    def needs_red_cell_qc(self, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n        \"\"\"\n        Get or print sessions that need red cell quality control.\n\n        Parameters\n        ----------\n        return_df : bool, default=True\n            If True, returns a DataFrame. If False, prints the sessions instead.\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            If return_df=True, returns DataFrame containing sessions that need red cell QC.\n            Sessions must have imaging, suite2p processing, and registration completed.\n            If return_df=False, returns None and prints the sessions instead.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"vrRegistration\"] == True) &amp; (df[\"redCellQC\"] == False)]\n\n        if return_df:\n            return df\n        else:\n            for idx, row in df.iterrows():\n                print(f\"Database indicates that redCellQC has not been performed for session: {self.make_b2session(row).session_print()}\")\n            return None\n\n    def iter_sessions_need_red_cell_qc(self, **kw_conditions: Any) -&gt; List[B2Session]:\n        \"\"\"\n        Get list of sessions that require red cell quality control.\n\n        Parameters\n        ----------\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n\n        Returns\n        -------\n        list[B2Session]\n            List of session objects that need red cell QC.\n        \"\"\"\n        df = self.needs_red_cell_qc(return_df=True, **kw_conditions)\n        sessions = []\n        for idx, row in df.iterrows():\n            sessions.append(self.make_b2session(row))\n        return sessions\n\n    def set_red_cell_qc(self, mouse_name: str, date_string: str, session_id: Union[str, int], state: bool = True) -&gt; bool:\n        \"\"\"\n        Set the red cell QC status for a specific session.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Mouse name identifier.\n        date_string : str\n            Session date in 'YYYY-MM-DD' format.\n        session_id : str or int\n            Session ID.\n        state : bool, default=True\n            Red cell QC status to set. If True, also sets the QC date to now.\n\n        Returns\n        -------\n        bool\n            True if update was successful, False otherwise.\n        \"\"\"\n        record = self.get_record(mouse_name, date_string, session_id)\n        if record is None:\n            print(f\"Could not find session {mouse_name}/{date_string}/{session_id} in database.\")\n            return False\n\n        try:\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"redCellQC\", record[self.uid]), state)\n                if state == True:\n                    # If setting red cell QC to true, add the date\n                    cursor.execute(\n                        self.create_update_statement(\"redCellQCDate\", record[self.uid]),\n                        datetime.now(),\n                    )\n                else:\n                    # Otherwise remove the date\n                    cursor.execute(self.create_update_statement(\"redCellQCDate\", record[self.uid]), \"\")\n            return True\n\n        except Exception as ex:\n            print(f\"Failed to update database for session: {mouse_name}/{date_string}/{session_id}\")\n            print(f\"Error: {ex}\")\n            return False\n\n    # == operating vrExperiment pipeline ==\n    def register_record(self, record: pd.Series, raise_exception: bool = False, imaging: Optional[bool] = None) -&gt; Tuple[bool, int]:\n        \"\"\"\n        Perform registration preprocessing for a single session record.\n\n        Creates a B2Registration object and runs preprocessing. Updates the database\n        with success/failure status and error information if applicable.\n\n        Parameters\n        ----------\n        record : pandas.Series\n            Database record containing session information.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting. If None (default), uses the value from the\n            database record. If True or False, overrides the database value.\n\n        Returns\n        -------\n        tuple\n            Tuple of (success, data_size) where:\n            - success: bool indicating if registration succeeded\n            - data_size: int size in bytes of registered oneData (0 if failed)\n\n        Notes\n        -----\n        On failure, clears all oneData files and updates database error fields.\n        On success, updates registration status and date in the database.\n        \"\"\"\n        opts = B2RegistrationOpts()\n        opts.imaging = bool(imaging) if imaging is not None else bool(record[\"imaging\"])\n        opts.facecam = bool(record[\"faceCamera\"])\n        opts.vrBehaviorVersion = record[\"vrBehaviorVersion\"]\n        b2reg = self.make_b2registration(record, opts)\n        try:\n            print(f\"Registering data for session: {b2reg.session_print()}\")\n            b2reg.register()\n        except Exception as ex:\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), True)\n                cursor.execute(\n                    self.create_update_statement(\"vrRegistrationException\", record[self.uid]),\n                    str(ex),\n                )\n            if raise_exception:\n                raise ex\n            print(f\"The following exception was raised when trying to preprocess session: {b2reg.session_print()}. Clearing all oneData.\")\n            b2reg.clear_one_data(certainty=True)\n            error_print(f\"Last traceback: {traceback.extract_tb(ex.__traceback__, limit=-1)}\")\n            error_print(f\"Exception: {ex}\")\n            # If failed, return (False, 0)\n            out = (False, 0)\n        else:\n            with self.open_cursor(commit_changes=True) as cursor:\n                # Tell the database that vrRegistration was performed and the time of processing\n                cursor.execute(self.create_update_statement(\"vrRegistration\", record[self.uid]), True)\n                cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), False)\n                cursor.execute(self.create_update_statement(\"vrRegistrationException\", record[self.uid]), \"\")\n                cursor.execute(\n                    self.create_update_statement(\"vrRegistrationDate\", record[self.uid]),\n                    datetime.now(),\n                )\n            # If successful, return (True, size of registered oneData)\n            out = (True, sum([one_file.stat().st_size for one_file in b2reg.get_saved_one()]))\n            print(f\"Session {b2reg.session_print()} registered with {readable_bytes(out[1])} oneData.\")\n        finally:\n            del b2reg\n        return out\n\n    def register_single_session(\n        self,\n        mouse_name: str,\n        session_date: str,\n        session_id: Union[str, int],\n        raise_exception: bool = False,\n        imaging: Optional[bool] = None,\n    ) -&gt; Optional[bool]:\n        \"\"\"\n        Register a single session by its identifiers.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Mouse name identifier.\n        session_date : str\n            Session date in 'YYYY-MM-DD' format.\n        session_id : str or int\n            Session ID.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting. If None (default), uses the value from the\n            database record. If True or False, overrides the database value to enable\n            or disable imaging processing during registration.\n\n        Returns\n        -------\n        bool or None\n            True if registration succeeded, False if failed, None if session not found.\n        \"\"\"\n        record = self.get_record(mouse_name, session_date, session_id)\n        if record is None:\n            print(f\"Session {'/'.join([mouse_name, session_date, session_id])} is not in the database\")\n            return\n        out = self.register_record(record, raise_exception=raise_exception, imaging=imaging)\n        return out[0]\n\n    def register_sessions(\n        self,\n        max_data: float = 30e9,\n        skip_errors: bool = True,\n        raise_exception: bool = False,\n        imaging: Optional[bool] = None,\n    ) -&gt; None:\n        \"\"\"\n        Register multiple sessions that need registration.\n\n        Processes sessions in batches, stopping when the total data size limit is reached.\n        Provides progress updates including accumulated data size and estimates.\n\n        Parameters\n        ----------\n        max_data : float, default=30e9\n            Maximum total data size (in bytes) to process before stopping.\n            Default is 30 GB.\n        skip_errors : bool, default=True\n            If True, skip sessions that had previous registration errors.\n        raise_exception : bool, default=False\n            If True, raises exceptions instead of handling them silently.\n        imaging : bool, optional\n            Override the imaging setting for all sessions. If None (default), uses the\n            value from each session's database record. If True or False, overrides the\n            database value for all sessions to enable or disable imaging processing.\n\n        Notes\n        -----\n        Prints progress information including:\n        - Accumulated oneData registered\n        - Average data size per session\n        - Estimated remaining data to process\n        \"\"\"\n        count_sessions = 0\n        total_one_data = 0.0\n        df_to_register = self.needs_registration(skip_errors=skip_errors)\n\n        for idx, (_, row) in enumerate(df_to_register.iterrows()):\n            if total_one_data &gt; max_data:\n                print(f\"\\nMax data limit reached. Total processed: {readable_bytes(total_one_data)}. Limit: {readable_bytes(max_data)}\")\n                return\n            print(\"\")\n            out = self.register_record(row, raise_exception=raise_exception, imaging=imaging)\n            if out[0]:\n                count_sessions += 1  # count successful sessions\n                total_one_data += out[1]  # accumulated oneData registered\n                estimate_remaining = len(df_to_register) - idx - 1\n                print(\n                    f\"Accumulated oneData registered: {readable_bytes(total_one_data)}. \"\n                    f\"Averaging: {readable_bytes(total_one_data/count_sessions)} / session. \"\n                    f\"Estimate remaining: {readable_bytes(total_one_data/count_sessions*estimate_remaining)}\"\n                )\n\n    def print_registration_errors(self, **kw_conditions: Any) -&gt; None:\n        \"\"\"\n        Print registration errors for sessions that failed registration.\n\n        Parameters\n        ----------\n        **kw_conditions : dict, optional\n            Additional filtering conditions passed to get_table().\n            See get_table() documentation for filtering syntax.\n        \"\"\"\n        df = self.get_table(**kw_conditions)\n        for idx, row in df[df[\"vrRegistrationError\"] == True].iterrows():\n            print(f\"{'/'.join(self.session_name(row))} had error: {row['vrRegistrationException']}\")\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.check_s2p","title":"<code>check_s2p(with_database_update=False, return_check=False)</code>","text":"<p>Verify suite2p status consistency between database and file system.</p> <p>Checks for discrepancies where: - Database says suite2p is done but files don't exist - Files exist but database says suite2p wasn't done</p> <p>Parameters:</p> Name Type Description Default <code>with_database_update</code> <code>bool</code> <p>If True, automatically corrects database entries when discrepancies are found.</p> <code>False</code> <code>return_check</code> <code>bool</code> <p>If True, returns a boolean indicating whether any discrepancies were found.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool or None</code> <p>If return_check is True, returns True if any discrepancies were found, False otherwise. Returns None if return_check is False.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def check_s2p(self, with_database_update: bool = False, return_check: bool = False) -&gt; Optional[bool]:\n    \"\"\"\n    Verify suite2p status consistency between database and file system.\n\n    Checks for discrepancies where:\n    - Database says suite2p is done but files don't exist\n    - Files exist but database says suite2p wasn't done\n\n    Parameters\n    ----------\n    with_database_update : bool, default=False\n        If True, automatically corrects database entries when discrepancies are found.\n    return_check : bool, default=False\n        If True, returns a boolean indicating whether any discrepancies were found.\n\n    Returns\n    -------\n    bool or None\n        If return_check is True, returns True if any discrepancies were found,\n        False otherwise. Returns None if return_check is False.\n    \"\"\"\n    df = self.get_table()\n\n    # Check sessions where database says suite2p is done but files don't exist\n    check_s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n    checked_not_done = check_s2p_done.apply(lambda row: not (self.make_b2session(row).s2p_path.exists()), axis=1)\n    not_actually_done = check_s2p_done[checked_not_done]\n\n    # Check sessions where files exist but database says suite2p wasn't done\n    check_s2p_needed = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n    checked_not_needed = check_s2p_needed.apply(lambda row: self.make_b2session(row).s2p_path.exists(), axis=1)\n    not_actually_needed = check_s2p_needed[checked_not_needed]\n\n    # Print database errors to workspace\n    for idx, row in not_actually_done.iterrows():\n        print(f\"Database said suite2p has been ran, but it actually hasn't: {self.make_b2session(row).session_print()}\")\n    for idx, row in not_actually_needed.iterrows():\n        print(f\"Database said suite2p didn't run, but it already did: {self.make_b2session(row).session_print()}\")\n\n    # If with_database_update is True, correct the database\n    if with_database_update:\n        for idx, row in not_actually_done.iterrows():\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), False)\n\n        for idx, row in not_actually_needed.iterrows():\n            with self.open_cursor(commit_changes=True) as cursor:\n                cursor.execute(self.create_update_statement(\"suite2p\", row[self.uid]), True)\n\n    # If return_check is requested, return True if any records were invalid\n    if return_check:\n        return checked_not_done.any() or checked_not_needed.any()\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.gen_sessions","title":"<code>gen_sessions(session_params={}, **kw_conditions)</code>","text":"<p>Generate sessions matching conditions.</p> <p>Parameters:</p> Name Type Description Default <code>session_params</code> <code>dict</code> <p>Additional parameters to pass to the session constructor when creating B2Session objects. These are passed through to B2Session.create().</p> <code>{}</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>generator[B2Session]</code> <p>Generator of sessions matching the conditions.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def gen_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; Generator[B2Session, None, None]:\n    \"\"\"Generate sessions matching conditions.\n\n    Parameters\n    ----------\n    session_params : dict, default={}\n        Additional parameters to pass to the session constructor when creating\n        B2Session objects. These are passed through to B2Session.create().\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    generator[B2Session]\n        Generator of sessions matching the conditions.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    for _, row in df.iterrows():\n        yield B2Session.create(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.iter_sessions","title":"<code>iter_sessions(session_params={}, **kw_conditions)</code>","text":"<p>Iterate over sessions matching conditions.</p> <p>Parameters:</p> Name Type Description Default <code>session_params</code> <code>dict</code> <p>Additional parameters to pass to the session constructor when creating B2Session objects. These are passed through to B2Session.create().</p> <code>{}</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax. Examples: <code>mouseName='ATL028'</code>, <code>imaging=True</code>, <code>sessionID=(5, '&gt;')</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>sessions</code> <code>list[B2Session]</code> <p>List of sessions matching the conditions.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def iter_sessions(self, session_params: Dict[str, Any] = {}, **kw_conditions: Any) -&gt; List[B2Session]:\n    \"\"\"Iterate over sessions matching conditions.\n\n    Parameters\n    ----------\n    session_params : dict, default={}\n        Additional parameters to pass to the session constructor when creating\n        B2Session objects. These are passed through to B2Session.create().\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n        Examples: ``mouseName='ATL028'``, ``imaging=True``, ``sessionID=(5, '&gt;')``\n\n    Returns\n    -------\n    sessions : list[B2Session]\n        List of sessions matching the conditions.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    sessions = []\n    for _, row in df.iterrows():\n        sessions.append(B2Session.create(row[\"mouseName\"], row[\"sessionDate\"], str(row[\"sessionID\"]), params=session_params))\n    return sessions\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.iter_sessions_need_red_cell_qc","title":"<code>iter_sessions_need_red_cell_qc(**kw_conditions)</code>","text":"<p>Get list of sessions that require red cell quality control.</p> <p>Parameters:</p> Name Type Description Default <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[B2Session]</code> <p>List of session objects that need red cell QC.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def iter_sessions_need_red_cell_qc(self, **kw_conditions: Any) -&gt; List[B2Session]:\n    \"\"\"\n    Get list of sessions that require red cell quality control.\n\n    Parameters\n    ----------\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    list[B2Session]\n        List of session objects that need red cell QC.\n    \"\"\"\n    df = self.needs_red_cell_qc(return_df=True, **kw_conditions)\n    sessions = []\n    for idx, row in df.iterrows():\n        sessions.append(self.make_b2session(row))\n    return sessions\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.make_b2registration","title":"<code>make_b2registration(row, opts)</code>","text":"<p>Create a B2Registration object from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <code>opts</code> <code>B2RegistrationOpts</code> <p>Registration options to use for the session.</p> required <p>Returns:</p> Type Description <code>B2Registration</code> <p>Registration object initialized with session data and options.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def make_b2registration(self, row: pd.Series, opts: B2RegistrationOpts) -&gt; B2Registration:\n    \"\"\"\n    Create a B2Registration object from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n    opts : B2RegistrationOpts\n        Registration options to use for the session.\n\n    Returns\n    -------\n    B2Registration\n        Registration object initialized with session data and options.\n    \"\"\"\n    mouse_name, session_date, session_id = self.session_name(row)\n    return B2Registration(mouse_name, session_date, session_id, opts)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.make_b2session","title":"<code>make_b2session(row)</code>","text":"<p>Create a B2Session object from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <p>Returns:</p> Type Description <code>B2Session</code> <p>Session object initialized with data from the record.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def make_b2session(self, row: pd.Series) -&gt; B2Session:\n    \"\"\"\n    Create a B2Session object from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n\n    Returns\n    -------\n    B2Session\n        Session object initialized with data from the record.\n    \"\"\"\n    mouse_name, session_date, session_id = self.session_name(row)\n    return B2Session.create(mouse_name, session_date, session_id)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.needs_red_cell_qc","title":"<code>needs_red_cell_qc(return_df=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need red cell quality control.</p> <p>Parameters:</p> Name Type Description Default <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need red cell QC. Sessions must have imaging, suite2p processing, and registration completed. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def needs_red_cell_qc(self, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need red cell quality control.\n\n    Parameters\n    ----------\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need red cell QC.\n        Sessions must have imaging, suite2p processing, and registration completed.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"vrRegistration\"] == True) &amp; (df[\"redCellQC\"] == False)]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            print(f\"Database indicates that redCellQC has not been performed for session: {self.make_b2session(row).session_print()}\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.needs_registration","title":"<code>needs_registration(skip_errors=True, return_df=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need registration preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>skip_errors</code> <code>bool</code> <p>If True, exclude sessions that had registration errors.</p> <code>True</code> <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need registration. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def needs_registration(self, skip_errors: bool = True, return_df: bool = True, **kw_conditions: Any) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need registration preprocessing.\n\n    Parameters\n    ----------\n    skip_errors : bool, default=True\n        If True, exclude sessions that had registration errors.\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need registration.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    if skip_errors:\n        df = df[df[\"vrRegistrationError\"] == False]\n    df = df[df[\"vrRegistration\"] == False]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            session = self.make_b2session(row)\n            print(f\"Session needs registration: {session.session_print()}\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.needs_s2p","title":"<code>needs_s2p(needs_qc=False, return_df=True, print_targets=True, **kw_conditions)</code>","text":"<p>Get or print sessions that need suite2p processing or quality control.</p> <p>Parameters:</p> Name Type Description Default <code>needs_qc</code> <code>bool</code> <p>If False, returns/prints sessions that need suite2p processing. If True, returns/prints sessions that need suite2p quality control.</p> <code>False</code> <code>return_df</code> <code>bool</code> <p>If True, returns a DataFrame. If False, prints the sessions instead.</p> <code>True</code> <code>print_targets</code> <code>bool</code> <p>If True and return_df=False, prints suite2p target information for sessions needing processing.</p> <code>True</code> <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC. If return_df=False, returns None and prints the sessions instead.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def needs_s2p(\n    self, needs_qc: bool = False, return_df: bool = True, print_targets: bool = True, **kw_conditions: Any\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Get or print sessions that need suite2p processing or quality control.\n\n    Parameters\n    ----------\n    needs_qc : bool, default=False\n        If False, returns/prints sessions that need suite2p processing.\n        If True, returns/prints sessions that need suite2p quality control.\n    return_df : bool, default=True\n        If True, returns a DataFrame. If False, prints the sessions instead.\n    print_targets : bool, default=True\n        If True and return_df=False, prints suite2p target information for sessions needing processing.\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        If return_df=True, returns DataFrame containing sessions that need suite2p processing or QC.\n        If return_df=False, returns None and prints the sessions instead.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    if needs_qc:\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"suite2pQC\"] == False)]\n    else:\n        df = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == False)]\n\n    if return_df:\n        return df\n    else:\n        for idx, row in df.iterrows():\n            session = self.make_b2session(row)\n            if needs_qc:\n                print(f\"Database indicates that suite2p has been run but not QC'd: {session.session_print()}\")\n            else:\n                print(f\"Database indicates that suite2p has not been run: {session.session_print()}\")\n                if print_targets:\n                    mouse_name, session_date, session_id = self.session_name(row)\n                    s2p_targets(mouse_name, session_date, session_id)\n                    print(\"\")\n        return None\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.print_registration_errors","title":"<code>print_registration_errors(**kw_conditions)</code>","text":"<p>Print registration errors for sessions that failed registration.</p> <p>Parameters:</p> Name Type Description Default <code>**kw_conditions</code> <code>dict</code> <p>Additional filtering conditions passed to get_table(). See get_table() documentation for filtering syntax.</p> <code>{}</code> Source code in <code>vrAnalysis/database.py</code> <pre><code>def print_registration_errors(self, **kw_conditions: Any) -&gt; None:\n    \"\"\"\n    Print registration errors for sessions that failed registration.\n\n    Parameters\n    ----------\n    **kw_conditions : dict, optional\n        Additional filtering conditions passed to get_table().\n        See get_table() documentation for filtering syntax.\n    \"\"\"\n    df = self.get_table(**kw_conditions)\n    for idx, row in df[df[\"vrRegistrationError\"] == True].iterrows():\n        print(f\"{'/'.join(self.session_name(row))} had error: {row['vrRegistrationException']}\")\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.register_record","title":"<code>register_record(record, raise_exception=False, imaging=None)</code>","text":"<p>Perform registration preprocessing for a single session record.</p> <p>Creates a B2Registration object and runs preprocessing. Updates the database with success/failure status and error information if applicable.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Series</code> <p>Database record containing session information.</p> required <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting. If None (default), uses the value from the database record. If True or False, overrides the database value.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (success, data_size) where: - success: bool indicating if registration succeeded - data_size: int size in bytes of registered oneData (0 if failed)</p> Notes <p>On failure, clears all oneData files and updates database error fields. On success, updates registration status and date in the database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def register_record(self, record: pd.Series, raise_exception: bool = False, imaging: Optional[bool] = None) -&gt; Tuple[bool, int]:\n    \"\"\"\n    Perform registration preprocessing for a single session record.\n\n    Creates a B2Registration object and runs preprocessing. Updates the database\n    with success/failure status and error information if applicable.\n\n    Parameters\n    ----------\n    record : pandas.Series\n        Database record containing session information.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting. If None (default), uses the value from the\n        database record. If True or False, overrides the database value.\n\n    Returns\n    -------\n    tuple\n        Tuple of (success, data_size) where:\n        - success: bool indicating if registration succeeded\n        - data_size: int size in bytes of registered oneData (0 if failed)\n\n    Notes\n    -----\n    On failure, clears all oneData files and updates database error fields.\n    On success, updates registration status and date in the database.\n    \"\"\"\n    opts = B2RegistrationOpts()\n    opts.imaging = bool(imaging) if imaging is not None else bool(record[\"imaging\"])\n    opts.facecam = bool(record[\"faceCamera\"])\n    opts.vrBehaviorVersion = record[\"vrBehaviorVersion\"]\n    b2reg = self.make_b2registration(record, opts)\n    try:\n        print(f\"Registering data for session: {b2reg.session_print()}\")\n        b2reg.register()\n    except Exception as ex:\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), True)\n            cursor.execute(\n                self.create_update_statement(\"vrRegistrationException\", record[self.uid]),\n                str(ex),\n            )\n        if raise_exception:\n            raise ex\n        print(f\"The following exception was raised when trying to preprocess session: {b2reg.session_print()}. Clearing all oneData.\")\n        b2reg.clear_one_data(certainty=True)\n        error_print(f\"Last traceback: {traceback.extract_tb(ex.__traceback__, limit=-1)}\")\n        error_print(f\"Exception: {ex}\")\n        # If failed, return (False, 0)\n        out = (False, 0)\n    else:\n        with self.open_cursor(commit_changes=True) as cursor:\n            # Tell the database that vrRegistration was performed and the time of processing\n            cursor.execute(self.create_update_statement(\"vrRegistration\", record[self.uid]), True)\n            cursor.execute(self.create_update_statement(\"vrRegistrationError\", record[self.uid]), False)\n            cursor.execute(self.create_update_statement(\"vrRegistrationException\", record[self.uid]), \"\")\n            cursor.execute(\n                self.create_update_statement(\"vrRegistrationDate\", record[self.uid]),\n                datetime.now(),\n            )\n        # If successful, return (True, size of registered oneData)\n        out = (True, sum([one_file.stat().st_size for one_file in b2reg.get_saved_one()]))\n        print(f\"Session {b2reg.session_print()} registered with {readable_bytes(out[1])} oneData.\")\n    finally:\n        del b2reg\n    return out\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.register_sessions","title":"<code>register_sessions(max_data=30000000000.0, skip_errors=True, raise_exception=False, imaging=None)</code>","text":"<p>Register multiple sessions that need registration.</p> <p>Processes sessions in batches, stopping when the total data size limit is reached. Provides progress updates including accumulated data size and estimates.</p> <p>Parameters:</p> Name Type Description Default <code>max_data</code> <code>float</code> <p>Maximum total data size (in bytes) to process before stopping. Default is 30 GB.</p> <code>30e9</code> <code>skip_errors</code> <code>bool</code> <p>If True, skip sessions that had previous registration errors.</p> <code>True</code> <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting for all sessions. If None (default), uses the value from each session's database record. If True or False, overrides the database value for all sessions to enable or disable imaging processing.</p> <code>None</code> Notes <p>Prints progress information including: - Accumulated oneData registered - Average data size per session - Estimated remaining data to process</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def register_sessions(\n    self,\n    max_data: float = 30e9,\n    skip_errors: bool = True,\n    raise_exception: bool = False,\n    imaging: Optional[bool] = None,\n) -&gt; None:\n    \"\"\"\n    Register multiple sessions that need registration.\n\n    Processes sessions in batches, stopping when the total data size limit is reached.\n    Provides progress updates including accumulated data size and estimates.\n\n    Parameters\n    ----------\n    max_data : float, default=30e9\n        Maximum total data size (in bytes) to process before stopping.\n        Default is 30 GB.\n    skip_errors : bool, default=True\n        If True, skip sessions that had previous registration errors.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting for all sessions. If None (default), uses the\n        value from each session's database record. If True or False, overrides the\n        database value for all sessions to enable or disable imaging processing.\n\n    Notes\n    -----\n    Prints progress information including:\n    - Accumulated oneData registered\n    - Average data size per session\n    - Estimated remaining data to process\n    \"\"\"\n    count_sessions = 0\n    total_one_data = 0.0\n    df_to_register = self.needs_registration(skip_errors=skip_errors)\n\n    for idx, (_, row) in enumerate(df_to_register.iterrows()):\n        if total_one_data &gt; max_data:\n            print(f\"\\nMax data limit reached. Total processed: {readable_bytes(total_one_data)}. Limit: {readable_bytes(max_data)}\")\n            return\n        print(\"\")\n        out = self.register_record(row, raise_exception=raise_exception, imaging=imaging)\n        if out[0]:\n            count_sessions += 1  # count successful sessions\n            total_one_data += out[1]  # accumulated oneData registered\n            estimate_remaining = len(df_to_register) - idx - 1\n            print(\n                f\"Accumulated oneData registered: {readable_bytes(total_one_data)}. \"\n                f\"Averaging: {readable_bytes(total_one_data/count_sessions)} / session. \"\n                f\"Estimate remaining: {readable_bytes(total_one_data/count_sessions*estimate_remaining)}\"\n            )\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.register_single_session","title":"<code>register_single_session(mouse_name, session_date, session_id, raise_exception=False, imaging=None)</code>","text":"<p>Register a single session by its identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Mouse name identifier.</p> required <code>session_date</code> <code>str</code> <p>Session date in 'YYYY-MM-DD' format.</p> required <code>session_id</code> <code>str or int</code> <p>Session ID.</p> required <code>raise_exception</code> <code>bool</code> <p>If True, raises exceptions instead of handling them silently.</p> <code>False</code> <code>imaging</code> <code>bool</code> <p>Override the imaging setting. If None (default), uses the value from the database record. If True or False, overrides the database value to enable or disable imaging processing during registration.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool or None</code> <p>True if registration succeeded, False if failed, None if session not found.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def register_single_session(\n    self,\n    mouse_name: str,\n    session_date: str,\n    session_id: Union[str, int],\n    raise_exception: bool = False,\n    imaging: Optional[bool] = None,\n) -&gt; Optional[bool]:\n    \"\"\"\n    Register a single session by its identifiers.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Mouse name identifier.\n    session_date : str\n        Session date in 'YYYY-MM-DD' format.\n    session_id : str or int\n        Session ID.\n    raise_exception : bool, default=False\n        If True, raises exceptions instead of handling them silently.\n    imaging : bool, optional\n        Override the imaging setting. If None (default), uses the value from the\n        database record. If True or False, overrides the database value to enable\n        or disable imaging processing during registration.\n\n    Returns\n    -------\n    bool or None\n        True if registration succeeded, False if failed, None if session not found.\n    \"\"\"\n    record = self.get_record(mouse_name, session_date, session_id)\n    if record is None:\n        print(f\"Session {'/'.join([mouse_name, session_date, session_id])} is not in the database\")\n        return\n    out = self.register_record(record, raise_exception=raise_exception, imaging=imaging)\n    return out[0]\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.session_name","title":"<code>session_name(row)</code>","text":"<p>Extract session identifiers from a database record.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Database record containing session information.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple of (mouse_name, session_date, session_id) where session_date is formatted as 'YYYY-MM-DD' and session_id is converted to string.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def session_name(self, row: pd.Series) -&gt; Tuple[str, str, str]:\n    \"\"\"\n    Extract session identifiers from a database record.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        Database record containing session information.\n\n    Returns\n    -------\n    tuple\n        Tuple of (mouse_name, session_date, session_id) where session_date is\n        formatted as 'YYYY-MM-DD' and session_id is converted to string.\n    \"\"\"\n    mouse_name = row[\"mouseName\"]\n    session_date = row[\"sessionDate\"].strftime(\"%Y-%m-%d\")\n    session_id = str(row[\"sessionID\"])\n    return mouse_name, session_date, session_id\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.set_red_cell_qc","title":"<code>set_red_cell_qc(mouse_name, date_string, session_id, state=True)</code>","text":"<p>Set the red cell QC status for a specific session.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Mouse name identifier.</p> required <code>date_string</code> <code>str</code> <p>Session date in 'YYYY-MM-DD' format.</p> required <code>session_id</code> <code>str or int</code> <p>Session ID.</p> required <code>state</code> <code>bool</code> <p>Red cell QC status to set. If True, also sets the QC date to now.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful, False otherwise.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def set_red_cell_qc(self, mouse_name: str, date_string: str, session_id: Union[str, int], state: bool = True) -&gt; bool:\n    \"\"\"\n    Set the red cell QC status for a specific session.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Mouse name identifier.\n    date_string : str\n        Session date in 'YYYY-MM-DD' format.\n    session_id : str or int\n        Session ID.\n    state : bool, default=True\n        Red cell QC status to set. If True, also sets the QC date to now.\n\n    Returns\n    -------\n    bool\n        True if update was successful, False otherwise.\n    \"\"\"\n    record = self.get_record(mouse_name, date_string, session_id)\n    if record is None:\n        print(f\"Could not find session {mouse_name}/{date_string}/{session_id} in database.\")\n        return False\n\n    try:\n        with self.open_cursor(commit_changes=True) as cursor:\n            cursor.execute(self.create_update_statement(\"redCellQC\", record[self.uid]), state)\n            if state == True:\n                # If setting red cell QC to true, add the date\n                cursor.execute(\n                    self.create_update_statement(\"redCellQCDate\", record[self.uid]),\n                    datetime.now(),\n                )\n            else:\n                # Otherwise remove the date\n                cursor.execute(self.create_update_statement(\"redCellQCDate\", record[self.uid]), \"\")\n        return True\n\n    except Exception as ex:\n        print(f\"Failed to update database for session: {mouse_name}/{date_string}/{session_id}\")\n        print(f\"Error: {ex}\")\n        return False\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.update_red_cell_qc_date_time","title":"<code>update_red_cell_qc_date_time()</code>","text":"<p>Update red cell QC dates in the database based on file modification times.</p> <p>For all sessions where red cell QC is complete, finds the most recent modification time of relevant red cell QC files and updates the redCellQCDate field in the database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def update_red_cell_qc_date_time(self) -&gt; None:\n    \"\"\"\n    Update red cell QC dates in the database based on file modification times.\n\n    For all sessions where red cell QC is complete, finds the most recent\n    modification time of relevant red cell QC files and updates the\n    redCellQCDate field in the database.\n    \"\"\"\n    relevant_one_files = [\n        \"mpciROIs.redCellIdx.npy\",\n        \"mpciROIs.redCellManualAssignment.npy\",\n        \"parametersRed*\",  # wildcard because there are multiple possibilities\n    ]\n\n    df = self.get_table()\n    red_cell_qc_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True) &amp; (df[\"redCellQC\"] == True)]\n    uids = red_cell_qc_done[self.uid].tolist()\n    rc_edit_date = []\n    for idx, row in red_cell_qc_done.iterrows():\n        session = self.make_b2session(row)  # create vrSession to point to session folder\n        c_latest_mod = 0\n        for f in relevant_one_files:\n            for file in session.one_path.rglob(f):\n                c_latest_mod = max(file.stat().st_mtime, c_latest_mod)\n        c_date_time = datetime.fromtimestamp(c_latest_mod)\n        rc_edit_date.append(c_date_time)  # get red cell QC file modification date\n\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(self.create_update_many_statement(\"redCellQCDate\"), zip(rc_edit_date, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.SessionDatabase.update_s2p_date_time","title":"<code>update_s2p_date_time()</code>","text":"<p>Update suite2p creation dates in the database based on file modification times.</p> <p>For all sessions where suite2p processing is complete, finds the most recent file modification time in the suite2p output directory and updates the suite2pDate field in the database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def update_s2p_date_time(self) -&gt; None:\n    \"\"\"\n    Update suite2p creation dates in the database based on file modification times.\n\n    For all sessions where suite2p processing is complete, finds the most recent\n    file modification time in the suite2p output directory and updates the\n    suite2pDate field in the database.\n    \"\"\"\n    df = self.get_table()\n    s2p_done = df[(df[\"imaging\"] == True) &amp; (df[\"suite2p\"] == True)]\n    uids = s2p_done[self.uid].tolist()\n    s2p_creation_date = []\n    for idx, row in s2p_done.iterrows():\n        session = self.make_b2session(row)  # create vrSession to point to session folder\n        c_latest_mod = 0\n        for p in session.s2p_path.rglob(\"*\"):\n            if not (p.is_dir()):\n                c_latest_mod = max(p.stat().st_mtime, c_latest_mod)\n        c_date_time = datetime.fromtimestamp(c_latest_mod)\n        s2p_creation_date.append(c_date_time)  # get suite2p path creation date\n\n    with self.open_cursor(commit_changes=True) as cursor:\n        cursor.executemany(self.create_update_many_statement(\"suite2pDate\"), zip(s2p_creation_date, uids))\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.get_database","title":"<code>get_database(db_name)</code>","text":"<p>Retrieve an appropriate database object instance.</p> <p>This function retrieves metadata for the specified database and instantiates the appropriate database class (BaseDatabase or a subclass like SessionDatabase).</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database to retrieve.</p> required <p>Returns:</p> Type Description <code>BaseDatabase or SessionDatabase</code> <p>An instance of the appropriate database class as specified in the metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the constructor specified in metadata is not a subclass of BaseDatabase.</p> See Also <p>get_database_metadata : Retrieve metadata for a database.</p> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_database(db_name: str) -&gt; Union[\"BaseDatabase\", \"SessionDatabase\"]:\n    \"\"\"\n    Retrieve an appropriate database object instance.\n\n    This function retrieves metadata for the specified database and instantiates\n    the appropriate database class (BaseDatabase or a subclass like SessionDatabase).\n\n    Parameters\n    ----------\n    db_name : str\n        The name of the database to retrieve.\n\n    Returns\n    -------\n    BaseDatabase or SessionDatabase\n        An instance of the appropriate database class as specified in the metadata.\n\n    Raises\n    ------\n    ValueError\n        If the constructor specified in metadata is not a subclass of BaseDatabase.\n\n    See Also\n    --------\n    get_database_metadata : Retrieve metadata for a database.\n    \"\"\"\n    metadata = get_database_metadata(db_name)\n    if \"constructor\" in metadata:\n        if issubclass(metadata[\"constructor\"], BaseDatabase):\n            constructor = metadata[\"constructor\"]  # get class constructor method for this database\n        else:\n            raise ValueError(f\"{metadata['constructor']} must be a subclass of the `BaseDatabase` class!\")\n    else:\n        constructor = BaseDatabase\n    return constructor(db_name)\n</code></pre>"},{"location":"api/database/#vrAnalysis.database.get_database_metadata","title":"<code>get_database_metadata(db_name)</code>","text":"<p>Retrieve metadata for a specified database.</p> <p>This function retrieves metadata for a specified database from the <code>dbdict</code> dictionary. The <code>dbdict</code> dictionary contains the database paths, names, and primary table name.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>The name of the database for which to retrieve metadata.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing metadata for the specified database. It requires the following keys:     'db_path': path to the database file     'db_name': name of the database file     'db_ext': extension of the database file     'table_name': name of the table to use     'uid': name of the field defining a unique ID for each row in the table     'backup_path': path to the database backup (None if there isn't one)     'unique_fields': list of names of fields for which there should only be one                     database row per combination of the values in unique_fields                     note: assumes string, but make it a tuple for different types     'default_conditions': dictionary containing key-value pairs of any default                          conditions to filter by when retrieving table data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>db_name</code> is not recognized as a valid database name.</p> Example <p>metadata = get_database_metadata('vrSessions') print(metadata['db_path']) 'C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement' print(metadata['db_name']) 'vrDatabase'</p> Notes <ul> <li>The <code>dbdict</code> dictionary contains metadata for recognized databases.</li> <li>Edit this function to specify the path, database, and primary table on your computer.</li> </ul> Source code in <code>vrAnalysis/database.py</code> <pre><code>def get_database_metadata(db_name: str) -&gt; dict:\n    \"\"\"\n    Retrieve metadata for a specified database.\n\n    This function retrieves metadata for a specified database from the `dbdict` dictionary.\n    The `dbdict` dictionary contains the database paths, names, and primary table name.\n\n    Parameters\n    ----------\n    db_name : str\n        The name of the database for which to retrieve metadata.\n\n    Returns\n    -------\n    dict\n        A dictionary containing metadata for the specified database.\n        It requires the following keys:\n            'db_path': path to the database file\n            'db_name': name of the database file\n            'db_ext': extension of the database file\n            'table_name': name of the table to use\n            'uid': name of the field defining a unique ID for each row in the table\n            'backup_path': path to the database backup (None if there isn't one)\n            'unique_fields': list of names of fields for which there should only be one\n                            database row per combination of the values in unique_fields\n                            note: assumes string, but make it a tuple for different types\n            'default_conditions': dictionary containing key-value pairs of any default\n                                 conditions to filter by when retrieving table data\n\n    Raises\n    ------\n    ValueError\n        If the provided `db_name` is not recognized as a valid database name.\n\n    Example\n    -------\n    &gt;&gt;&gt; metadata = get_database_metadata('vrSessions')\n    &gt;&gt;&gt; print(metadata['db_path'])\n    'C:\\\\Users\\\\andrew\\\\Documents\\\\localData\\\\vrDatabaseManagement'\n    &gt;&gt;&gt; print(metadata['db_name'])\n    'vrDatabase'\n\n    Notes\n    -----\n    - The `dbdict` dictionary contains metadata for recognized databases.\n    - Edit this function to specify the path, database, and primary table on your computer.\n    \"\"\"\n\n    dbdict = {\n        \"vrSessions\": {\n            \"db_path\": r\"C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"sessiondb\",\n            \"uid\": \"uSessionID\",\n            \"backup_path\": r\"D:\\localData\\vrDatabaseManagement\",\n            \"unique_fields\": [(\"mouseName\", str), (\"sessionDate\", datetime), (\"sessionID\", int)],\n            \"default_conditions\": {\n                \"sessionQC\": True,\n            },\n            \"constructor\": SessionDatabase,\n        },\n        \"vrMice\": {\n            \"db_path\": r\"C:\\Users\\andrew\\Documents\\localData\\vrDatabaseManagement\",\n            \"db_name\": \"vrDatabase\",\n            \"db_ext\": \".accdb\",\n            \"table_name\": \"mousedb\",\n            \"uid\": \"uMouseID\",\n            \"backup_path\": r\"D:\\localData\\vrDatabaseManagement\",\n            \"unique_fields\": [(\"mouseName\", str)],\n            \"default_conditions\": {},\n            \"constructor\": BaseDatabase,\n        },\n    }\n    if db_name not in dbdict.keys():\n        raise ValueError(f\"Did not recognize database={db_name}, valid database names are: {[key for key in dbdict.keys()]}\")\n    return dbdict[db_name]\n</code></pre>"},{"location":"api/processors/","title":"Processors API Reference","text":"<p>The processors module provides data processing pipelines that transform session data into analysis-ready formats. The main component is the <code>SpkmapProcessor</code>, which creates spatial maps of neural activity, behavioral occupancy, and speed.</p>"},{"location":"api/processors/#overview","title":"Overview","text":"<p>The processors module contains:</p> <ul> <li>SpkmapProcessor: Main class for processing spike maps from session data</li> <li>Maps: Container class for occupancy, speed, and spike maps</li> <li>Reliability: Container class for reliability measurements</li> <li>SpkmapParams: Configuration parameters for spike map processing</li> </ul>"},{"location":"api/processors/#core-classes","title":"Core Classes","text":"<p>options:     show_root_heading: true     show_root_toc_entry: true     heading_level: 3</p> <p>options:     show_root_heading: true     show_root_toc_entry: true     heading_level: 3</p> <p>options:     show_root_heading: true     show_root_toc_entry: true     heading_level: 3</p> <p>options:     show_root_heading: true     show_root_toc_entry: true     heading_level: 3</p>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor","title":"<code>SpkmapProcessor</code>  <code>dataclass</code>","text":"<p>Class for processing and caching spike maps from session data</p> <p>NOTES ON ENGINEERING: I want the variables required for processing spkmaps to be properties (@property) that have hidden attributes for caching. Therefore, we can use the property method to get the attribute and each property method can do whatever processing is needed for that attribute. (Uh, duh). Time to get modern. lol.</p> <p>Right now I've almost got the register_spkmaps method working again (not tested yet) but now is when the dataclass refactoring comes in. 1. Make it possible to separate the occmap from the spkmap loading.    - do so by making the preliminary variables properties with caching 2. Consider how to implement smoothing then correctMap functionality -- it should    be possible to do this in a way that allows me to iteratively try different    parameterizations without having to go through the whole pipeline again. 3. Consider how / when to implement reliability measures. In PCSS, they're done all    right there with get_spkmaps. But it's probably not always necessary and can    actually take a bit of time? It would also be nice to save reliability scores for    the neurons... but then we'd also need an independent params saving system for them. 4. Re: the point above, I wonder if the one.data loading system is ideal or if I should    use a more explicit and dedicated SpkmapProcessor saving / loading system.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass SpkmapProcessor:\n    \"\"\"Class for processing and caching spike maps from session data\n\n    NOTES ON ENGINEERING:\n    I want the variables required for processing spkmaps to be properties (@property)\n    that have hidden attributes for caching. Therefore, we can use the property method\n    to get the attribute and each property method can do whatever processing is needed\n    for that attribute. (Uh, duh). Time to get modern. lol.\n\n    Right now I've almost got the register_spkmaps method working again (not tested yet)\n    but now is when the dataclass refactoring comes in.\n    1. Make it possible to separate the occmap from the spkmap loading.\n       - do so by making the preliminary variables properties with caching\n    2. Consider how to implement smoothing then correctMap functionality -- it should\n       be possible to do this in a way that allows me to iteratively try different\n       parameterizations without having to go through the whole pipeline again.\n    3. Consider how / when to implement reliability measures. In PCSS, they're done all\n       right there with get_spkmaps. But it's probably not always necessary and can\n       actually take a bit of time? It would also be nice to save reliability scores for\n       the neurons... but then we'd also need an independent params saving system for them.\n    4. Re: the point above, I wonder if the one.data loading system is ideal or if I should\n       use a more explicit and dedicated SpkmapProcessor saving / loading system.\n    \"\"\"\n\n    session: Union[SessionData, B2Session, SessionToSpkmapProtocol]\n    params: SpkmapParams = field(default_factory=SpkmapParams, repr=False)\n    data_cache: dict = field(default_factory=dict, repr=False, init=False)\n\n    def __post_init__(self):\n        # Check if the session provided is compatible with SpkmapProcessing\n        if not isinstance(self.session, SessionData):\n            raise ValueError(f\"session must be a SessionData instance, not {type(self.session)}\")\n        # (Don't check if it's a SessionToSpkmapProtocol because hasattr() will call properties which loads data...)\n\n        # We need to handle the case where params is a dictionary of partial updates to the default params\n        self.params = helpers.resolve_dataclass(self.params, SpkmapParams)\n\n    def cached_dependencies(self, data_type: str) -&gt; List[str]:\n        \"\"\"Get the parameter dependencies for a given data type.\n\n        Parameters\n        ----------\n        data_type : str\n            Type of cached data (\"raw_maps\", \"processed_maps\", \"env_maps\", or \"reliability\").\n\n        Returns\n        -------\n        list of str\n            List of parameter names that affect the cache validity for this data type.\n        \"\"\"\n        if data_type == \"raw_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\"]\n        elif data_type == \"processed_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\"]\n        elif data_type == \"env_maps\":\n            return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\", \"full_trial_flexibility\"]\n        elif data_type == \"reliability\":\n            return [\n                \"dist_step\",\n                \"speed_threshold\",\n                \"speed_max_allowed\",\n                \"standardize_spks\",\n                \"smooth_width\",\n                \"full_trial_flexibility\",\n                \"reliability_method\",\n            ]\n        # Otherwise just return all params\n        return list(self.params.__dict__.keys())\n\n    def show_cache(self, data_type: Optional[str] = None) -&gt; None:\n        \"\"\"Helper function that scrapes the cache directory and shows cached files\n\n        Parameters\n        ----------\n        data_type: Optional[str] = None\n            Indicate a data type to filter which parts of the cache to show\n\n        Notes\n        -----\n        Prints a formatted table showing cache information including data_type, size,\n        parameters, and modification date. If no cache directory exists, prints a message.\n        \"\"\"\n        import os\n        from datetime import datetime\n\n        # Get the base cache directory\n        base_cache_dir = self.cache_directory()\n\n        if not base_cache_dir.exists():\n            print(f\"No cache directory found at: {base_cache_dir}\")\n            return\n\n        # Collect information about all cache files\n        cache_info = []\n\n        # Define the data types to check\n        if data_type is not None:\n            data_types_to_check = [data_type]\n        else:\n            data_types_to_check = [\"raw_maps\", \"processed_maps\", \"env_maps\", \"reliability\"]\n\n        for dt in data_types_to_check:\n            cache_dir = self.cache_directory(dt)\n            if not cache_dir.exists():\n                continue\n\n            # Find all parameter files (they define what caches exist)\n            param_files = list(cache_dir.glob(\"params_*.npz\"))\n\n            for param_file in param_files:\n                # Extract the hash from the filename\n                params_hash = param_file.stem.replace(\"params_\", \"\")\n\n                # Load the parameters\n                try:\n                    cached_params = dict(np.load(param_file))\n                    param_str = \", \".join([f\"{k}={v}\" for k, v in cached_params.items()])\n                except Exception as e:\n                    param_str = f\"Error loading params: {e}\"\n\n                # Get file modification time\n                mod_time = datetime.fromtimestamp(param_file.stat().st_mtime)\n                date_str = mod_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n                # Calculate total size of all related cache files\n                total_size = param_file.stat().st_size\n\n                if dt in [\"raw_maps\", \"processed_maps\"]:\n                    # For maps, look for data files for each map type\n                    for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                        data_file = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                        if data_file.exists():\n                            total_size += data_file.stat().st_size\n\n                elif dt == \"env_maps\":\n                    # For env_maps, look for environment file and individual environment data files\n                    env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                    if env_file.exists():\n                        total_size += env_file.stat().st_size\n                        # Load environments to find all data files\n                        try:\n                            environments = np.load(env_file)\n                            for env in environments:\n                                for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                                    data_file = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                                    if data_file.exists():\n                                        total_size += data_file.stat().st_size\n                        except Exception:\n                            pass  # Continue even if we can't load environments\n\n                elif dt == \"reliability\":\n                    # For reliability, look for environments and reliability data files\n                    env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                    rel_file = cache_dir / f\"data_reliability_{params_hash}.npy\"\n                    if env_file.exists():\n                        total_size += env_file.stat().st_size\n                    if rel_file.exists():\n                        total_size += rel_file.stat().st_size\n\n                # Convert size to human readable format\n                size_str = self._format_file_size(total_size)\n\n                cache_info.append(\n                    {\n                        \"data_type\": dt,\n                        \"size\": size_str,\n                        \"parameters\": param_str,\n                        \"date\": date_str,\n                        \"hash\": params_hash[:8],  # Show first 8 chars of hash\n                    }\n                )\n\n        if not cache_info:\n            print(\"No cache files found.\")\n            return\n\n        # Format the output as a table\n        output_lines = []\n        output_lines.append(\"Cache Files Summary\")\n        output_lines.append(\"=\" * 80)\n        output_lines.append(f\"{'Data Type':&lt;15} {'Size':&lt;10} {'Date':&lt;20} {'Hash':&lt;10} {'Parameters'}\")\n        output_lines.append(\"-\" * 80)\n\n        for info in cache_info:\n            output_lines.append(f\"{info['data_type']:&lt;15} {info['size']:&lt;10} {info['date']:&lt;20} \" f\"{info['hash']:&lt;10} {info['parameters']}\")\n\n        output_lines.append(\"-\" * 80)\n        output_lines.append(f\"Total cache entries: {len(cache_info)}\")\n\n        result = \"\\n\".join(output_lines)\n        print(result)\n\n    def _format_file_size(self, size_bytes: int) -&gt; str:\n        \"\"\"Convert bytes to human-readable format.\n\n        Parameters\n        ----------\n        size_bytes : int\n            Size in bytes.\n\n        Returns\n        -------\n        str\n            Human-readable size string (e.g., \"1.5 MB\").\n        \"\"\"\n        if size_bytes == 0:\n            return \"0 B\"\n\n        size_names = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n        import math\n\n        i = int(math.floor(math.log(size_bytes, 1024)))\n        p = math.pow(1024, i)\n        s = round(size_bytes / p, 2)\n        return f\"{s} {size_names[i]}\"\n\n    def cache_directory(self, data_type: Optional[str] = None) -&gt; Path:\n        \"\"\"Get the cache directory path for a given data type.\n\n        Parameters\n        ----------\n        data_type : str, optional\n            Type of cached data. If None, returns the base cache directory.\n            Default is None.\n\n        Returns\n        -------\n        Path\n            Path to the cache directory for the specified data type.\n        \"\"\"\n        if data_type is None:\n            return self.session.data_path / \"spkmaps\"\n        else:\n            folder_name = f\"{data_type}_{self.session.spks_type}\"\n            return self.session.data_path / \"spkmaps\" / folder_name\n\n    def dependent_params(self, data_type: str) -&gt; dict:\n        \"\"\"Get the dependent parameters for a given data type as a dictionary.\n\n        Parameters\n        ----------\n        data_type : str\n            Type of cached data.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping parameter names to their values for the given data type.\n        \"\"\"\n        return {k: getattr(self.params, k) for k in self.cached_dependencies(data_type)}\n\n    def _params_hash(self, data_type: str) -&gt; str:\n        \"\"\"Get the hash of the dependent parameters for a given data type.\n\n        Parameters\n        ----------\n        data_type : str\n            Type of cached data.\n\n        Returns\n        -------\n        str\n            SHA256 hash of the dependent parameters (as hexadecimal string).\n        \"\"\"\n        return hashlib.sha256(json.dumps(self.dependent_params(data_type), sort_keys=True).encode()).hexdigest()\n\n    def save_cache(self, data_type: str, data: Union[Maps, Reliability]) -&gt; None:\n        \"\"\"Save the cached parameters and data for a given data type.\n\n        Parameters\n        ----------\n        data_type : str\n            Type of data being cached (\"raw_maps\", \"processed_maps\", \"env_maps\", or \"reliability\").\n        data : Maps or Reliability\n            The data object to cache.\n\n        Notes\n        -----\n        Creates the cache directory if it doesn't exist. Saves parameters as an NPZ file\n        and data as NPY files, using a hash of the parameters in the filenames.\n        \"\"\"\n        cache_dir = self.cache_directory(data_type)\n        params_hash = self._params_hash(data_type)\n        cache_param_path = cache_dir / f\"params_{params_hash}.npz\"\n        if not cache_dir.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n        np.savez(cache_param_path, **self.dependent_params(data_type))\n        if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n            for mapname in Maps.map_types():\n                cache_data_path = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                np.save(cache_data_path, getattr(data, mapname))\n        elif data_type == \"env_maps\":\n            environments = data.environments\n            np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n            for ienv, env in enumerate(environments):\n                for mapname in Maps.map_types():\n                    cache_data_path = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                    np.save(cache_data_path, getattr(data, mapname)[ienv])\n        elif data_type == \"reliability\":\n            values = data.values\n            environments = data.environments\n            # don't need data.method because it's in params...\n            np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n            np.save(cache_dir / f\"data_reliability_{params_hash}.npy\", values)\n        else:\n            raise ValueError(f\"Unknown data type: {data_type}\")\n\n    def load_from_cache(self, data_type: str) -&gt; Tuple[Union[Maps, Reliability, None], bool]:\n        \"\"\"Load cached parameters and data for a given data type.\n\n        Parameters\n        ----------\n        data_type : str\n            Type of cached data to load.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            - The cached data (Maps or Reliability), or None if not found\n            - A boolean indicating whether valid cache was found\n        \"\"\"\n        cache_dir = self.cache_directory(data_type)\n        if cache_dir.exists():\n            # If the directory exists, check if there are any cached params that match the expected hash\n            params_hash = self._params_hash(data_type)\n            cached_params_path = cache_dir / f\"params_{params_hash}.npz\"\n            if cached_params_path.exists():\n                cached_params = dict(np.load(cached_params_path))\n                # Check if the cached params match the dependent params\n                if self.check_params_match(cached_params):\n                    return self._load_from_cache(data_type, params_hash, params=cached_params), True\n        return None, False\n\n    def check_params_match(self, cached_params: dict) -&gt; bool:\n        \"\"\"Check if the cached params and the current params are the same.\n\n        Parameters\n        ----------\n        cached_params : dict\n            The cached params to check against the current params\n\n        Returns\n        -------\n        bool\n            True if the cached params are nonempty and match the current params, False otherwise\n        \"\"\"\n        return cached_params and all(cached_params[k] == getattr(self.params, k) for k in cached_params)\n\n    def _load_from_cache(self, data_type: str, params_hash: str, params: Optional[Dict[str, Any]] | None = None) -&gt; Union[Maps, Reliability]:\n        \"\"\"Load cached data from disk using a parameter hash.\n\n        Parameters\n        ----------\n        data_type : str\n            Type of cached data to load.\n        params_hash : str\n            Hash string identifying the cached parameters.\n        params : dict, optional\n            Dictionary of cached parameters. Used for reliability method.\n            Default is None.\n\n        Returns\n        -------\n        Maps or Reliability\n            The loaded cached data object.\n\n        Raises\n        ------\n        ValueError\n            If data_type is not recognized.\n        \"\"\"\n        cache_dir = self.cache_directory(data_type)\n        if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n            cached_data = {}\n            for name in Maps.map_types():\n                cached_data[name] = np.load(cache_dir / f\"data_{name}_{params_hash}.npy\", mmap_mode=\"r\")\n            if data_type == \"raw_maps\":\n                return Maps.create_raw_maps(**cached_data)\n            elif data_type == \"processed_maps\":\n                return Maps.create_processed_maps(**cached_data)\n        elif data_type == \"env_maps\":\n            environments = np.load(cache_dir / f\"data_environments_{params_hash}.npy\")\n            cached_data = dict(environments=environments)\n            for name in Maps.map_types():\n                cached_data[name] = []\n                for env in environments:\n                    cached_data[name].append(np.load(cache_dir / f\"data_{name}_{env}_{params_hash}.npy\", mmap_mode=\"r\"))\n            return Maps.create_environment_maps(**cached_data)\n        elif data_type == \"reliability\":\n            environments = np.load(cache_dir / f\"data_environments_{params_hash}.npy\")\n            values = np.load(cache_dir / f\"data_reliability_{params_hash}.npy\")\n            method = params[\"reliability_method\"]\n            return Reliability(values, environments, method)\n        else:\n            raise ValueError(f\"Unknown data type: {data_type}\")\n\n    @manage_one_cache\n    def _filter_environments(\n        self,\n        envnum: Union[int, Iterable[int], None] = None,\n        clear_one_cache: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Filter the session data to only include trials from certain environments.\n\n        Parameters\n        ----------\n        envnum : int, iterable of int, or None, optional\n            Environment number(s) to filter. If None, returns all trials.\n            Default is None.\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after filtering. Default is True.\n\n        Returns\n        -------\n        np.ndarray\n            Boolean array indicating which trials belong to the specified environment(s).\n\n        Notes\n        -----\n        This assumes that the trials are in order. We might want to use the third\n        output of session.positions to get the \"real\" trial numbers which aren't\n        always contiguous and 0 indexed.\n        \"\"\"\n        if envnum is None:\n            envnum = self.session.environments\n        envnum = helpers.check_iterable(envnum)\n        return np.isin(self.session.trial_environment, envnum)\n\n    @property\n    def dist_edges(self) -&gt; np.ndarray:\n        \"\"\"Distance edges for the position bins.\n\n        Returns\n        -------\n        np.ndarray\n            1D array of position bin edges. Shape is (num_positions + 1,).\n\n        Raises\n        ------\n        ValueError\n            If not all trials have the same environment length.\n\n        Notes\n        -----\n        The number of position bins is determined by dividing the environment\n        length by dist_step. This property caches the environment length\n        internally after first access.\n        \"\"\"\n        if not hasattr(self, \"_env_length\"):\n            env_length = self.session.env_length\n            if hasattr(env_length, \"__len__\"):\n                if np.unique(env_length).size != 1:\n                    msg = \"SpkmapProcessor (currently) requires all trials to have the same env length!\"\n                    raise ValueError(msg)\n                env_length = env_length[0]\n            self._env_length = env_length\n\n        num_positions = int(self._env_length / self.params.dist_step)\n        return np.linspace(0, self._env_length, num_positions + 1)\n\n    @property\n    def dist_centers(self) -&gt; np.ndarray:\n        \"\"\"Distance centers for the position bins.\n\n        Returns\n        -------\n        np.ndarray\n            1D array of position bin centers. Shape is (num_positions,).\n        \"\"\"\n        return helpers.edge2center(self.dist_edges)\n\n    @manage_one_cache\n    def _idx_required_position_bins(self, clear_one_cache: bool = True) -&gt; np.ndarray:\n        \"\"\"Get the indices of the position bins that are required for a full trial\n\n        Parameters\n        ----------\n        clear_one_cache : bool, default=False\n            Whether to clear the onefile cache after getting the indices\n\n        Returns\n        -------\n        np.ndarray\n            The indices of the position bins that are required for a trial to be considered full\n        \"\"\"\n        num_position_bins = len(self.dist_centers)\n        if self.params.full_trial_flexibility is None:\n            idx_to_required_bins = np.arange(num_position_bins)\n        else:\n            start_idx = np.where(self.dist_edges &gt;= self.params.full_trial_flexibility)[0][0]\n            end_idx = np.where(self.dist_edges &lt;= self.dist_edges[-1] - self.params.full_trial_flexibility)[0][-1]\n            idx_to_required_bins = np.arange(start_idx, end_idx)\n        return idx_to_required_bins\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"raw_maps\", disable=False)\n    def get_raw_maps(\n        self,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get raw maps (occupancy, speed, spkmap) from session data.\n\n        This method processes session data to create spatial maps representing\n        occupancy, speed, and neural activity across position bins. The maps\n        are in raw format (not smoothed or normalized by occupancy).\n\n        Parameters\n        ----------\n        force_recompute : bool, optional\n            Whether to force recomputation even if cached data exists. Default is False.\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after processing. Default is True.\n        params : SpkmapParams, dict, or None, optional\n            Parameters for processing. If None, uses instance parameters.\n            If a dict, updates instance parameters temporarily.\n            Parameters are restored after method execution. Default is None.\n\n        Returns\n        -------\n        Maps\n            Maps instance containing raw occupancy, speed, and spike maps.\n            Shape: (trials, positions) for occmap/speedmap,\n            (trials, positions, rois) for spkmap.\n\n        Notes\n        -----\n        The method:\n        1. Bins positions according to dist_step\n        2. Filters by speed threshold\n        3. Computes occupancy, speed, and spike maps\n        4. Sets unvisited position bins to NaN\n        5. Optionally standardizes spike data\n\n        Results are cached based on parameter hash for efficient reuse.\n        \"\"\"\n        dist_edges = self.dist_edges\n        dist_centers = self.dist_centers\n        num_positions = len(dist_centers)\n\n        # Get behavioral timestamps and positions\n        timestamps, positions, trial_numbers, idx_behave_to_frame = self.session.positions\n\n        # compute behavioral speed on each sample\n        within_trial_sample = np.append(np.diff(trial_numbers) == 0, True)\n        sample_duration = np.append(np.diff(timestamps), 0)\n        speeds = np.append(np.diff(positions) / sample_duration[:-1], 0)\n        # do this after division so no /0 errors\n        sample_duration = sample_duration * within_trial_sample\n        # speed 0 in last sample for each trial (it's undefined)\n        speeds = speeds * within_trial_sample\n        # Convert positions to position bins\n        position_bin = np.digitize(positions, dist_edges) - 1\n\n        # get imaging information\n        frame_time_stamps = self.session.timestamps\n        sampling_period = np.median(np.diff(frame_time_stamps))\n        dist_cutoff = sampling_period / 2\n        delay_position_to_imaging = frame_time_stamps[idx_behave_to_frame] - timestamps\n\n        # get spiking information\n        spks = self.session.spks\n        num_rois = self.session.get_value(\"numROIs\")\n\n        # Do standardization\n        if self.params.standardize_spks:\n            spks = median_zscore(spks, median_subtract=not self.session.zero_baseline_spks)\n\n        # Get high resolution occupancy and speed maps\n        dtype = np.float32\n        occmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        speedmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n        spkmap = np.zeros((self.session.num_trials, num_positions, num_rois), dtype=dtype)\n        extra_counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n\n        # Get maps -- doing this independently for each map allows for more\n        # flexibility in which data to load (basically the occmap &amp; speedmap\n        # are instantaneous, but the spkmap is a bit slower)\n        get_summation_map(\n            sample_duration,\n            trial_numbers,\n            position_bin,\n            occmap,\n            counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=False,\n            use_sample_to_value_idx=False,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n        get_summation_map(\n            speeds,\n            trial_numbers,\n            position_bin,\n            speedmap,\n            counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=True,\n            use_sample_to_value_idx=False,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n        get_summation_map(\n            spks,\n            trial_numbers,\n            position_bin,\n            spkmap,\n            extra_counts,\n            speeds,\n            self.params.speed_threshold,\n            self.params.speed_max_allowed,\n            delay_position_to_imaging,\n            dist_cutoff,\n            sample_duration,\n            scale_by_sample_duration=True,\n            use_sample_to_value_idx=True,\n            sample_to_value_idx=idx_behave_to_frame,\n        )\n\n        # Figure out the valid range (outside of this range, set the maps to nan, because their values are not meaningful)\n        position_bin_per_trial = [position_bin[trial_numbers == tnum] for tnum in range(self.session.num_trials)]\n\n        # offsetting by 1 because there is a bug in the vrControl software where the first sample is always set\n        # to the minimum position (which is 0), but if there is a built-up buffer in the rotary encoder, the position\n        # will jump at the second sample. In general this will always work unless the mice have a truly ridiculous\n        # speed at the beginning of the trial...\n        first_valid_bin = [np.min(bpb[1:] if len(bpb) &gt; 1 else bpb) for bpb in position_bin_per_trial]\n        last_valid_bin = [np.max(bpb) for bpb in position_bin_per_trial]\n\n        # set bins to nan when mouse didn't visit them\n        occmap = replace_missing_data(occmap, first_valid_bin, last_valid_bin)\n        speedmap = replace_missing_data(speedmap, first_valid_bin, last_valid_bin)\n        spkmap = replace_missing_data(spkmap, first_valid_bin, last_valid_bin)\n\n        return Maps.create_raw_maps(occmap, speedmap, spkmap)\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"processed_maps\", disable=False)\n    def get_processed_maps(\n        self,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get processed maps (smoothed and normalized by occupancy).\n\n        This method creates processed maps by:\n        1. Getting raw maps\n        2. Optionally smoothing with a Gaussian kernel\n        3. Normalizing speedmap and spkmap by occupancy\n        4. Reorganizing spkmap to have ROIs as the first dimension\n\n        Parameters\n        ----------\n        force_recompute : bool, optional\n            Whether to force recomputation even if cached data exists. Default is False.\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after processing. Default is True.\n        params : SpkmapParams, dict, or None, optional\n            Parameters for processing. If None, uses instance parameters.\n            If a dict, updates instance parameters temporarily.\n            Parameters are restored after method execution. Default is None.\n\n        Returns\n        -------\n        Maps\n            Maps instance containing processed occupancy, speed, and spike maps.\n            Shape: (trials, positions) for occmap/speedmap,\n            (rois, trials, positions) for spkmap.\n        \"\"\"\n        # Get the raw maps first (don't need to specify params because they're already set by this method)\n        maps = self.get_raw_maps(\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n        )\n\n        # Process the maps (smooth, divide by occupancy, and change to ROIs first)\n        return maps.raw_to_processed(self.dist_centers, self.params.smooth_width)\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"env_maps\", disable=False)\n    def get_env_maps(\n        self,\n        use_session_filters: bool = True,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Maps:\n        \"\"\"Get processed maps separated by environment.\n\n        This method creates environment-separated maps by:\n        1. Getting processed maps\n        2. Filtering to include only full trials (based on full_trial_flexibility)\n        3. Filtering ROIs if use_session_filters=True\n        4. Grouping maps by environment\n\n        Parameters\n        ----------\n        use_session_filters : bool, optional\n            Whether to filter ROIs using session.idx_rois. Default is True.\n        force_recompute : bool, optional\n            Whether to force recomputation even if cached data exists. Default is False.\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after processing. Default is True.\n        params : SpkmapParams, dict, or None, optional\n            Parameters for processing. If None, uses instance parameters.\n            If a dict, updates instance parameters temporarily.\n            Parameters are restored after method execution. Default is None.\n\n        Returns\n        -------\n        Maps\n            Maps instance with by_environment=True, containing lists of maps\n            for each environment. Shape per environment:\n            (trials_in_env, positions) for occmap/speedmap,\n            (rois, trials_in_env, positions) for spkmap.\n        \"\"\"\n        # Make sure it's an iterable -- the output will always be a list\n        envnum = helpers.check_iterable(self.session.environments)\n\n        # Get the indices of the trials to each environment\n        idx_each_environment = [self._filter_environments(env) for env in envnum]\n\n        # Then get the indices of the position bins that are required for a full trial\n        idx_required_position_bins = self._idx_required_position_bins(clear_one_cache)\n\n        # Get the processed maps (don't need to specify params because they're already set by the decorator)\n        maps = self.get_processed_maps(\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n        )\n\n        # Add the list of environments to the maps\n        maps.environments = envnum\n\n        # Make a list of the maps we are processing\n        maps_to_process = Maps.map_types()\n\n        # Filter the maps to only include the ROIs we want\n        if use_session_filters:\n            idx_rois = np.where(self.session.idx_rois)[0]\n        else:\n            idx_rois = np.arange(self.session.get_value(\"numROIs\"), dtype=int)\n\n        # Filter the maps to only include the full trials\n        full_trials = np.where(np.all(~np.isnan(maps.occmap[:, idx_required_position_bins]), axis=1))[0]\n\n        # Implement trial &amp; ROI filtering here\n        for mapname in maps_to_process:\n            if mapname == \"spkmap\":\n                maps[mapname] = np.take(np.take(maps[mapname], idx_rois, axis=0), full_trials, axis=1)\n            else:\n                maps[mapname] = np.take(maps[mapname], full_trials, axis=0)\n\n        # Filter the trial indices to only include full trials\n        idx_each_environment = [np.where(np.take(idx, full_trials, axis=0))[0] for idx in idx_each_environment]\n\n        # Then group each one by environment\n        # -&gt; this is now (trials_in_env, position_bins, ...(roi if spkmap)...)\n        maps.by_environment = True\n        for mapname in maps_to_process:\n            if mapname == \"spkmap\":\n                maps[mapname] = [np.take(maps[mapname], idx, axis=1) for idx in idx_each_environment]\n            else:\n                maps[mapname] = [np.take(maps[mapname], idx, axis=0) for idx in idx_each_environment]\n\n        return maps\n\n    @with_temp_params\n    @manage_one_cache\n    @cached_processor(\"reliability\", disable=False)\n    def get_reliability(\n        self,\n        use_session_filters: bool = True,\n        force_recompute: bool = False,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Reliability:\n        \"\"\"Calculate reliability of spike maps across trials.\n\n        Reliability measures how consistent neural activity is across trials\n        within each environment. Multiple methods are supported.\n\n        Parameters\n        ----------\n        use_session_filters : bool, optional\n            Whether to filter ROIs using session.idx_rois. Default is True.\n        force_recompute : bool, optional\n            Whether to force recomputation even if cached data exists. Default is False.\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after processing. Default is True.\n        params : SpkmapParams, dict, or None, optional\n            Parameters for processing. If None, uses instance parameters.\n            If a dict, updates instance parameters temporarily.\n            Parameters are restored after method execution. Default is None.\n\n        Returns\n        -------\n        Reliability\n            Reliability instance containing reliability values for each ROI\n            in each environment. Shape: (num_environments, num_rois).\n\n        Notes\n        -----\n        Supported reliability methods:\n        - \"leave_one_out\": Leave-one-out cross-validation\n        - \"correlation\": Correlation between trial pairs\n        - \"mse\": Mean squared error between trial pairs\n\n        All reliability measures require maps with no NaN positions.\n        \"\"\"\n        envnum = helpers.check_iterable(self.session.environments)\n\n        # A list of the requested environments (all if not specified)\n        maps = self.get_env_maps(\n            use_session_filters=use_session_filters,\n            force_recompute=force_recompute,\n            clear_one_cache=clear_one_cache,\n            params={\"autosave\": False},  # Prevent saving in the case of a recompute\n        )\n\n        # All reliability measures require no NaNs\n        maps.pop_nan_positions()\n\n        if self.params.reliability_method == \"leave_one_out\":\n            rel_values = [helpers.reliability_loo(spkmap) for spkmap in maps.spkmap]\n        elif self.params.reliability_method == \"correlation\" or self.params.reliability_method == \"mse\":\n            rel_mse, rel_cor = helpers.named_transpose([helpers.measureReliability(spkmap) for spkmap in maps.spkmap])\n            rel_values = rel_mse if self.params.reliability_method == \"mse\" else rel_cor\n        else:\n            raise ValueError(f\"Method {self.params.reliability_method} not supported\")\n\n        return Reliability(\n            np.stack(rel_values),\n            environments=envnum,\n            method=self.params.reliability_method,\n        )\n\n    # ------------------- convert between imaging and behavioral time -------------------\n    @with_temp_params\n    @manage_one_cache\n    def get_frame_behavior(\n        self,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Get position and environment data for each imaging frame.\n\n        This method aligns behavioral data (position, speed, environment, trial)\n        to imaging frame timestamps. Returns NaN for frames where no position\n        data is available (e.g., if the closest behavioral sample is further\n        away in time than half the sampling period).\n\n        Parameters\n        ----------\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after processing. Default is True.\n        params : SpkmapParams, dict, or None, optional\n            Parameters for processing. If None, uses instance parameters.\n            If a dict, updates instance parameters temporarily.\n            Parameters are restored after method execution. Default is None.\n\n        Returns\n        -------\n        tuple\n            A tuple containing four arrays (all with shape (num_frames,)):\n            - frame_position: Position for each frame (NaN if unavailable)\n            - frame_speed: Speed for each frame (NaN if unavailable)\n            - frame_environment: Environment number for each frame (NaN if unavailable)\n            - frame_trial: Trial number for each frame (NaN if unavailable)\n        \"\"\"\n        timestamps = self.session.loadone(\"positionTracking.times\")\n        position = self.session.loadone(\"positionTracking.position\")\n        idx_behave_to_frame = self.session.loadone(\"positionTracking.mpci\")\n        trial_start_index = self.session.loadone(\"trials.positionTracking\")\n        num_samples = len(position)\n        trial_numbers = np.arange(len(trial_start_index))\n        trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n        trial_numbers = np.repeat(trial_numbers, trial_lengths)\n        trial_environment = self.session.loadone(\"trials.environmentIndex\")\n        trial_environment = np.repeat(trial_environment, trial_lengths)\n\n        within_trial = np.append(np.diff(trial_numbers) == 0, True)\n        sample_duration = np.append(np.diff(timestamps), 0)\n        speed = np.append(np.diff(position) / sample_duration[:-1], 0)\n        sample_duration = sample_duration * within_trial\n        speed = speed * within_trial\n\n        frame_timestamps = self.session.loadone(\"mpci.times\")\n        difference_timestamps = np.abs(timestamps - frame_timestamps[idx_behave_to_frame])\n        sampling_period = np.median(np.diff(frame_timestamps))\n        dist_cutoff = sampling_period / 2\n\n        frame_position = np.zeros_like(frame_timestamps)\n        count = np.zeros_like(frame_timestamps)\n        helpers.get_average_frame_position(position, idx_behave_to_frame, difference_timestamps, dist_cutoff, frame_position, count)\n        frame_position[count &gt; 0] /= count[count &gt; 0]\n        frame_position[count == 0] = np.nan\n        frame_speed = np.diff(frame_position) / np.diff(frame_timestamps)\n        frame_speed = np.append(frame_speed, 0)\n\n        # Get a map from frame to behavior time for quick lookup\n        idx_frame_to_behave, dist_frame_to_behave = helpers.nearestpoint(frame_timestamps, timestamps)\n        idx_get_position = dist_frame_to_behave &lt; dist_cutoff\n\n        frame_environment = np.full(len(frame_timestamps), np.nan)\n        frame_environment[idx_get_position] = trial_environment[idx_frame_to_behave[idx_get_position]]\n        frame_environment[count == 0] = np.nan\n\n        frame_trial = np.full(len(frame_timestamps), np.nan)\n        frame_trial[idx_get_position] = trial_numbers[idx_frame_to_behave[idx_get_position]]\n        frame_trial[count == 0] = np.nan\n\n        return frame_position, frame_speed, frame_environment, frame_trial\n\n    @with_temp_params\n    @manage_one_cache\n    def get_placefield_prediction(\n        self,\n        use_session_filters: bool = True,\n        spks_type: Union[str, None] = None,\n        use_speed_threshold: bool = True,\n        clear_one_cache: bool = True,\n        params: Union[SpkmapParams, Dict[str, Any], None] = None,\n    ) -&gt; Tuple[np.ndarray, Dict[str, np.ndarray]]:\n        \"\"\"Predict neural activity from place field maps.\n\n        This method uses averaged environment maps to predict neural activity\n        at each imaging frame based on the animal's position and environment.\n\n        Parameters\n        ----------\n        use_session_filters : bool, optional\n            Whether to filter ROIs using session.idx_rois. Default is True.\n        spks_type : str or None, optional\n            Type of spike data to use. If None, uses session's current spks_type.\n            Temporarily changes session.spks_type if provided. Default is None.\n        use_speed_threshold : bool, optional\n            Whether to only predict for frames where speed exceeds threshold.\n            Default is True.\n        clear_one_cache : bool, optional\n            Whether to clear the onefile cache after processing. Default is True.\n        params : SpkmapParams, dict, or None, optional\n            Parameters for processing. If None, uses instance parameters.\n            If a dict, updates instance parameters temporarily.\n            Parameters are restored after method execution. Default is None.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            - placefield_prediction: Predicted activity array with shape (frames, rois).\n              NaN for frames where prediction is not possible.\n            - extras: Dictionary with additional information:\n              - frame_position_index: Position bin index for each frame\n              - frame_environment_index: Environment index for each frame\n              - idx_valid: Boolean array indicating valid predictions\n\n        Notes\n        -----\n        Predictions are based on averaged trial maps. Frames where the animal\n        is not moving (if use_speed_threshold=True) or where position/environment\n        data is unavailable will have NaN predictions.\n        \"\"\"\n        if spks_type is not None:\n            _spks_type = self.session.spks_type\n            self.session.params.spks_type = spks_type\n\n        frame_position, frame_speed, frame_environment, _ = self.get_frame_behavior(clear_one_cache, params)\n        idx_valid = ~np.isnan(frame_position)\n        if use_speed_threshold:\n            idx_valid = idx_valid &amp; (frame_speed &gt; self.params.speed_threshold)\n\n        # Convert frame position to bins indices\n        frame_position_index = np.searchsorted(self.dist_edges, frame_position, side=\"right\") - 1\n\n        # Get the place field for each neuron\n        env_maps = self.get_env_maps(use_session_filters=use_session_filters)\n        env_maps.average_trials()\n\n        # Convert frame environment to indices\n        env_to_idx = {env: i for i, env in enumerate(env_maps.environments)}\n        frame_environment_index = np.array([env_to_idx[env] if not np.isnan(env) else -1000 for env in frame_environment], dtype=int)\n\n        # Get the original spks data\n        spks = self.session.spks\n        if use_session_filters:\n            spks = spks[:, self.session.idx_rois]\n\n        # Use a numba speed up to get the placefield prediction (single pass simple algorithm)\n        placefield_prediction = np.full(spks.shape, np.nan)\n        spkmaps = np.stack(list(map(lambda x: x.T, env_maps.spkmap)))\n        placefield_prediction = placefield_prediction_numba(\n            placefield_prediction,\n            spkmaps,\n            frame_environment_index,\n            frame_position_index,\n            idx_valid,\n        )\n\n        # This will add samples for which a place field was not estimable (at the edges of the environment)\n        idx_valid = np.all(~np.isnan(placefield_prediction), axis=1)\n\n        # Reset spks_type\n        if spks_type is not None:\n            self.session.params.spks_type = _spks_type\n\n        # Include extra details in a dictionary for forward compatibility\n        extras = dict(\n            frame_position_index=frame_position_index,\n            frame_environment_index=frame_environment_index,\n            idx_valid=idx_valid,\n        )\n\n        return placefield_prediction, extras\n\n    def get_traversals(\n        self,\n        idx_roi: int,\n        idx_env: int,\n        width: int = 10,\n        placefield_threshold: float = 5.0,\n        fill_nan: bool = False,\n        spks: np.ndarray = None,\n        spks_prediction: np.ndarray = None,\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Extract neural activity around place field peak during traversals.\n\n        This method identifies trials where the animal passes through a neuron's\n        place field peak and extracts activity windows around those moments.\n\n        Parameters\n        ----------\n        idx_roi : int\n            Index of the ROI (neuron) to analyze.\n        idx_env : int\n            Index of the environment to analyze (index into env_maps.environments).\n        width : int, optional\n            Number of frames on each side of the peak to include. Total window\n            size is 2*width + 1. Default is 10.\n        placefield_threshold : float, optional\n            Maximum distance from place field peak to include a trial (in spatial units).\n            Default is 5.0.\n        fill_nan : bool, optional\n            Whether to fill NaN values with 0. Default is False.\n        spks : np.ndarray, optional\n            Spike data array. If None, loads from session. Default is None.\n        spks_prediction : np.ndarray, optional\n            Place field prediction array. If None, computes it. Default is None.\n\n        Returns\n        -------\n        tuple\n            A tuple containing:\n            - traversals: Array of shape (num_traversals, 2*width+1) containing\n              actual neural activity around each traversal.\n            - pred_travs: Array of shape (num_traversals, 2*width+1) containing\n              predicted activity around each traversal.\n\n        Notes\n        -----\n        Only includes trials where the animal passes within placefield_threshold\n        of the place field peak. The peak is determined from the averaged spike\n        map for the specified ROI and environment.\n        \"\"\"\n        frame_position, _, frame_environment, frame_trial = self.get_frame_behavior()\n        if spks_prediction is None:\n            spks_prediction = self.get_placefield_prediction(use_session_filters=True)[0]\n        if spks is None:\n            spks = self.session.spks[:, self.session.idx_rois]\n\n        if spks.shape != spks_prediction.shape:\n            raise ValueError(\"spks and spks_prediction must have the same shape\")\n\n        env_maps = self.get_env_maps()\n        pos_peak = self.dist_centers[np.nanargmax(np.nanmean(env_maps.spkmap[idx_env][idx_roi], axis=0))]\n        envnum = env_maps.environments[idx_env]\n\n        env_trials = np.unique(frame_trial[frame_environment == envnum])\n\n        num_trials = len(env_trials)\n        idx_traversal = -1 * np.ones(num_trials, dtype=int)\n        for itrial, trialnum in enumerate(env_trials):\n            idx_trial = frame_trial == trialnum\n            idx_closest_pos = np.nanargmin(np.abs(frame_position - pos_peak) + 10000 * ~idx_trial)\n\n            # Only include the trial if the closest position is within placefield threshold of the peak\n            if np.abs(frame_position[idx_closest_pos] - pos_peak) &lt; placefield_threshold:\n                idx_traversal[itrial] = idx_closest_pos\n\n        # Filter out trials that don't have a traversal\n        idx_traversal = idx_traversal[idx_traversal != -1]\n\n        # Get traversals through place field in requested environment\n        traversals = np.zeros((len(idx_traversal), width * 2 + 1))\n        pred_travs = np.zeros((len(idx_traversal), width * 2 + 1))\n        for ii, it in enumerate(idx_traversal):\n            istart = it - width\n            iend = it + width + 1\n            istartoffset = max(0, -istart)\n            iendoffset = max(0, iend - spks.shape[0])\n            traversals[ii, istartoffset : width * 2 + 1 - iendoffset] = spks[istart + istartoffset : iend - iendoffset, idx_roi]\n            pred_travs[ii, istartoffset : width * 2 + 1 - iendoffset] = spks_prediction[istart + istartoffset : iend - iendoffset, idx_roi]\n\n        if fill_nan:\n            traversals[np.isnan(traversals)] = 0.0\n            pred_travs[np.isnan(pred_travs)] = 0.0\n\n        return traversals, pred_travs\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.dist_centers","title":"<code>dist_centers</code>  <code>property</code>","text":"<p>Distance centers for the position bins.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>1D array of position bin centers. Shape is (num_positions,).</p>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.dist_edges","title":"<code>dist_edges</code>  <code>property</code>","text":"<p>Distance edges for the position bins.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>1D array of position bin edges. Shape is (num_positions + 1,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not all trials have the same environment length.</p> Notes <p>The number of position bins is determined by dividing the environment length by dist_step. This property caches the environment length internally after first access.</p>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.cache_directory","title":"<code>cache_directory(data_type=None)</code>","text":"<p>Get the cache directory path for a given data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>Type of cached data. If None, returns the base cache directory. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the cache directory for the specified data type.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def cache_directory(self, data_type: Optional[str] = None) -&gt; Path:\n    \"\"\"Get the cache directory path for a given data type.\n\n    Parameters\n    ----------\n    data_type : str, optional\n        Type of cached data. If None, returns the base cache directory.\n        Default is None.\n\n    Returns\n    -------\n    Path\n        Path to the cache directory for the specified data type.\n    \"\"\"\n    if data_type is None:\n        return self.session.data_path / \"spkmaps\"\n    else:\n        folder_name = f\"{data_type}_{self.session.spks_type}\"\n        return self.session.data_path / \"spkmaps\" / folder_name\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.cached_dependencies","title":"<code>cached_dependencies(data_type)</code>","text":"<p>Get the parameter dependencies for a given data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>Type of cached data (\"raw_maps\", \"processed_maps\", \"env_maps\", or \"reliability\").</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of parameter names that affect the cache validity for this data type.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def cached_dependencies(self, data_type: str) -&gt; List[str]:\n    \"\"\"Get the parameter dependencies for a given data type.\n\n    Parameters\n    ----------\n    data_type : str\n        Type of cached data (\"raw_maps\", \"processed_maps\", \"env_maps\", or \"reliability\").\n\n    Returns\n    -------\n    list of str\n        List of parameter names that affect the cache validity for this data type.\n    \"\"\"\n    if data_type == \"raw_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\"]\n    elif data_type == \"processed_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\"]\n    elif data_type == \"env_maps\":\n        return [\"dist_step\", \"speed_threshold\", \"speed_max_allowed\", \"standardize_spks\", \"smooth_width\", \"full_trial_flexibility\"]\n    elif data_type == \"reliability\":\n        return [\n            \"dist_step\",\n            \"speed_threshold\",\n            \"speed_max_allowed\",\n            \"standardize_spks\",\n            \"smooth_width\",\n            \"full_trial_flexibility\",\n            \"reliability_method\",\n        ]\n    # Otherwise just return all params\n    return list(self.params.__dict__.keys())\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.check_params_match","title":"<code>check_params_match(cached_params)</code>","text":"<p>Check if the cached params and the current params are the same.</p> <p>Parameters:</p> Name Type Description Default <code>cached_params</code> <code>dict</code> <p>The cached params to check against the current params</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the cached params are nonempty and match the current params, False otherwise</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def check_params_match(self, cached_params: dict) -&gt; bool:\n    \"\"\"Check if the cached params and the current params are the same.\n\n    Parameters\n    ----------\n    cached_params : dict\n        The cached params to check against the current params\n\n    Returns\n    -------\n    bool\n        True if the cached params are nonempty and match the current params, False otherwise\n    \"\"\"\n    return cached_params and all(cached_params[k] == getattr(self.params, k) for k in cached_params)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.dependent_params","title":"<code>dependent_params(data_type)</code>","text":"<p>Get the dependent parameters for a given data type as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>Type of cached data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping parameter names to their values for the given data type.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def dependent_params(self, data_type: str) -&gt; dict:\n    \"\"\"Get the dependent parameters for a given data type as a dictionary.\n\n    Parameters\n    ----------\n    data_type : str\n        Type of cached data.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping parameter names to their values for the given data type.\n    \"\"\"\n    return {k: getattr(self.params, k) for k in self.cached_dependencies(data_type)}\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_env_maps","title":"<code>get_env_maps(use_session_filters=True, force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get processed maps separated by environment.</p> <p>This method creates environment-separated maps by: 1. Getting processed maps 2. Filtering to include only full trials (based on full_trial_flexibility) 3. Filtering ROIs if use_session_filters=True 4. Grouping maps by environment</p> <p>Parameters:</p> Name Type Description Default <code>use_session_filters</code> <code>bool</code> <p>Whether to filter ROIs using session.idx_rois. Default is True.</p> <code>True</code> <code>force_recompute</code> <code>bool</code> <p>Whether to force recomputation even if cached data exists. Default is False.</p> <code>False</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after processing. Default is True.</p> <code>True</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for processing. If None, uses instance parameters. If a dict, updates instance parameters temporarily. Parameters are restored after method execution. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Maps instance with by_environment=True, containing lists of maps for each environment. Shape per environment: (trials_in_env, positions) for occmap/speedmap, (rois, trials_in_env, positions) for spkmap.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"env_maps\", disable=False)\ndef get_env_maps(\n    self,\n    use_session_filters: bool = True,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get processed maps separated by environment.\n\n    This method creates environment-separated maps by:\n    1. Getting processed maps\n    2. Filtering to include only full trials (based on full_trial_flexibility)\n    3. Filtering ROIs if use_session_filters=True\n    4. Grouping maps by environment\n\n    Parameters\n    ----------\n    use_session_filters : bool, optional\n        Whether to filter ROIs using session.idx_rois. Default is True.\n    force_recompute : bool, optional\n        Whether to force recomputation even if cached data exists. Default is False.\n    clear_one_cache : bool, optional\n        Whether to clear the onefile cache after processing. Default is True.\n    params : SpkmapParams, dict, or None, optional\n        Parameters for processing. If None, uses instance parameters.\n        If a dict, updates instance parameters temporarily.\n        Parameters are restored after method execution. Default is None.\n\n    Returns\n    -------\n    Maps\n        Maps instance with by_environment=True, containing lists of maps\n        for each environment. Shape per environment:\n        (trials_in_env, positions) for occmap/speedmap,\n        (rois, trials_in_env, positions) for spkmap.\n    \"\"\"\n    # Make sure it's an iterable -- the output will always be a list\n    envnum = helpers.check_iterable(self.session.environments)\n\n    # Get the indices of the trials to each environment\n    idx_each_environment = [self._filter_environments(env) for env in envnum]\n\n    # Then get the indices of the position bins that are required for a full trial\n    idx_required_position_bins = self._idx_required_position_bins(clear_one_cache)\n\n    # Get the processed maps (don't need to specify params because they're already set by the decorator)\n    maps = self.get_processed_maps(\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n    )\n\n    # Add the list of environments to the maps\n    maps.environments = envnum\n\n    # Make a list of the maps we are processing\n    maps_to_process = Maps.map_types()\n\n    # Filter the maps to only include the ROIs we want\n    if use_session_filters:\n        idx_rois = np.where(self.session.idx_rois)[0]\n    else:\n        idx_rois = np.arange(self.session.get_value(\"numROIs\"), dtype=int)\n\n    # Filter the maps to only include the full trials\n    full_trials = np.where(np.all(~np.isnan(maps.occmap[:, idx_required_position_bins]), axis=1))[0]\n\n    # Implement trial &amp; ROI filtering here\n    for mapname in maps_to_process:\n        if mapname == \"spkmap\":\n            maps[mapname] = np.take(np.take(maps[mapname], idx_rois, axis=0), full_trials, axis=1)\n        else:\n            maps[mapname] = np.take(maps[mapname], full_trials, axis=0)\n\n    # Filter the trial indices to only include full trials\n    idx_each_environment = [np.where(np.take(idx, full_trials, axis=0))[0] for idx in idx_each_environment]\n\n    # Then group each one by environment\n    # -&gt; this is now (trials_in_env, position_bins, ...(roi if spkmap)...)\n    maps.by_environment = True\n    for mapname in maps_to_process:\n        if mapname == \"spkmap\":\n            maps[mapname] = [np.take(maps[mapname], idx, axis=1) for idx in idx_each_environment]\n        else:\n            maps[mapname] = [np.take(maps[mapname], idx, axis=0) for idx in idx_each_environment]\n\n    return maps\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_frame_behavior","title":"<code>get_frame_behavior(clear_one_cache=True, params=None)</code>","text":"<p>Get position and environment data for each imaging frame.</p> <p>This method aligns behavioral data (position, speed, environment, trial) to imaging frame timestamps. Returns NaN for frames where no position data is available (e.g., if the closest behavioral sample is further away in time than half the sampling period).</p> <p>Parameters:</p> Name Type Description Default <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after processing. Default is True.</p> <code>True</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for processing. If None, uses instance parameters. If a dict, updates instance parameters temporarily. Parameters are restored after method execution. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing four arrays (all with shape (num_frames,)): - frame_position: Position for each frame (NaN if unavailable) - frame_speed: Speed for each frame (NaN if unavailable) - frame_environment: Environment number for each frame (NaN if unavailable) - frame_trial: Trial number for each frame (NaN if unavailable)</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\ndef get_frame_behavior(\n    self,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Get position and environment data for each imaging frame.\n\n    This method aligns behavioral data (position, speed, environment, trial)\n    to imaging frame timestamps. Returns NaN for frames where no position\n    data is available (e.g., if the closest behavioral sample is further\n    away in time than half the sampling period).\n\n    Parameters\n    ----------\n    clear_one_cache : bool, optional\n        Whether to clear the onefile cache after processing. Default is True.\n    params : SpkmapParams, dict, or None, optional\n        Parameters for processing. If None, uses instance parameters.\n        If a dict, updates instance parameters temporarily.\n        Parameters are restored after method execution. Default is None.\n\n    Returns\n    -------\n    tuple\n        A tuple containing four arrays (all with shape (num_frames,)):\n        - frame_position: Position for each frame (NaN if unavailable)\n        - frame_speed: Speed for each frame (NaN if unavailable)\n        - frame_environment: Environment number for each frame (NaN if unavailable)\n        - frame_trial: Trial number for each frame (NaN if unavailable)\n    \"\"\"\n    timestamps = self.session.loadone(\"positionTracking.times\")\n    position = self.session.loadone(\"positionTracking.position\")\n    idx_behave_to_frame = self.session.loadone(\"positionTracking.mpci\")\n    trial_start_index = self.session.loadone(\"trials.positionTracking\")\n    num_samples = len(position)\n    trial_numbers = np.arange(len(trial_start_index))\n    trial_lengths = np.append(np.diff(trial_start_index), num_samples - trial_start_index[-1])\n    trial_numbers = np.repeat(trial_numbers, trial_lengths)\n    trial_environment = self.session.loadone(\"trials.environmentIndex\")\n    trial_environment = np.repeat(trial_environment, trial_lengths)\n\n    within_trial = np.append(np.diff(trial_numbers) == 0, True)\n    sample_duration = np.append(np.diff(timestamps), 0)\n    speed = np.append(np.diff(position) / sample_duration[:-1], 0)\n    sample_duration = sample_duration * within_trial\n    speed = speed * within_trial\n\n    frame_timestamps = self.session.loadone(\"mpci.times\")\n    difference_timestamps = np.abs(timestamps - frame_timestamps[idx_behave_to_frame])\n    sampling_period = np.median(np.diff(frame_timestamps))\n    dist_cutoff = sampling_period / 2\n\n    frame_position = np.zeros_like(frame_timestamps)\n    count = np.zeros_like(frame_timestamps)\n    helpers.get_average_frame_position(position, idx_behave_to_frame, difference_timestamps, dist_cutoff, frame_position, count)\n    frame_position[count &gt; 0] /= count[count &gt; 0]\n    frame_position[count == 0] = np.nan\n    frame_speed = np.diff(frame_position) / np.diff(frame_timestamps)\n    frame_speed = np.append(frame_speed, 0)\n\n    # Get a map from frame to behavior time for quick lookup\n    idx_frame_to_behave, dist_frame_to_behave = helpers.nearestpoint(frame_timestamps, timestamps)\n    idx_get_position = dist_frame_to_behave &lt; dist_cutoff\n\n    frame_environment = np.full(len(frame_timestamps), np.nan)\n    frame_environment[idx_get_position] = trial_environment[idx_frame_to_behave[idx_get_position]]\n    frame_environment[count == 0] = np.nan\n\n    frame_trial = np.full(len(frame_timestamps), np.nan)\n    frame_trial[idx_get_position] = trial_numbers[idx_frame_to_behave[idx_get_position]]\n    frame_trial[count == 0] = np.nan\n\n    return frame_position, frame_speed, frame_environment, frame_trial\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_placefield_prediction","title":"<code>get_placefield_prediction(use_session_filters=True, spks_type=None, use_speed_threshold=True, clear_one_cache=True, params=None)</code>","text":"<p>Predict neural activity from place field maps.</p> <p>This method uses averaged environment maps to predict neural activity at each imaging frame based on the animal's position and environment.</p> <p>Parameters:</p> Name Type Description Default <code>use_session_filters</code> <code>bool</code> <p>Whether to filter ROIs using session.idx_rois. Default is True.</p> <code>True</code> <code>spks_type</code> <code>str or None</code> <p>Type of spike data to use. If None, uses session's current spks_type. Temporarily changes session.spks_type if provided. Default is None.</p> <code>None</code> <code>use_speed_threshold</code> <code>bool</code> <p>Whether to only predict for frames where speed exceeds threshold. Default is True.</p> <code>True</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after processing. Default is True.</p> <code>True</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for processing. If None, uses instance parameters. If a dict, updates instance parameters temporarily. Parameters are restored after method execution. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - placefield_prediction: Predicted activity array with shape (frames, rois).   NaN for frames where prediction is not possible. - extras: Dictionary with additional information:   - frame_position_index: Position bin index for each frame   - frame_environment_index: Environment index for each frame   - idx_valid: Boolean array indicating valid predictions</p> Notes <p>Predictions are based on averaged trial maps. Frames where the animal is not moving (if use_speed_threshold=True) or where position/environment data is unavailable will have NaN predictions.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\ndef get_placefield_prediction(\n    self,\n    use_session_filters: bool = True,\n    spks_type: Union[str, None] = None,\n    use_speed_threshold: bool = True,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Tuple[np.ndarray, Dict[str, np.ndarray]]:\n    \"\"\"Predict neural activity from place field maps.\n\n    This method uses averaged environment maps to predict neural activity\n    at each imaging frame based on the animal's position and environment.\n\n    Parameters\n    ----------\n    use_session_filters : bool, optional\n        Whether to filter ROIs using session.idx_rois. Default is True.\n    spks_type : str or None, optional\n        Type of spike data to use. If None, uses session's current spks_type.\n        Temporarily changes session.spks_type if provided. Default is None.\n    use_speed_threshold : bool, optional\n        Whether to only predict for frames where speed exceeds threshold.\n        Default is True.\n    clear_one_cache : bool, optional\n        Whether to clear the onefile cache after processing. Default is True.\n    params : SpkmapParams, dict, or None, optional\n        Parameters for processing. If None, uses instance parameters.\n        If a dict, updates instance parameters temporarily.\n        Parameters are restored after method execution. Default is None.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - placefield_prediction: Predicted activity array with shape (frames, rois).\n          NaN for frames where prediction is not possible.\n        - extras: Dictionary with additional information:\n          - frame_position_index: Position bin index for each frame\n          - frame_environment_index: Environment index for each frame\n          - idx_valid: Boolean array indicating valid predictions\n\n    Notes\n    -----\n    Predictions are based on averaged trial maps. Frames where the animal\n    is not moving (if use_speed_threshold=True) or where position/environment\n    data is unavailable will have NaN predictions.\n    \"\"\"\n    if spks_type is not None:\n        _spks_type = self.session.spks_type\n        self.session.params.spks_type = spks_type\n\n    frame_position, frame_speed, frame_environment, _ = self.get_frame_behavior(clear_one_cache, params)\n    idx_valid = ~np.isnan(frame_position)\n    if use_speed_threshold:\n        idx_valid = idx_valid &amp; (frame_speed &gt; self.params.speed_threshold)\n\n    # Convert frame position to bins indices\n    frame_position_index = np.searchsorted(self.dist_edges, frame_position, side=\"right\") - 1\n\n    # Get the place field for each neuron\n    env_maps = self.get_env_maps(use_session_filters=use_session_filters)\n    env_maps.average_trials()\n\n    # Convert frame environment to indices\n    env_to_idx = {env: i for i, env in enumerate(env_maps.environments)}\n    frame_environment_index = np.array([env_to_idx[env] if not np.isnan(env) else -1000 for env in frame_environment], dtype=int)\n\n    # Get the original spks data\n    spks = self.session.spks\n    if use_session_filters:\n        spks = spks[:, self.session.idx_rois]\n\n    # Use a numba speed up to get the placefield prediction (single pass simple algorithm)\n    placefield_prediction = np.full(spks.shape, np.nan)\n    spkmaps = np.stack(list(map(lambda x: x.T, env_maps.spkmap)))\n    placefield_prediction = placefield_prediction_numba(\n        placefield_prediction,\n        spkmaps,\n        frame_environment_index,\n        frame_position_index,\n        idx_valid,\n    )\n\n    # This will add samples for which a place field was not estimable (at the edges of the environment)\n    idx_valid = np.all(~np.isnan(placefield_prediction), axis=1)\n\n    # Reset spks_type\n    if spks_type is not None:\n        self.session.params.spks_type = _spks_type\n\n    # Include extra details in a dictionary for forward compatibility\n    extras = dict(\n        frame_position_index=frame_position_index,\n        frame_environment_index=frame_environment_index,\n        idx_valid=idx_valid,\n    )\n\n    return placefield_prediction, extras\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_processed_maps","title":"<code>get_processed_maps(force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get processed maps (smoothed and normalized by occupancy).</p> <p>This method creates processed maps by: 1. Getting raw maps 2. Optionally smoothing with a Gaussian kernel 3. Normalizing speedmap and spkmap by occupancy 4. Reorganizing spkmap to have ROIs as the first dimension</p> <p>Parameters:</p> Name Type Description Default <code>force_recompute</code> <code>bool</code> <p>Whether to force recomputation even if cached data exists. Default is False.</p> <code>False</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after processing. Default is True.</p> <code>True</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for processing. If None, uses instance parameters. If a dict, updates instance parameters temporarily. Parameters are restored after method execution. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Maps instance containing processed occupancy, speed, and spike maps. Shape: (trials, positions) for occmap/speedmap, (rois, trials, positions) for spkmap.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"processed_maps\", disable=False)\ndef get_processed_maps(\n    self,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get processed maps (smoothed and normalized by occupancy).\n\n    This method creates processed maps by:\n    1. Getting raw maps\n    2. Optionally smoothing with a Gaussian kernel\n    3. Normalizing speedmap and spkmap by occupancy\n    4. Reorganizing spkmap to have ROIs as the first dimension\n\n    Parameters\n    ----------\n    force_recompute : bool, optional\n        Whether to force recomputation even if cached data exists. Default is False.\n    clear_one_cache : bool, optional\n        Whether to clear the onefile cache after processing. Default is True.\n    params : SpkmapParams, dict, or None, optional\n        Parameters for processing. If None, uses instance parameters.\n        If a dict, updates instance parameters temporarily.\n        Parameters are restored after method execution. Default is None.\n\n    Returns\n    -------\n    Maps\n        Maps instance containing processed occupancy, speed, and spike maps.\n        Shape: (trials, positions) for occmap/speedmap,\n        (rois, trials, positions) for spkmap.\n    \"\"\"\n    # Get the raw maps first (don't need to specify params because they're already set by this method)\n    maps = self.get_raw_maps(\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n    )\n\n    # Process the maps (smooth, divide by occupancy, and change to ROIs first)\n    return maps.raw_to_processed(self.dist_centers, self.params.smooth_width)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_raw_maps","title":"<code>get_raw_maps(force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Get raw maps (occupancy, speed, spkmap) from session data.</p> <p>This method processes session data to create spatial maps representing occupancy, speed, and neural activity across position bins. The maps are in raw format (not smoothed or normalized by occupancy).</p> <p>Parameters:</p> Name Type Description Default <code>force_recompute</code> <code>bool</code> <p>Whether to force recomputation even if cached data exists. Default is False.</p> <code>False</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after processing. Default is True.</p> <code>True</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for processing. If None, uses instance parameters. If a dict, updates instance parameters temporarily. Parameters are restored after method execution. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Maps instance containing raw occupancy, speed, and spike maps. Shape: (trials, positions) for occmap/speedmap, (trials, positions, rois) for spkmap.</p> Notes <p>The method: 1. Bins positions according to dist_step 2. Filters by speed threshold 3. Computes occupancy, speed, and spike maps 4. Sets unvisited position bins to NaN 5. Optionally standardizes spike data</p> <p>Results are cached based on parameter hash for efficient reuse.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"raw_maps\", disable=False)\ndef get_raw_maps(\n    self,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Maps:\n    \"\"\"Get raw maps (occupancy, speed, spkmap) from session data.\n\n    This method processes session data to create spatial maps representing\n    occupancy, speed, and neural activity across position bins. The maps\n    are in raw format (not smoothed or normalized by occupancy).\n\n    Parameters\n    ----------\n    force_recompute : bool, optional\n        Whether to force recomputation even if cached data exists. Default is False.\n    clear_one_cache : bool, optional\n        Whether to clear the onefile cache after processing. Default is True.\n    params : SpkmapParams, dict, or None, optional\n        Parameters for processing. If None, uses instance parameters.\n        If a dict, updates instance parameters temporarily.\n        Parameters are restored after method execution. Default is None.\n\n    Returns\n    -------\n    Maps\n        Maps instance containing raw occupancy, speed, and spike maps.\n        Shape: (trials, positions) for occmap/speedmap,\n        (trials, positions, rois) for spkmap.\n\n    Notes\n    -----\n    The method:\n    1. Bins positions according to dist_step\n    2. Filters by speed threshold\n    3. Computes occupancy, speed, and spike maps\n    4. Sets unvisited position bins to NaN\n    5. Optionally standardizes spike data\n\n    Results are cached based on parameter hash for efficient reuse.\n    \"\"\"\n    dist_edges = self.dist_edges\n    dist_centers = self.dist_centers\n    num_positions = len(dist_centers)\n\n    # Get behavioral timestamps and positions\n    timestamps, positions, trial_numbers, idx_behave_to_frame = self.session.positions\n\n    # compute behavioral speed on each sample\n    within_trial_sample = np.append(np.diff(trial_numbers) == 0, True)\n    sample_duration = np.append(np.diff(timestamps), 0)\n    speeds = np.append(np.diff(positions) / sample_duration[:-1], 0)\n    # do this after division so no /0 errors\n    sample_duration = sample_duration * within_trial_sample\n    # speed 0 in last sample for each trial (it's undefined)\n    speeds = speeds * within_trial_sample\n    # Convert positions to position bins\n    position_bin = np.digitize(positions, dist_edges) - 1\n\n    # get imaging information\n    frame_time_stamps = self.session.timestamps\n    sampling_period = np.median(np.diff(frame_time_stamps))\n    dist_cutoff = sampling_period / 2\n    delay_position_to_imaging = frame_time_stamps[idx_behave_to_frame] - timestamps\n\n    # get spiking information\n    spks = self.session.spks\n    num_rois = self.session.get_value(\"numROIs\")\n\n    # Do standardization\n    if self.params.standardize_spks:\n        spks = median_zscore(spks, median_subtract=not self.session.zero_baseline_spks)\n\n    # Get high resolution occupancy and speed maps\n    dtype = np.float32\n    occmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    speedmap = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n    spkmap = np.zeros((self.session.num_trials, num_positions, num_rois), dtype=dtype)\n    extra_counts = np.zeros((self.session.num_trials, num_positions), dtype=dtype)\n\n    # Get maps -- doing this independently for each map allows for more\n    # flexibility in which data to load (basically the occmap &amp; speedmap\n    # are instantaneous, but the spkmap is a bit slower)\n    get_summation_map(\n        sample_duration,\n        trial_numbers,\n        position_bin,\n        occmap,\n        counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=False,\n        use_sample_to_value_idx=False,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n    get_summation_map(\n        speeds,\n        trial_numbers,\n        position_bin,\n        speedmap,\n        counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=True,\n        use_sample_to_value_idx=False,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n    get_summation_map(\n        spks,\n        trial_numbers,\n        position_bin,\n        spkmap,\n        extra_counts,\n        speeds,\n        self.params.speed_threshold,\n        self.params.speed_max_allowed,\n        delay_position_to_imaging,\n        dist_cutoff,\n        sample_duration,\n        scale_by_sample_duration=True,\n        use_sample_to_value_idx=True,\n        sample_to_value_idx=idx_behave_to_frame,\n    )\n\n    # Figure out the valid range (outside of this range, set the maps to nan, because their values are not meaningful)\n    position_bin_per_trial = [position_bin[trial_numbers == tnum] for tnum in range(self.session.num_trials)]\n\n    # offsetting by 1 because there is a bug in the vrControl software where the first sample is always set\n    # to the minimum position (which is 0), but if there is a built-up buffer in the rotary encoder, the position\n    # will jump at the second sample. In general this will always work unless the mice have a truly ridiculous\n    # speed at the beginning of the trial...\n    first_valid_bin = [np.min(bpb[1:] if len(bpb) &gt; 1 else bpb) for bpb in position_bin_per_trial]\n    last_valid_bin = [np.max(bpb) for bpb in position_bin_per_trial]\n\n    # set bins to nan when mouse didn't visit them\n    occmap = replace_missing_data(occmap, first_valid_bin, last_valid_bin)\n    speedmap = replace_missing_data(speedmap, first_valid_bin, last_valid_bin)\n    spkmap = replace_missing_data(spkmap, first_valid_bin, last_valid_bin)\n\n    return Maps.create_raw_maps(occmap, speedmap, spkmap)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_reliability","title":"<code>get_reliability(use_session_filters=True, force_recompute=False, clear_one_cache=True, params=None)</code>","text":"<p>Calculate reliability of spike maps across trials.</p> <p>Reliability measures how consistent neural activity is across trials within each environment. Multiple methods are supported.</p> <p>Parameters:</p> Name Type Description Default <code>use_session_filters</code> <code>bool</code> <p>Whether to filter ROIs using session.idx_rois. Default is True.</p> <code>True</code> <code>force_recompute</code> <code>bool</code> <p>Whether to force recomputation even if cached data exists. Default is False.</p> <code>False</code> <code>clear_one_cache</code> <code>bool</code> <p>Whether to clear the onefile cache after processing. Default is True.</p> <code>True</code> <code>params</code> <code>SpkmapParams, dict, or None</code> <p>Parameters for processing. If None, uses instance parameters. If a dict, updates instance parameters temporarily. Parameters are restored after method execution. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Reliability</code> <p>Reliability instance containing reliability values for each ROI in each environment. Shape: (num_environments, num_rois).</p> Notes <p>Supported reliability methods: - \"leave_one_out\": Leave-one-out cross-validation - \"correlation\": Correlation between trial pairs - \"mse\": Mean squared error between trial pairs</p> <p>All reliability measures require maps with no NaN positions.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@with_temp_params\n@manage_one_cache\n@cached_processor(\"reliability\", disable=False)\ndef get_reliability(\n    self,\n    use_session_filters: bool = True,\n    force_recompute: bool = False,\n    clear_one_cache: bool = True,\n    params: Union[SpkmapParams, Dict[str, Any], None] = None,\n) -&gt; Reliability:\n    \"\"\"Calculate reliability of spike maps across trials.\n\n    Reliability measures how consistent neural activity is across trials\n    within each environment. Multiple methods are supported.\n\n    Parameters\n    ----------\n    use_session_filters : bool, optional\n        Whether to filter ROIs using session.idx_rois. Default is True.\n    force_recompute : bool, optional\n        Whether to force recomputation even if cached data exists. Default is False.\n    clear_one_cache : bool, optional\n        Whether to clear the onefile cache after processing. Default is True.\n    params : SpkmapParams, dict, or None, optional\n        Parameters for processing. If None, uses instance parameters.\n        If a dict, updates instance parameters temporarily.\n        Parameters are restored after method execution. Default is None.\n\n    Returns\n    -------\n    Reliability\n        Reliability instance containing reliability values for each ROI\n        in each environment. Shape: (num_environments, num_rois).\n\n    Notes\n    -----\n    Supported reliability methods:\n    - \"leave_one_out\": Leave-one-out cross-validation\n    - \"correlation\": Correlation between trial pairs\n    - \"mse\": Mean squared error between trial pairs\n\n    All reliability measures require maps with no NaN positions.\n    \"\"\"\n    envnum = helpers.check_iterable(self.session.environments)\n\n    # A list of the requested environments (all if not specified)\n    maps = self.get_env_maps(\n        use_session_filters=use_session_filters,\n        force_recompute=force_recompute,\n        clear_one_cache=clear_one_cache,\n        params={\"autosave\": False},  # Prevent saving in the case of a recompute\n    )\n\n    # All reliability measures require no NaNs\n    maps.pop_nan_positions()\n\n    if self.params.reliability_method == \"leave_one_out\":\n        rel_values = [helpers.reliability_loo(spkmap) for spkmap in maps.spkmap]\n    elif self.params.reliability_method == \"correlation\" or self.params.reliability_method == \"mse\":\n        rel_mse, rel_cor = helpers.named_transpose([helpers.measureReliability(spkmap) for spkmap in maps.spkmap])\n        rel_values = rel_mse if self.params.reliability_method == \"mse\" else rel_cor\n    else:\n        raise ValueError(f\"Method {self.params.reliability_method} not supported\")\n\n    return Reliability(\n        np.stack(rel_values),\n        environments=envnum,\n        method=self.params.reliability_method,\n    )\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.get_traversals","title":"<code>get_traversals(idx_roi, idx_env, width=10, placefield_threshold=5.0, fill_nan=False, spks=None, spks_prediction=None)</code>","text":"<p>Extract neural activity around place field peak during traversals.</p> <p>This method identifies trials where the animal passes through a neuron's place field peak and extracts activity windows around those moments.</p> <p>Parameters:</p> Name Type Description Default <code>idx_roi</code> <code>int</code> <p>Index of the ROI (neuron) to analyze.</p> required <code>idx_env</code> <code>int</code> <p>Index of the environment to analyze (index into env_maps.environments).</p> required <code>width</code> <code>int</code> <p>Number of frames on each side of the peak to include. Total window size is 2*width + 1. Default is 10.</p> <code>10</code> <code>placefield_threshold</code> <code>float</code> <p>Maximum distance from place field peak to include a trial (in spatial units). Default is 5.0.</p> <code>5.0</code> <code>fill_nan</code> <code>bool</code> <p>Whether to fill NaN values with 0. Default is False.</p> <code>False</code> <code>spks</code> <code>ndarray</code> <p>Spike data array. If None, loads from session. Default is None.</p> <code>None</code> <code>spks_prediction</code> <code>ndarray</code> <p>Place field prediction array. If None, computes it. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - traversals: Array of shape (num_traversals, 2width+1) containing   actual neural activity around each traversal. - pred_travs: Array of shape (num_traversals, 2width+1) containing   predicted activity around each traversal.</p> Notes <p>Only includes trials where the animal passes within placefield_threshold of the place field peak. The peak is determined from the averaged spike map for the specified ROI and environment.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def get_traversals(\n    self,\n    idx_roi: int,\n    idx_env: int,\n    width: int = 10,\n    placefield_threshold: float = 5.0,\n    fill_nan: bool = False,\n    spks: np.ndarray = None,\n    spks_prediction: np.ndarray = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Extract neural activity around place field peak during traversals.\n\n    This method identifies trials where the animal passes through a neuron's\n    place field peak and extracts activity windows around those moments.\n\n    Parameters\n    ----------\n    idx_roi : int\n        Index of the ROI (neuron) to analyze.\n    idx_env : int\n        Index of the environment to analyze (index into env_maps.environments).\n    width : int, optional\n        Number of frames on each side of the peak to include. Total window\n        size is 2*width + 1. Default is 10.\n    placefield_threshold : float, optional\n        Maximum distance from place field peak to include a trial (in spatial units).\n        Default is 5.0.\n    fill_nan : bool, optional\n        Whether to fill NaN values with 0. Default is False.\n    spks : np.ndarray, optional\n        Spike data array. If None, loads from session. Default is None.\n    spks_prediction : np.ndarray, optional\n        Place field prediction array. If None, computes it. Default is None.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - traversals: Array of shape (num_traversals, 2*width+1) containing\n          actual neural activity around each traversal.\n        - pred_travs: Array of shape (num_traversals, 2*width+1) containing\n          predicted activity around each traversal.\n\n    Notes\n    -----\n    Only includes trials where the animal passes within placefield_threshold\n    of the place field peak. The peak is determined from the averaged spike\n    map for the specified ROI and environment.\n    \"\"\"\n    frame_position, _, frame_environment, frame_trial = self.get_frame_behavior()\n    if spks_prediction is None:\n        spks_prediction = self.get_placefield_prediction(use_session_filters=True)[0]\n    if spks is None:\n        spks = self.session.spks[:, self.session.idx_rois]\n\n    if spks.shape != spks_prediction.shape:\n        raise ValueError(\"spks and spks_prediction must have the same shape\")\n\n    env_maps = self.get_env_maps()\n    pos_peak = self.dist_centers[np.nanargmax(np.nanmean(env_maps.spkmap[idx_env][idx_roi], axis=0))]\n    envnum = env_maps.environments[idx_env]\n\n    env_trials = np.unique(frame_trial[frame_environment == envnum])\n\n    num_trials = len(env_trials)\n    idx_traversal = -1 * np.ones(num_trials, dtype=int)\n    for itrial, trialnum in enumerate(env_trials):\n        idx_trial = frame_trial == trialnum\n        idx_closest_pos = np.nanargmin(np.abs(frame_position - pos_peak) + 10000 * ~idx_trial)\n\n        # Only include the trial if the closest position is within placefield threshold of the peak\n        if np.abs(frame_position[idx_closest_pos] - pos_peak) &lt; placefield_threshold:\n            idx_traversal[itrial] = idx_closest_pos\n\n    # Filter out trials that don't have a traversal\n    idx_traversal = idx_traversal[idx_traversal != -1]\n\n    # Get traversals through place field in requested environment\n    traversals = np.zeros((len(idx_traversal), width * 2 + 1))\n    pred_travs = np.zeros((len(idx_traversal), width * 2 + 1))\n    for ii, it in enumerate(idx_traversal):\n        istart = it - width\n        iend = it + width + 1\n        istartoffset = max(0, -istart)\n        iendoffset = max(0, iend - spks.shape[0])\n        traversals[ii, istartoffset : width * 2 + 1 - iendoffset] = spks[istart + istartoffset : iend - iendoffset, idx_roi]\n        pred_travs[ii, istartoffset : width * 2 + 1 - iendoffset] = spks_prediction[istart + istartoffset : iend - iendoffset, idx_roi]\n\n    if fill_nan:\n        traversals[np.isnan(traversals)] = 0.0\n        pred_travs[np.isnan(pred_travs)] = 0.0\n\n    return traversals, pred_travs\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.load_from_cache","title":"<code>load_from_cache(data_type)</code>","text":"<p>Load cached parameters and data for a given data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>Type of cached data to load.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: - The cached data (Maps or Reliability), or None if not found - A boolean indicating whether valid cache was found</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def load_from_cache(self, data_type: str) -&gt; Tuple[Union[Maps, Reliability, None], bool]:\n    \"\"\"Load cached parameters and data for a given data type.\n\n    Parameters\n    ----------\n    data_type : str\n        Type of cached data to load.\n\n    Returns\n    -------\n    tuple\n        A tuple containing:\n        - The cached data (Maps or Reliability), or None if not found\n        - A boolean indicating whether valid cache was found\n    \"\"\"\n    cache_dir = self.cache_directory(data_type)\n    if cache_dir.exists():\n        # If the directory exists, check if there are any cached params that match the expected hash\n        params_hash = self._params_hash(data_type)\n        cached_params_path = cache_dir / f\"params_{params_hash}.npz\"\n        if cached_params_path.exists():\n            cached_params = dict(np.load(cached_params_path))\n            # Check if the cached params match the dependent params\n            if self.check_params_match(cached_params):\n                return self._load_from_cache(data_type, params_hash, params=cached_params), True\n    return None, False\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.save_cache","title":"<code>save_cache(data_type, data)</code>","text":"<p>Save the cached parameters and data for a given data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>Type of data being cached (\"raw_maps\", \"processed_maps\", \"env_maps\", or \"reliability\").</p> required <code>data</code> <code>Maps or Reliability</code> <p>The data object to cache.</p> required Notes <p>Creates the cache directory if it doesn't exist. Saves parameters as an NPZ file and data as NPY files, using a hash of the parameters in the filenames.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def save_cache(self, data_type: str, data: Union[Maps, Reliability]) -&gt; None:\n    \"\"\"Save the cached parameters and data for a given data type.\n\n    Parameters\n    ----------\n    data_type : str\n        Type of data being cached (\"raw_maps\", \"processed_maps\", \"env_maps\", or \"reliability\").\n    data : Maps or Reliability\n        The data object to cache.\n\n    Notes\n    -----\n    Creates the cache directory if it doesn't exist. Saves parameters as an NPZ file\n    and data as NPY files, using a hash of the parameters in the filenames.\n    \"\"\"\n    cache_dir = self.cache_directory(data_type)\n    params_hash = self._params_hash(data_type)\n    cache_param_path = cache_dir / f\"params_{params_hash}.npz\"\n    if not cache_dir.exists():\n        cache_dir.mkdir(parents=True, exist_ok=True)\n    np.savez(cache_param_path, **self.dependent_params(data_type))\n    if data_type == \"raw_maps\" or data_type == \"processed_maps\":\n        for mapname in Maps.map_types():\n            cache_data_path = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n            np.save(cache_data_path, getattr(data, mapname))\n    elif data_type == \"env_maps\":\n        environments = data.environments\n        np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n        for ienv, env in enumerate(environments):\n            for mapname in Maps.map_types():\n                cache_data_path = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                np.save(cache_data_path, getattr(data, mapname)[ienv])\n    elif data_type == \"reliability\":\n        values = data.values\n        environments = data.environments\n        # don't need data.method because it's in params...\n        np.save(cache_dir / f\"data_environments_{params_hash}.npy\", environments)\n        np.save(cache_dir / f\"data_reliability_{params_hash}.npy\", values)\n    else:\n        raise ValueError(f\"Unknown data type: {data_type}\")\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapProcessor.show_cache","title":"<code>show_cache(data_type=None)</code>","text":"<p>Helper function that scrapes the cache directory and shows cached files</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>Optional[str]</code> <p>Indicate a data type to filter which parts of the cache to show</p> <code>None</code> Notes <p>Prints a formatted table showing cache information including data_type, size, parameters, and modification date. If no cache directory exists, prints a message.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def show_cache(self, data_type: Optional[str] = None) -&gt; None:\n    \"\"\"Helper function that scrapes the cache directory and shows cached files\n\n    Parameters\n    ----------\n    data_type: Optional[str] = None\n        Indicate a data type to filter which parts of the cache to show\n\n    Notes\n    -----\n    Prints a formatted table showing cache information including data_type, size,\n    parameters, and modification date. If no cache directory exists, prints a message.\n    \"\"\"\n    import os\n    from datetime import datetime\n\n    # Get the base cache directory\n    base_cache_dir = self.cache_directory()\n\n    if not base_cache_dir.exists():\n        print(f\"No cache directory found at: {base_cache_dir}\")\n        return\n\n    # Collect information about all cache files\n    cache_info = []\n\n    # Define the data types to check\n    if data_type is not None:\n        data_types_to_check = [data_type]\n    else:\n        data_types_to_check = [\"raw_maps\", \"processed_maps\", \"env_maps\", \"reliability\"]\n\n    for dt in data_types_to_check:\n        cache_dir = self.cache_directory(dt)\n        if not cache_dir.exists():\n            continue\n\n        # Find all parameter files (they define what caches exist)\n        param_files = list(cache_dir.glob(\"params_*.npz\"))\n\n        for param_file in param_files:\n            # Extract the hash from the filename\n            params_hash = param_file.stem.replace(\"params_\", \"\")\n\n            # Load the parameters\n            try:\n                cached_params = dict(np.load(param_file))\n                param_str = \", \".join([f\"{k}={v}\" for k, v in cached_params.items()])\n            except Exception as e:\n                param_str = f\"Error loading params: {e}\"\n\n            # Get file modification time\n            mod_time = datetime.fromtimestamp(param_file.stat().st_mtime)\n            date_str = mod_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            # Calculate total size of all related cache files\n            total_size = param_file.stat().st_size\n\n            if dt in [\"raw_maps\", \"processed_maps\"]:\n                # For maps, look for data files for each map type\n                for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                    data_file = cache_dir / f\"data_{mapname}_{params_hash}.npy\"\n                    if data_file.exists():\n                        total_size += data_file.stat().st_size\n\n            elif dt == \"env_maps\":\n                # For env_maps, look for environment file and individual environment data files\n                env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                if env_file.exists():\n                    total_size += env_file.stat().st_size\n                    # Load environments to find all data files\n                    try:\n                        environments = np.load(env_file)\n                        for env in environments:\n                            for mapname in [\"occmap\", \"speedmap\", \"spkmap\"]:\n                                data_file = cache_dir / f\"data_{mapname}_{env}_{params_hash}.npy\"\n                                if data_file.exists():\n                                    total_size += data_file.stat().st_size\n                    except Exception:\n                        pass  # Continue even if we can't load environments\n\n            elif dt == \"reliability\":\n                # For reliability, look for environments and reliability data files\n                env_file = cache_dir / f\"data_environments_{params_hash}.npy\"\n                rel_file = cache_dir / f\"data_reliability_{params_hash}.npy\"\n                if env_file.exists():\n                    total_size += env_file.stat().st_size\n                if rel_file.exists():\n                    total_size += rel_file.stat().st_size\n\n            # Convert size to human readable format\n            size_str = self._format_file_size(total_size)\n\n            cache_info.append(\n                {\n                    \"data_type\": dt,\n                    \"size\": size_str,\n                    \"parameters\": param_str,\n                    \"date\": date_str,\n                    \"hash\": params_hash[:8],  # Show first 8 chars of hash\n                }\n            )\n\n    if not cache_info:\n        print(\"No cache files found.\")\n        return\n\n    # Format the output as a table\n    output_lines = []\n    output_lines.append(\"Cache Files Summary\")\n    output_lines.append(\"=\" * 80)\n    output_lines.append(f\"{'Data Type':&lt;15} {'Size':&lt;10} {'Date':&lt;20} {'Hash':&lt;10} {'Parameters'}\")\n    output_lines.append(\"-\" * 80)\n\n    for info in cache_info:\n        output_lines.append(f\"{info['data_type']:&lt;15} {info['size']:&lt;10} {info['date']:&lt;20} \" f\"{info['hash']:&lt;10} {info['parameters']}\")\n\n    output_lines.append(\"-\" * 80)\n    output_lines.append(f\"Total cache entries: {len(cache_info)}\")\n\n    result = \"\\n\".join(output_lines)\n    print(result)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps","title":"<code>Maps</code>  <code>dataclass</code>","text":"<p>Container for occupancy, speed, and spike maps.</p> <p>This class holds spatial maps representing neural activity, behavioral occupancy, and speed across position bins. Maps can be organized either as single arrays (all trials combined) or as lists of arrays (separated by environment).</p> <p>Attributes:</p> Name Type Description <code>occmap</code> <code>np.ndarray or list of np.ndarray</code> <p>Occupancy map(s) representing time spent in each position bin. Shape: (trials, positions) for single array, or list of (trials, positions) arrays when by_environment=True.</p> <code>speedmap</code> <code>np.ndarray or list of np.ndarray</code> <p>Speed map(s) representing average speed in each position bin. Shape: (trials, positions) for single array, or list of (trials, positions) arrays when by_environment=True.</p> <code>spkmap</code> <code>np.ndarray or list of np.ndarray</code> <p>Spike map(s) representing neural activity in each position bin. Shape depends on rois_first: - If rois_first=True: (rois, trials, positions) or list of (rois, trials, positions) - If rois_first=False: (trials, positions, rois) or list of (trials, positions, rois)</p> <code>by_environment</code> <code>bool</code> <p>Whether maps are separated by environment (True) or combined (False).</p> <code>rois_first</code> <code>bool</code> <p>Whether ROI dimension is first (True) or last (False) in spkmap arrays.</p> <code>environments</code> <code>list of int, optional</code> <p>List of environment numbers when by_environment=True. Default is None.</p> <code>distcenters</code> <code>(ndarray, optional)</code> <p>Center positions of distance bins. Default is None.</p> <code>_averaged</code> <code>bool</code> <p>Internal flag indicating whether trials have been averaged. Default is False.</p> Notes <p>The Maps class supports two organizational modes: 1. Single maps: All trials combined in single arrays (by_environment=False) 2. Environment-separated maps: Maps split by environment (by_environment=True)</p> <p>The spkmap can have ROIs as the first or last dimension depending on rois_first. This allows flexibility in how data is organized for different processing steps.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass Maps:\n    \"\"\"Container for occupancy, speed, and spike maps.\n\n    This class holds spatial maps representing neural activity, behavioral occupancy,\n    and speed across position bins. Maps can be organized either as single arrays\n    (all trials combined) or as lists of arrays (separated by environment).\n\n    Attributes\n    ----------\n    occmap : np.ndarray or list of np.ndarray\n        Occupancy map(s) representing time spent in each position bin.\n        Shape: (trials, positions) for single array, or list of (trials, positions)\n        arrays when by_environment=True.\n    speedmap : np.ndarray or list of np.ndarray\n        Speed map(s) representing average speed in each position bin.\n        Shape: (trials, positions) for single array, or list of (trials, positions)\n        arrays when by_environment=True.\n    spkmap : np.ndarray or list of np.ndarray\n        Spike map(s) representing neural activity in each position bin.\n        Shape depends on rois_first:\n        - If rois_first=True: (rois, trials, positions) or list of (rois, trials, positions)\n        - If rois_first=False: (trials, positions, rois) or list of (trials, positions, rois)\n    by_environment : bool\n        Whether maps are separated by environment (True) or combined (False).\n    rois_first : bool\n        Whether ROI dimension is first (True) or last (False) in spkmap arrays.\n    environments : list of int, optional\n        List of environment numbers when by_environment=True. Default is None.\n    distcenters : np.ndarray, optional\n        Center positions of distance bins. Default is None.\n    _averaged : bool\n        Internal flag indicating whether trials have been averaged. Default is False.\n\n    Notes\n    -----\n    The Maps class supports two organizational modes:\n    1. Single maps: All trials combined in single arrays (by_environment=False)\n    2. Environment-separated maps: Maps split by environment (by_environment=True)\n\n    The spkmap can have ROIs as the first or last dimension depending on rois_first.\n    This allows flexibility in how data is organized for different processing steps.\n    \"\"\"\n\n    occmap: np.ndarray | list[np.ndarray]\n    speedmap: np.ndarray | list[np.ndarray]\n    spkmap: np.ndarray | list[np.ndarray]\n    by_environment: bool\n    rois_first: bool\n    environments: list[int] | None = None\n    distcenters: np.ndarray | None = None\n    _averaged: bool = field(default=False, init=False)\n\n    def __post_init__(self):\n        if self.occmap is None or self.speedmap is None or self.spkmap is None:\n            raise ValueError(\"occmap, speedmap, and spkmap must be provided\")\n\n        if self.by_environment:\n            if self.environments is None:\n                raise ValueError(\"environments must be provided if by_environment is True\")\n            if not isinstance(self.occmap, list) or not isinstance(self.speedmap, list) or not isinstance(self.spkmap, list):\n                raise ValueError(\"occmap, speedmap, and spkmap must be lists if by_environment is True\")\n        else:\n            if isinstance(self.occmap, list) or isinstance(self.speedmap, list) or isinstance(self.spkmap, list):\n                raise ValueError(\"occmap, speedmap, and spkmap must be single arrays if by_environment is False\")\n\n        if not self.by_environment:\n            spkmap_shape = self.spkmap.shape[1:] if self.rois_first else self.spkmap.shape[:2]\n            if not (self.occmap.shape == self.speedmap.shape == spkmap_shape):\n                raise ValueError(\"occmap, speedmap, and spkmap must have the same shape\")\n        else:\n            if not (len(self.occmap) == len(self.speedmap) == len(self.spkmap) == len(self.environments)):\n                raise ValueError(\"occmap, speedmap, and spkmap must have the same number of environments\")\n            for i in range(len(self.environments)):\n                spkmap_shape = self.spkmap[i].shape[1:] if self.rois_first else self.spkmap[i].shape[:2]\n                if not (self.occmap[i].shape == self.speedmap[i].shape == spkmap_shape):\n                    raise ValueError(\"occmap, speedmap, and spkmap must have the same shape for each environment\")\n            roi_axis = 0 if self.rois_first else -1\n            rois_per_env = [spkmap.shape[roi_axis] for spkmap in self.spkmap]\n            if not all([rpe == rois_per_env[0] for rpe in rois_per_env]):\n                raise ValueError(\"All environments must have the same number of ROIs\")\n\n    def __repr__(self) -&gt; str:\n        # Get number of positions\n        if self.by_environment:\n            num_positions = self.occmap[0].shape[-1]\n        else:\n            num_positions = self.occmap.shape[-1]\n        # Get number of trials\n        if self._averaged:\n            num_trials = \"averaged\"\n        else:\n            if self.by_environment:\n                num_trials = [occmap.shape[0] for occmap in self.occmap]\n                num_trials = \"{\" + \", \".join([str(nt) for nt in num_trials]) + \"}\"\n            else:\n                num_trials = self.occmap.shape[0]\n        # Get number of ROIs\n        if self.by_environment:\n            num_rois = self.spkmap[0].shape[0] if self.rois_first else self.spkmap[0].shape[1]\n        else:\n            num_rois = self.spkmap.shape[0] if self.rois_first else self.spkmap.shape[1]\n        environments = f\", environments={{{', '.join([str(env) for env in self.environments])}}}\" if self.by_environment else \"\"\n        return f\"Maps(num_trials={num_trials}, num_positions={num_positions}, num_rois={num_rois}{environments}, rois_first={self.rois_first})\"\n\n    @classmethod\n    def create_raw_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n        \"\"\"Create a Maps instance from raw (unprocessed) map data.\n\n        Parameters\n        ----------\n        occmap : np.ndarray\n            Occupancy map with shape (trials, positions).\n        speedmap : np.ndarray\n            Speed map with shape (trials, positions).\n        spkmap : np.ndarray\n            Spike map with shape (trials, positions, rois).\n        distcenters : np.ndarray, optional\n            Center positions of distance bins. Default is None.\n\n        Returns\n        -------\n        Maps\n            Maps instance with by_environment=False and rois_first=False.\n        \"\"\"\n        return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=False)\n\n    @classmethod\n    def create_processed_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n        \"\"\"Create a Maps instance from processed map data.\n\n        Parameters\n        ----------\n        occmap : np.ndarray\n            Occupancy map with shape (trials, positions).\n        speedmap : np.ndarray\n            Speed map with shape (trials, positions).\n        spkmap : np.ndarray\n            Spike map with shape (rois, trials, positions).\n        distcenters : np.ndarray, optional\n            Center positions of distance bins. Default is None.\n\n        Returns\n        -------\n        Maps\n            Maps instance with by_environment=False and rois_first=True.\n        \"\"\"\n        return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=True)\n\n    @classmethod\n    def create_environment_maps(\n        cls,\n        occmap: list[np.ndarray],\n        speedmap: list[np.ndarray],\n        spkmap: list[np.ndarray],\n        environments: list[int],\n        distcenters: np.ndarray = None,\n    ) -&gt; \"Maps\":\n        \"\"\"Create a Maps instance with maps separated by environment.\n\n        Parameters\n        ----------\n        occmap : list of np.ndarray\n            List of occupancy maps, one per environment. Each with shape (trials, positions).\n        speedmap : list of np.ndarray\n            List of speed maps, one per environment. Each with shape (trials, positions).\n        spkmap : list of np.ndarray\n            List of spike maps, one per environment. Each with shape (rois, trials, positions).\n        environments : list of int\n            List of environment numbers corresponding to each map in the lists.\n        distcenters : np.ndarray, optional\n            Center positions of distance bins. Default is None.\n\n        Returns\n        -------\n        Maps\n            Maps instance with by_environment=True and rois_first=True.\n        \"\"\"\n        return cls(\n            occmap=occmap,\n            speedmap=speedmap,\n            spkmap=spkmap,\n            distcenters=distcenters,\n            environments=environments,\n            by_environment=True,\n            rois_first=True,\n        )\n\n    @classmethod\n    def map_types(cls) -&gt; List[str]:\n        \"\"\"Get the list of map type names.\n\n        Returns\n        -------\n        list of str\n            List containing [\"occmap\", \"speedmap\", \"spkmap\"].\n        \"\"\"\n        return [\"occmap\", \"speedmap\", \"spkmap\"]\n\n    def __getitem__(self, key: str) -&gt; np.ndarray | list[np.ndarray]:\n        \"\"\"Get a map by name using dictionary-like access.\n\n        Parameters\n        ----------\n        key : str\n            Name of the map to retrieve (\"occmap\", \"speedmap\", or \"spkmap\").\n\n        Returns\n        -------\n        np.ndarray or list of np.ndarray\n            The requested map array(s).\n        \"\"\"\n        return getattr(self, key)\n\n    def __setitem__(self, key: str, value: np.ndarray | list[np.ndarray]) -&gt; None:\n        \"\"\"Set a map by name using dictionary-like access.\n\n        Parameters\n        ----------\n        key : str\n            Name of the map to set (\"occmap\", \"speedmap\", or \"spkmap\").\n        value : np.ndarray or list of np.ndarray\n            The map array(s) to assign.\n        \"\"\"\n        setattr(self, key, value)\n\n    def _get_position_axis(self, mapname: str) -&gt; int:\n        \"\"\"Get the axis index for the position dimension.\n\n        Parameters\n        ----------\n        mapname : str\n            Name of the map (\"occmap\", \"speedmap\", or \"spkmap\").\n\n        Returns\n        -------\n        int\n            Axis index for the position dimension. Typically -1 (last axis),\n            except for spkmap when rois_first=False, where it's -2.\n\n        Notes\n        -----\n        The only time the position axis isn't the last one is for spkmap when\n        rois_first is False, where the shape is (trials, positions, rois).\n        \"\"\"\n        average_offset = -1 if self._averaged else 0\n        if mapname == \"spkmap\" and not self.rois_first:\n            return -2 + average_offset\n        else:\n            return -1\n\n    def filter_positions(self, idx_positions: np.ndarray) -&gt; None:\n        \"\"\"Filter maps to keep only specified position bins.\n\n        Parameters\n        ----------\n        idx_positions : np.ndarray\n            Indices of position bins to keep. Must be a 1D array of integers.\n\n        Notes\n        -----\n        This method modifies the maps in-place, keeping only the position bins\n        specified by idx_positions. Also updates distcenters if present.\n        \"\"\"\n        if self.distcenters is not None:\n            self.distcenters = self.distcenters[idx_positions]\n        for mapname in self.map_types():\n            axis = self._get_position_axis(mapname)\n            if self.by_environment:\n                self[mapname] = [np.take(x, idx_positions, axis=axis) for x in self[mapname]]\n            else:\n                self[mapname] = np.take(self[mapname], idx_positions, axis=axis)\n\n    def filter_rois(self, idx_rois: np.ndarray) -&gt; None:\n        \"\"\"Filter spike maps to keep only specified ROIs.\n\n        Parameters\n        ----------\n        idx_rois : np.ndarray\n            Indices of ROIs to keep. Must be a 1D array of integers.\n\n        Notes\n        -----\n        This method modifies the spkmap in-place, keeping only the ROIs\n        specified by idx_rois. Only affects spkmap; occmap and speedmap\n        are unchanged.\n        \"\"\"\n        axis = 0 if self.rois_first else -1\n        if self.by_environment:\n            self.spkmap = [np.take(x, idx_rois, axis=axis) for x in self.spkmap]\n        else:\n            self.spkmap = np.take(self.spkmap, idx_rois, axis=axis)\n\n    def filter_environments(self, environments: list[int]) -&gt; None:\n        \"\"\"Filter maps to keep only specified environments.\n\n        Parameters\n        ----------\n        environments : list of int\n            List of environment numbers to keep.\n\n        Raises\n        ------\n        ValueError\n            If by_environment is False, since environments cannot be filtered\n            when maps are not separated by environment.\n\n        Notes\n        -----\n        This method modifies the maps in-place, keeping only the environments\n        specified. Only works when by_environment=True.\n        \"\"\"\n        if self.by_environment:\n            idx_to_requested_env = [i for i, env in enumerate(self.environments) if env in environments]\n            self.occmap = [self.occmap[i] for i in idx_to_requested_env]\n            self.speedmap = [self.speedmap[i] for i in idx_to_requested_env]\n            self.spkmap = [self.spkmap[i] for i in idx_to_requested_env]\n            self.environments = [self.environments[i] for i in idx_to_requested_env]\n        else:\n            raise ValueError(\"Cannot filter environments when maps aren't separated by environment!\")\n\n    def pop_nan_positions(self) -&gt; None:\n        \"\"\"Remove position bins that contain NaN values in any map.\n\n        Notes\n        -----\n        This method identifies position bins that have NaN values in any of the\n        maps (occmap, speedmap, or spkmap) and removes them from all maps.\n        Useful for cleaning data before analysis.\n        \"\"\"\n        if self.by_environment:\n            idx_valid_positions = np.where(~np.any(np.stack([np.any(np.isnan(occmap), axis=0) for occmap in self.occmap], axis=0), axis=0))[0]\n        else:\n            idx_valid_positions = np.where(~np.any(np.isnan(self.occmap), axis=0))[0]\n        self.filter_positions(idx_valid_positions)\n\n    def smooth_maps(self, positions: np.ndarray, kernel_width: float) -&gt; None:\n        \"\"\"Smooth the maps using a Gaussian kernel.\n\n        Parameters\n        ----------\n        positions : np.ndarray\n            Position values corresponding to the position bins. Used to compute\n            the Gaussian kernel.\n        kernel_width : float\n            Width of the Gaussian smoothing kernel in spatial units.\n\n        Notes\n        -----\n        This method applies Gaussian smoothing to all maps (occmap, speedmap, spkmap).\n        NaN values are temporarily replaced with 0 during smoothing, then restored\n        afterward. The smoothing is applied along the position dimension.\n        \"\"\"\n        kernel = get_gauss_kernel(positions, kernel_width)\n\n        # Replace nans with 0s\n        if self.by_environment:\n            idxnan = [np.isnan(occmap) for occmap in self.occmap]\n        else:\n            idxnan = np.isnan(self.occmap)\n\n        if self.rois_first:\n            # Move the rois axis to the last axis\n            if self.by_environment:\n                self.spkmap = [np.moveaxis(map, 0, -1) for map in self.spkmap]\n            else:\n                self.spkmap = np.moveaxis(self.spkmap, 0, -1)\n\n        for mapname in self.map_types():\n            if self.by_environment:\n                for ienv, inanenv in enumerate(idxnan):\n                    self[mapname][ienv][inanenv] = 0\n            else:\n                self[mapname][idxnan] = 0\n\n        for mapname in self.map_types():\n            # Since we moved ROIs to the last axis position will be axis=1 for all map types\n            if self.by_environment:\n                self[mapname] = [convolve_toeplitz(map, kernel, axis=1) for map in self[mapname]]\n            else:\n                self[mapname] = convolve_toeplitz(self[mapname], kernel, axis=1)\n\n        # Put nans back in place\n        for mapname in self.map_types():\n            if self.by_environment:\n                for ienv, inanenv in enumerate(idxnan):\n                    self[mapname][ienv][inanenv] = np.nan\n            else:\n                self[mapname][idxnan] = np.nan\n\n        # Move the rois axis back to the first axis\n        if self.rois_first:\n            if self.by_environment:\n                self.spkmap = [np.moveaxis(map, -1, 0) for map in self.spkmap]\n            else:\n                self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n\n    def average_trials(self, keepdims: bool = False) -&gt; None:\n        \"\"\"Average the trials within each environment.\n\n        Parameters\n        ----------\n        keepdims : bool, optional\n            Whether to keep the trial dimension with size 1 after averaging.\n            Default is False.\n\n        Notes\n        -----\n        This method computes the mean across trials for each map. After averaging,\n        the _averaged flag is set to True to prevent redundant averaging.\n        The trial dimension is removed unless keepdims=True.\n        \"\"\"\n        if self._averaged:\n            return\n        for mapname in self.map_types():\n            axis = 1 if mapname == \"spkmap\" and self.rois_first else 0\n            if self.by_environment:\n                self[mapname] = [ss.mean(map, axis=axis, keepdims=keepdims) for map in self[mapname]]\n            else:\n                self[mapname] = ss.mean(self[mapname], axis=axis, keepdims=keepdims)\n        self._averaged = True\n\n    def nbytes(self) -&gt; int:\n        \"\"\"Calculate the total memory size of all maps in bytes.\n\n        Returns\n        -------\n        int\n            Total number of bytes used by all map arrays.\n        \"\"\"\n        num_bytes = 0\n        for name in self.map_types():\n            if self.by_environment:\n                num_bytes += sum(x.nbytes for x in getattr(self, name))\n            else:\n                num_bytes += getattr(self, name).nbytes\n        return num_bytes\n\n    def raw_to_processed(self, positions: np.ndarray, smooth_width: float | None = None) -&gt; \"Maps\":\n        \"\"\"Convert raw maps to processed maps.\n\n        Processing steps:\n        1. Optionally smooth maps with a Gaussian kernel\n        2. Divide speedmap and spkmap by occmap (correct_map)\n        3. Reorganize spkmap to have ROIs as the first dimension\n\n        Parameters\n        ----------\n        positions : np.ndarray\n            Position values corresponding to the position bins.\n        smooth_width : float, optional\n            Width of the Gaussian smoothing kernel. If None, no smoothing is applied.\n            Default is None.\n\n        Returns\n        -------\n        Maps\n            Self, with maps now in processed format (rois_first=True).\n\n        Notes\n        -----\n        This method modifies the maps in-place. After processing, spkmap will\n        have shape (rois, trials, positions) instead of (trials, positions, rois).\n        \"\"\"\n        if smooth_width is not None:\n            self.smooth_maps(positions, smooth_width)\n\n        self.speedmap = correct_map(self.occmap, self.speedmap)\n        self.spkmap = correct_map(self.occmap, self.spkmap)\n\n        # Change spkmap to be ROIs first\n        self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n        self.rois_first = True\n\n        return self\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get a map by name using dictionary-like access.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the map to retrieve (\"occmap\", \"speedmap\", or \"spkmap\").</p> required <p>Returns:</p> Type Description <code>np.ndarray or list of np.ndarray</code> <p>The requested map array(s).</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def __getitem__(self, key: str) -&gt; np.ndarray | list[np.ndarray]:\n    \"\"\"Get a map by name using dictionary-like access.\n\n    Parameters\n    ----------\n    key : str\n        Name of the map to retrieve (\"occmap\", \"speedmap\", or \"spkmap\").\n\n    Returns\n    -------\n    np.ndarray or list of np.ndarray\n        The requested map array(s).\n    \"\"\"\n    return getattr(self, key)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a map by name using dictionary-like access.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the map to set (\"occmap\", \"speedmap\", or \"spkmap\").</p> required <code>value</code> <code>np.ndarray or list of np.ndarray</code> <p>The map array(s) to assign.</p> required Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def __setitem__(self, key: str, value: np.ndarray | list[np.ndarray]) -&gt; None:\n    \"\"\"Set a map by name using dictionary-like access.\n\n    Parameters\n    ----------\n    key : str\n        Name of the map to set (\"occmap\", \"speedmap\", or \"spkmap\").\n    value : np.ndarray or list of np.ndarray\n        The map array(s) to assign.\n    \"\"\"\n    setattr(self, key, value)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.average_trials","title":"<code>average_trials(keepdims=False)</code>","text":"<p>Average the trials within each environment.</p> <p>Parameters:</p> Name Type Description Default <code>keepdims</code> <code>bool</code> <p>Whether to keep the trial dimension with size 1 after averaging. Default is False.</p> <code>False</code> Notes <p>This method computes the mean across trials for each map. After averaging, the _averaged flag is set to True to prevent redundant averaging. The trial dimension is removed unless keepdims=True.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def average_trials(self, keepdims: bool = False) -&gt; None:\n    \"\"\"Average the trials within each environment.\n\n    Parameters\n    ----------\n    keepdims : bool, optional\n        Whether to keep the trial dimension with size 1 after averaging.\n        Default is False.\n\n    Notes\n    -----\n    This method computes the mean across trials for each map. After averaging,\n    the _averaged flag is set to True to prevent redundant averaging.\n    The trial dimension is removed unless keepdims=True.\n    \"\"\"\n    if self._averaged:\n        return\n    for mapname in self.map_types():\n        axis = 1 if mapname == \"spkmap\" and self.rois_first else 0\n        if self.by_environment:\n            self[mapname] = [ss.mean(map, axis=axis, keepdims=keepdims) for map in self[mapname]]\n        else:\n            self[mapname] = ss.mean(self[mapname], axis=axis, keepdims=keepdims)\n    self._averaged = True\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.create_environment_maps","title":"<code>create_environment_maps(occmap, speedmap, spkmap, environments, distcenters=None)</code>  <code>classmethod</code>","text":"<p>Create a Maps instance with maps separated by environment.</p> <p>Parameters:</p> Name Type Description Default <code>occmap</code> <code>list of np.ndarray</code> <p>List of occupancy maps, one per environment. Each with shape (trials, positions).</p> required <code>speedmap</code> <code>list of np.ndarray</code> <p>List of speed maps, one per environment. Each with shape (trials, positions).</p> required <code>spkmap</code> <code>list of np.ndarray</code> <p>List of spike maps, one per environment. Each with shape (rois, trials, positions).</p> required <code>environments</code> <code>list of int</code> <p>List of environment numbers corresponding to each map in the lists.</p> required <code>distcenters</code> <code>ndarray</code> <p>Center positions of distance bins. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Maps instance with by_environment=True and rois_first=True.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef create_environment_maps(\n    cls,\n    occmap: list[np.ndarray],\n    speedmap: list[np.ndarray],\n    spkmap: list[np.ndarray],\n    environments: list[int],\n    distcenters: np.ndarray = None,\n) -&gt; \"Maps\":\n    \"\"\"Create a Maps instance with maps separated by environment.\n\n    Parameters\n    ----------\n    occmap : list of np.ndarray\n        List of occupancy maps, one per environment. Each with shape (trials, positions).\n    speedmap : list of np.ndarray\n        List of speed maps, one per environment. Each with shape (trials, positions).\n    spkmap : list of np.ndarray\n        List of spike maps, one per environment. Each with shape (rois, trials, positions).\n    environments : list of int\n        List of environment numbers corresponding to each map in the lists.\n    distcenters : np.ndarray, optional\n        Center positions of distance bins. Default is None.\n\n    Returns\n    -------\n    Maps\n        Maps instance with by_environment=True and rois_first=True.\n    \"\"\"\n    return cls(\n        occmap=occmap,\n        speedmap=speedmap,\n        spkmap=spkmap,\n        distcenters=distcenters,\n        environments=environments,\n        by_environment=True,\n        rois_first=True,\n    )\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.create_processed_maps","title":"<code>create_processed_maps(occmap, speedmap, spkmap, distcenters=None)</code>  <code>classmethod</code>","text":"<p>Create a Maps instance from processed map data.</p> <p>Parameters:</p> Name Type Description Default <code>occmap</code> <code>ndarray</code> <p>Occupancy map with shape (trials, positions).</p> required <code>speedmap</code> <code>ndarray</code> <p>Speed map with shape (trials, positions).</p> required <code>spkmap</code> <code>ndarray</code> <p>Spike map with shape (rois, trials, positions).</p> required <code>distcenters</code> <code>ndarray</code> <p>Center positions of distance bins. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Maps instance with by_environment=False and rois_first=True.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef create_processed_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n    \"\"\"Create a Maps instance from processed map data.\n\n    Parameters\n    ----------\n    occmap : np.ndarray\n        Occupancy map with shape (trials, positions).\n    speedmap : np.ndarray\n        Speed map with shape (trials, positions).\n    spkmap : np.ndarray\n        Spike map with shape (rois, trials, positions).\n    distcenters : np.ndarray, optional\n        Center positions of distance bins. Default is None.\n\n    Returns\n    -------\n    Maps\n        Maps instance with by_environment=False and rois_first=True.\n    \"\"\"\n    return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=True)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.create_raw_maps","title":"<code>create_raw_maps(occmap, speedmap, spkmap, distcenters=None)</code>  <code>classmethod</code>","text":"<p>Create a Maps instance from raw (unprocessed) map data.</p> <p>Parameters:</p> Name Type Description Default <code>occmap</code> <code>ndarray</code> <p>Occupancy map with shape (trials, positions).</p> required <code>speedmap</code> <code>ndarray</code> <p>Speed map with shape (trials, positions).</p> required <code>spkmap</code> <code>ndarray</code> <p>Spike map with shape (trials, positions, rois).</p> required <code>distcenters</code> <code>ndarray</code> <p>Center positions of distance bins. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Maps instance with by_environment=False and rois_first=False.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef create_raw_maps(cls, occmap: np.ndarray, speedmap: np.ndarray, spkmap: np.ndarray, distcenters: np.ndarray = None) -&gt; \"Maps\":\n    \"\"\"Create a Maps instance from raw (unprocessed) map data.\n\n    Parameters\n    ----------\n    occmap : np.ndarray\n        Occupancy map with shape (trials, positions).\n    speedmap : np.ndarray\n        Speed map with shape (trials, positions).\n    spkmap : np.ndarray\n        Spike map with shape (trials, positions, rois).\n    distcenters : np.ndarray, optional\n        Center positions of distance bins. Default is None.\n\n    Returns\n    -------\n    Maps\n        Maps instance with by_environment=False and rois_first=False.\n    \"\"\"\n    return cls(occmap=occmap, speedmap=speedmap, spkmap=spkmap, distcenters=distcenters, by_environment=False, rois_first=False)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.filter_environments","title":"<code>filter_environments(environments)</code>","text":"<p>Filter maps to keep only specified environments.</p> <p>Parameters:</p> Name Type Description Default <code>environments</code> <code>list of int</code> <p>List of environment numbers to keep.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If by_environment is False, since environments cannot be filtered when maps are not separated by environment.</p> Notes <p>This method modifies the maps in-place, keeping only the environments specified. Only works when by_environment=True.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def filter_environments(self, environments: list[int]) -&gt; None:\n    \"\"\"Filter maps to keep only specified environments.\n\n    Parameters\n    ----------\n    environments : list of int\n        List of environment numbers to keep.\n\n    Raises\n    ------\n    ValueError\n        If by_environment is False, since environments cannot be filtered\n        when maps are not separated by environment.\n\n    Notes\n    -----\n    This method modifies the maps in-place, keeping only the environments\n    specified. Only works when by_environment=True.\n    \"\"\"\n    if self.by_environment:\n        idx_to_requested_env = [i for i, env in enumerate(self.environments) if env in environments]\n        self.occmap = [self.occmap[i] for i in idx_to_requested_env]\n        self.speedmap = [self.speedmap[i] for i in idx_to_requested_env]\n        self.spkmap = [self.spkmap[i] for i in idx_to_requested_env]\n        self.environments = [self.environments[i] for i in idx_to_requested_env]\n    else:\n        raise ValueError(\"Cannot filter environments when maps aren't separated by environment!\")\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.filter_positions","title":"<code>filter_positions(idx_positions)</code>","text":"<p>Filter maps to keep only specified position bins.</p> <p>Parameters:</p> Name Type Description Default <code>idx_positions</code> <code>ndarray</code> <p>Indices of position bins to keep. Must be a 1D array of integers.</p> required Notes <p>This method modifies the maps in-place, keeping only the position bins specified by idx_positions. Also updates distcenters if present.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def filter_positions(self, idx_positions: np.ndarray) -&gt; None:\n    \"\"\"Filter maps to keep only specified position bins.\n\n    Parameters\n    ----------\n    idx_positions : np.ndarray\n        Indices of position bins to keep. Must be a 1D array of integers.\n\n    Notes\n    -----\n    This method modifies the maps in-place, keeping only the position bins\n    specified by idx_positions. Also updates distcenters if present.\n    \"\"\"\n    if self.distcenters is not None:\n        self.distcenters = self.distcenters[idx_positions]\n    for mapname in self.map_types():\n        axis = self._get_position_axis(mapname)\n        if self.by_environment:\n            self[mapname] = [np.take(x, idx_positions, axis=axis) for x in self[mapname]]\n        else:\n            self[mapname] = np.take(self[mapname], idx_positions, axis=axis)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.filter_rois","title":"<code>filter_rois(idx_rois)</code>","text":"<p>Filter spike maps to keep only specified ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>idx_rois</code> <code>ndarray</code> <p>Indices of ROIs to keep. Must be a 1D array of integers.</p> required Notes <p>This method modifies the spkmap in-place, keeping only the ROIs specified by idx_rois. Only affects spkmap; occmap and speedmap are unchanged.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def filter_rois(self, idx_rois: np.ndarray) -&gt; None:\n    \"\"\"Filter spike maps to keep only specified ROIs.\n\n    Parameters\n    ----------\n    idx_rois : np.ndarray\n        Indices of ROIs to keep. Must be a 1D array of integers.\n\n    Notes\n    -----\n    This method modifies the spkmap in-place, keeping only the ROIs\n    specified by idx_rois. Only affects spkmap; occmap and speedmap\n    are unchanged.\n    \"\"\"\n    axis = 0 if self.rois_first else -1\n    if self.by_environment:\n        self.spkmap = [np.take(x, idx_rois, axis=axis) for x in self.spkmap]\n    else:\n        self.spkmap = np.take(self.spkmap, idx_rois, axis=axis)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.map_types","title":"<code>map_types()</code>  <code>classmethod</code>","text":"<p>Get the list of map type names.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List containing [\"occmap\", \"speedmap\", \"spkmap\"].</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef map_types(cls) -&gt; List[str]:\n    \"\"\"Get the list of map type names.\n\n    Returns\n    -------\n    list of str\n        List containing [\"occmap\", \"speedmap\", \"spkmap\"].\n    \"\"\"\n    return [\"occmap\", \"speedmap\", \"spkmap\"]\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.nbytes","title":"<code>nbytes()</code>","text":"<p>Calculate the total memory size of all maps in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total number of bytes used by all map arrays.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def nbytes(self) -&gt; int:\n    \"\"\"Calculate the total memory size of all maps in bytes.\n\n    Returns\n    -------\n    int\n        Total number of bytes used by all map arrays.\n    \"\"\"\n    num_bytes = 0\n    for name in self.map_types():\n        if self.by_environment:\n            num_bytes += sum(x.nbytes for x in getattr(self, name))\n        else:\n            num_bytes += getattr(self, name).nbytes\n    return num_bytes\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.pop_nan_positions","title":"<code>pop_nan_positions()</code>","text":"<p>Remove position bins that contain NaN values in any map.</p> Notes <p>This method identifies position bins that have NaN values in any of the maps (occmap, speedmap, or spkmap) and removes them from all maps. Useful for cleaning data before analysis.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def pop_nan_positions(self) -&gt; None:\n    \"\"\"Remove position bins that contain NaN values in any map.\n\n    Notes\n    -----\n    This method identifies position bins that have NaN values in any of the\n    maps (occmap, speedmap, or spkmap) and removes them from all maps.\n    Useful for cleaning data before analysis.\n    \"\"\"\n    if self.by_environment:\n        idx_valid_positions = np.where(~np.any(np.stack([np.any(np.isnan(occmap), axis=0) for occmap in self.occmap], axis=0), axis=0))[0]\n    else:\n        idx_valid_positions = np.where(~np.any(np.isnan(self.occmap), axis=0))[0]\n    self.filter_positions(idx_valid_positions)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.raw_to_processed","title":"<code>raw_to_processed(positions, smooth_width=None)</code>","text":"<p>Convert raw maps to processed maps.</p> <p>Processing steps: 1. Optionally smooth maps with a Gaussian kernel 2. Divide speedmap and spkmap by occmap (correct_map) 3. Reorganize spkmap to have ROIs as the first dimension</p> <p>Parameters:</p> Name Type Description Default <code>positions</code> <code>ndarray</code> <p>Position values corresponding to the position bins.</p> required <code>smooth_width</code> <code>float</code> <p>Width of the Gaussian smoothing kernel. If None, no smoothing is applied. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Maps</code> <p>Self, with maps now in processed format (rois_first=True).</p> Notes <p>This method modifies the maps in-place. After processing, spkmap will have shape (rois, trials, positions) instead of (trials, positions, rois).</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def raw_to_processed(self, positions: np.ndarray, smooth_width: float | None = None) -&gt; \"Maps\":\n    \"\"\"Convert raw maps to processed maps.\n\n    Processing steps:\n    1. Optionally smooth maps with a Gaussian kernel\n    2. Divide speedmap and spkmap by occmap (correct_map)\n    3. Reorganize spkmap to have ROIs as the first dimension\n\n    Parameters\n    ----------\n    positions : np.ndarray\n        Position values corresponding to the position bins.\n    smooth_width : float, optional\n        Width of the Gaussian smoothing kernel. If None, no smoothing is applied.\n        Default is None.\n\n    Returns\n    -------\n    Maps\n        Self, with maps now in processed format (rois_first=True).\n\n    Notes\n    -----\n    This method modifies the maps in-place. After processing, spkmap will\n    have shape (rois, trials, positions) instead of (trials, positions, rois).\n    \"\"\"\n    if smooth_width is not None:\n        self.smooth_maps(positions, smooth_width)\n\n    self.speedmap = correct_map(self.occmap, self.speedmap)\n    self.spkmap = correct_map(self.occmap, self.spkmap)\n\n    # Change spkmap to be ROIs first\n    self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n    self.rois_first = True\n\n    return self\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Maps.smooth_maps","title":"<code>smooth_maps(positions, kernel_width)</code>","text":"<p>Smooth the maps using a Gaussian kernel.</p> <p>Parameters:</p> Name Type Description Default <code>positions</code> <code>ndarray</code> <p>Position values corresponding to the position bins. Used to compute the Gaussian kernel.</p> required <code>kernel_width</code> <code>float</code> <p>Width of the Gaussian smoothing kernel in spatial units.</p> required Notes <p>This method applies Gaussian smoothing to all maps (occmap, speedmap, spkmap). NaN values are temporarily replaced with 0 during smoothing, then restored afterward. The smoothing is applied along the position dimension.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def smooth_maps(self, positions: np.ndarray, kernel_width: float) -&gt; None:\n    \"\"\"Smooth the maps using a Gaussian kernel.\n\n    Parameters\n    ----------\n    positions : np.ndarray\n        Position values corresponding to the position bins. Used to compute\n        the Gaussian kernel.\n    kernel_width : float\n        Width of the Gaussian smoothing kernel in spatial units.\n\n    Notes\n    -----\n    This method applies Gaussian smoothing to all maps (occmap, speedmap, spkmap).\n    NaN values are temporarily replaced with 0 during smoothing, then restored\n    afterward. The smoothing is applied along the position dimension.\n    \"\"\"\n    kernel = get_gauss_kernel(positions, kernel_width)\n\n    # Replace nans with 0s\n    if self.by_environment:\n        idxnan = [np.isnan(occmap) for occmap in self.occmap]\n    else:\n        idxnan = np.isnan(self.occmap)\n\n    if self.rois_first:\n        # Move the rois axis to the last axis\n        if self.by_environment:\n            self.spkmap = [np.moveaxis(map, 0, -1) for map in self.spkmap]\n        else:\n            self.spkmap = np.moveaxis(self.spkmap, 0, -1)\n\n    for mapname in self.map_types():\n        if self.by_environment:\n            for ienv, inanenv in enumerate(idxnan):\n                self[mapname][ienv][inanenv] = 0\n        else:\n            self[mapname][idxnan] = 0\n\n    for mapname in self.map_types():\n        # Since we moved ROIs to the last axis position will be axis=1 for all map types\n        if self.by_environment:\n            self[mapname] = [convolve_toeplitz(map, kernel, axis=1) for map in self[mapname]]\n        else:\n            self[mapname] = convolve_toeplitz(self[mapname], kernel, axis=1)\n\n    # Put nans back in place\n    for mapname in self.map_types():\n        if self.by_environment:\n            for ienv, inanenv in enumerate(idxnan):\n                self[mapname][ienv][inanenv] = np.nan\n        else:\n            self[mapname][idxnan] = np.nan\n\n    # Move the rois axis back to the first axis\n    if self.rois_first:\n        if self.by_environment:\n            self.spkmap = [np.moveaxis(map, -1, 0) for map in self.spkmap]\n        else:\n            self.spkmap = np.moveaxis(self.spkmap, -1, 0)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Reliability","title":"<code>Reliability</code>  <code>dataclass</code>","text":"<p>Container for reliability values.</p> <p>Attributes:</p> Name Type Description <code>values</code> <code>ndarray</code> <p>Reliability values for each neuron</p> <code>environments</code> <code>ndarray</code> <p>Environments for which the reliability was computed</p> <code>method</code> <code>str</code> <p>Method used to compute the reliability</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass Reliability:\n    \"\"\"Container for reliability values.\n\n    Attributes\n    ----------\n    values : np.ndarray\n        Reliability values for each neuron\n    environments : np.ndarray\n        Environments for which the reliability was computed\n    method : str\n        Method used to compute the reliability\n    \"\"\"\n\n    values: np.ndarray\n    environments: np.ndarray\n    method: str\n\n    def __post_init__(self):\n        if self.values.shape[0] != len(self.environments):\n            raise ValueError(\"values and environments must have the same number of environments\")\n\n    def __repr__(self) -&gt; str:\n        return f\"Reliability(num_rois={self.values.shape[1]}, environments={self.environments}, method={self.method})\"\n\n    def filter_rois(self, idx_rois: np.ndarray) -&gt; \"Reliability\":\n        \"\"\"Filter reliability values to keep only specified ROIs.\n\n        Parameters\n        ----------\n        idx_rois : np.ndarray\n            Indices of ROIs to keep. Must be a 1D array of integers.\n\n        Returns\n        -------\n        Reliability\n            New Reliability instance with filtered ROI values.\n        \"\"\"\n        return Reliability(self.values[:, idx_rois], self.environments, self.method)\n\n    def filter_environments(self, idx_environments: np.ndarray) -&gt; \"Reliability\":\n        \"\"\"Filter reliability values to keep only specified environments by index.\n\n        Parameters\n        ----------\n        idx_environments : np.ndarray\n            Indices of environments to keep. Must be a 1D array of integers.\n\n        Returns\n        -------\n        Reliability\n            New Reliability instance with filtered environment values.\n        \"\"\"\n        return Reliability(self.values[idx_environments], self.environments[idx_environments], self.method)\n\n    def filter_by_environment(self, environments: list[int]) -&gt; \"Reliability\":\n        \"\"\"Filter reliability values to keep only specified environments by environment number.\n\n        Parameters\n        ----------\n        environments : list of int\n            List of environment numbers to keep.\n\n        Returns\n        -------\n        Reliability\n            New Reliability instance with filtered environment values.\n        \"\"\"\n        idx_to_requested_env = [i for i, env in enumerate(self.environments) if env in environments]\n        return Reliability(self.values[idx_to_requested_env], self.environments[idx_to_requested_env], self.method)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Reliability.filter_by_environment","title":"<code>filter_by_environment(environments)</code>","text":"<p>Filter reliability values to keep only specified environments by environment number.</p> <p>Parameters:</p> Name Type Description Default <code>environments</code> <code>list of int</code> <p>List of environment numbers to keep.</p> required <p>Returns:</p> Type Description <code>Reliability</code> <p>New Reliability instance with filtered environment values.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def filter_by_environment(self, environments: list[int]) -&gt; \"Reliability\":\n    \"\"\"Filter reliability values to keep only specified environments by environment number.\n\n    Parameters\n    ----------\n    environments : list of int\n        List of environment numbers to keep.\n\n    Returns\n    -------\n    Reliability\n        New Reliability instance with filtered environment values.\n    \"\"\"\n    idx_to_requested_env = [i for i, env in enumerate(self.environments) if env in environments]\n    return Reliability(self.values[idx_to_requested_env], self.environments[idx_to_requested_env], self.method)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Reliability.filter_environments","title":"<code>filter_environments(idx_environments)</code>","text":"<p>Filter reliability values to keep only specified environments by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx_environments</code> <code>ndarray</code> <p>Indices of environments to keep. Must be a 1D array of integers.</p> required <p>Returns:</p> Type Description <code>Reliability</code> <p>New Reliability instance with filtered environment values.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def filter_environments(self, idx_environments: np.ndarray) -&gt; \"Reliability\":\n    \"\"\"Filter reliability values to keep only specified environments by index.\n\n    Parameters\n    ----------\n    idx_environments : np.ndarray\n        Indices of environments to keep. Must be a 1D array of integers.\n\n    Returns\n    -------\n    Reliability\n        New Reliability instance with filtered environment values.\n    \"\"\"\n    return Reliability(self.values[idx_environments], self.environments[idx_environments], self.method)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.Reliability.filter_rois","title":"<code>filter_rois(idx_rois)</code>","text":"<p>Filter reliability values to keep only specified ROIs.</p> <p>Parameters:</p> Name Type Description Default <code>idx_rois</code> <code>ndarray</code> <p>Indices of ROIs to keep. Must be a 1D array of integers.</p> required <p>Returns:</p> Type Description <code>Reliability</code> <p>New Reliability instance with filtered ROI values.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def filter_rois(self, idx_rois: np.ndarray) -&gt; \"Reliability\":\n    \"\"\"Filter reliability values to keep only specified ROIs.\n\n    Parameters\n    ----------\n    idx_rois : np.ndarray\n        Indices of ROIs to keep. Must be a 1D array of integers.\n\n    Returns\n    -------\n    Reliability\n        New Reliability instance with filtered ROI values.\n    \"\"\"\n    return Reliability(self.values[:, idx_rois], self.environments, self.method)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapParams","title":"<code>SpkmapParams</code>  <code>dataclass</code>","text":"<p>Parameters for spike map processing.</p> <p>Contains configuration settings that control how spike maps are processed, including distance steps, speed thresholds, and standardization options.</p> <p>Parameters:</p> Name Type Description Default <code>dist_step</code> <code>float</code> <p>Step size for distance calculations in spatial units</p> <code>1</code> <code>speed_threshold</code> <code>float</code> <p>Minimum speed threshold for valid movement periods</p> <code>1.0</code> <code>speed_max_allowed</code> <code>float</code> <p>Maximum speed allowed for valid movement periods (default is no maximum, can be useful when behavioral computer allows jumps in position which are usually due to hardware issues</p> <code>np.inf</code> <code>full_trial_flexibility</code> <code>float | None</code> <p>Flexibility parameter for trial alignment. If None, no flexibility</p> <code>None</code> <code>standardize_spks</code> <code>bool</code> <p>Whether to standardize spike counts by dividing by the standard deviation</p> <code>True</code> <code>smooth_width</code> <code>float | None</code> <p>Width of the Gaussian smoothing kernel to apply to the maps (width in spatial units)</p> <code>1</code> <code>reliability_method</code> <code>str</code> <p>Method to use for calculating reliability</p> <code>\"leave_one_out\"</code> <code>autosave</code> <code>bool</code> <p>Whether to save the cache automatically</p> <code>True</code> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@dataclass\nclass SpkmapParams:\n    \"\"\"Parameters for spike map processing.\n\n    Contains configuration settings that control how spike maps are processed,\n    including distance steps, speed thresholds, and standardization options.\n\n    Parameters\n    ----------\n    dist_step : float, default=1\n        Step size for distance calculations in spatial units\n    speed_threshold : float, default=1.0\n        Minimum speed threshold for valid movement periods\n    speed_max_allowed : float, default=np.inf\n        Maximum speed allowed for valid movement periods (default is no maximum,\n        can be useful when behavioral computer allows jumps in position which\n        are usually due to hardware issues\n    full_trial_flexibility : float | None, default=None\n        Flexibility parameter for trial alignment. If None, no flexibility\n    standardize_spks : bool, default=True\n        Whether to standardize spike counts by dividing by the standard deviation\n    smooth_width : float | None, default=1\n        Width of the Gaussian smoothing kernel to apply to the maps (width in spatial units)\n    reliability_method : str, default=\"leave_one_out\"\n        Method to use for calculating reliability\n    autosave : bool, default=True\n        Whether to save the cache automatically\n    \"\"\"\n\n    dist_step: float = 1.0\n    speed_threshold: float = 1.0\n    speed_max_allowed: float = np.inf\n    full_trial_flexibility: Union[float, None] = 3.0\n    standardize_spks: bool = True\n    smooth_width: Union[float, None] = 1.0\n    reliability_method: str = \"leave_one_out\"\n    autosave: bool = False\n\n    def __repr__(self) -&gt; str:\n        class_fields = fields(self)\n        lines = []\n        for field in class_fields:\n            field_name = field.name\n            field_value = getattr(self, field_name)\n            lines.append(f\"{field_name}={repr(field_value)}\")\n\n        class_name = self.__class__.__name__\n        joined_lines = \",\\n    \".join(lines)\n        return f\"{class_name}(\\n    {joined_lines}\\n)\"\n\n    @classmethod\n    def from_dict(cls, params_dict: dict) -&gt; \"SpkmapParams\":\n        \"\"\"Create a SpkmapParams instance from a dictionary.\n\n        Parameters\n        ----------\n        params_dict : dict\n            Dictionary of parameter names and values. Missing parameters will\n            use default values from SpkmapParams.\n\n        Returns\n        -------\n        SpkmapParams\n            New SpkmapParams instance with values from the dictionary.\n        \"\"\"\n        return cls(**{k: params_dict[k] for k in params_dict})\n\n    @classmethod\n    def from_path(cls, path: Path) -&gt; \"SpkmapParams\":\n        \"\"\"Create a SpkmapParams instance from a JSON file.\n\n        Parameters\n        ----------\n        path : Path\n            Path to the JSON file containing parameter values.\n\n        Returns\n        -------\n        SpkmapParams\n            New SpkmapParams instance loaded from the JSON file.\n        \"\"\"\n        with open(path, \"r\") as f:\n            return cls.from_dict(json.load(f))\n\n    def compare(self, other: \"SpkmapParams\", filter_keys: Optional[List[str]] = None) -&gt; bool:\n        \"\"\"Compare two SpkmapParams instances.\n\n        Parameters\n        ----------\n        other : SpkmapParams\n            Another SpkmapParams instance to compare against.\n        filter_keys : list of str, optional\n            If provided, only compare the specified parameter keys.\n            If None, compare all parameters. Default is None.\n\n        Returns\n        -------\n        bool\n            True if the parameters match (or specified keys match), False otherwise.\n        \"\"\"\n        if filter_keys is None:\n            return self == other\n        else:\n            return all(getattr(self, key) == getattr(other, key) for key in filter_keys)\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save the parameters to a JSON file.\n\n        Parameters\n        ----------\n        path : Path\n            Path where the JSON file will be saved.\n        \"\"\"\n        with open(path, \"w\") as f:\n            json.dump(asdict(self), f, sort_keys=True)\n\n    def __post_init__(self):\n        if self.dist_step &lt;= 0:\n            raise ValueError(\"dist_step must be positive\")\n        if self.speed_threshold &lt;= 0:\n            raise ValueError(\"speed_threshold must be positive\")\n        if self.full_trial_flexibility is not None and self.full_trial_flexibility &lt; 0:\n            raise ValueError(\"If used, full_trial_flexibility must be nonnegative (can also be None)\")\n        if self.smooth_width is not None and self.smooth_width &lt;= 0:\n            raise ValueError(\"smooth_width must be positive (can also be None)\")\n        # Convert floats to floats when not None\n        self.dist_step = float(self.dist_step)\n        self.speed_threshold = float(self.speed_threshold)\n        self.speed_max_allowed = float(self.speed_max_allowed)\n        self.full_trial_flexibility = float(self.full_trial_flexibility) if self.full_trial_flexibility is not None else None\n        self.smooth_width = float(self.smooth_width) if self.smooth_width is not None else None\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapParams.compare","title":"<code>compare(other, filter_keys=None)</code>","text":"<p>Compare two SpkmapParams instances.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>SpkmapParams</code> <p>Another SpkmapParams instance to compare against.</p> required <code>filter_keys</code> <code>list of str</code> <p>If provided, only compare the specified parameter keys. If None, compare all parameters. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the parameters match (or specified keys match), False otherwise.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def compare(self, other: \"SpkmapParams\", filter_keys: Optional[List[str]] = None) -&gt; bool:\n    \"\"\"Compare two SpkmapParams instances.\n\n    Parameters\n    ----------\n    other : SpkmapParams\n        Another SpkmapParams instance to compare against.\n    filter_keys : list of str, optional\n        If provided, only compare the specified parameter keys.\n        If None, compare all parameters. Default is None.\n\n    Returns\n    -------\n    bool\n        True if the parameters match (or specified keys match), False otherwise.\n    \"\"\"\n    if filter_keys is None:\n        return self == other\n    else:\n        return all(getattr(self, key) == getattr(other, key) for key in filter_keys)\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapParams.from_dict","title":"<code>from_dict(params_dict)</code>  <code>classmethod</code>","text":"<p>Create a SpkmapParams instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>params_dict</code> <code>dict</code> <p>Dictionary of parameter names and values. Missing parameters will use default values from SpkmapParams.</p> required <p>Returns:</p> Type Description <code>SpkmapParams</code> <p>New SpkmapParams instance with values from the dictionary.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef from_dict(cls, params_dict: dict) -&gt; \"SpkmapParams\":\n    \"\"\"Create a SpkmapParams instance from a dictionary.\n\n    Parameters\n    ----------\n    params_dict : dict\n        Dictionary of parameter names and values. Missing parameters will\n        use default values from SpkmapParams.\n\n    Returns\n    -------\n    SpkmapParams\n        New SpkmapParams instance with values from the dictionary.\n    \"\"\"\n    return cls(**{k: params_dict[k] for k in params_dict})\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapParams.from_path","title":"<code>from_path(path)</code>  <code>classmethod</code>","text":"<p>Create a SpkmapParams instance from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the JSON file containing parameter values.</p> required <p>Returns:</p> Type Description <code>SpkmapParams</code> <p>New SpkmapParams instance loaded from the JSON file.</p> Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>@classmethod\ndef from_path(cls, path: Path) -&gt; \"SpkmapParams\":\n    \"\"\"Create a SpkmapParams instance from a JSON file.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the JSON file containing parameter values.\n\n    Returns\n    -------\n    SpkmapParams\n        New SpkmapParams instance loaded from the JSON file.\n    \"\"\"\n    with open(path, \"r\") as f:\n        return cls.from_dict(json.load(f))\n</code></pre>"},{"location":"api/processors/#vrAnalysis.processors.spkmaps.SpkmapParams.save","title":"<code>save(path)</code>","text":"<p>Save the parameters to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path where the JSON file will be saved.</p> required Source code in <code>vrAnalysis/processors/spkmaps.py</code> <pre><code>def save(self, path: Path) -&gt; None:\n    \"\"\"Save the parameters to a JSON file.\n\n    Parameters\n    ----------\n    path : Path\n        Path where the JSON file will be saved.\n    \"\"\"\n    with open(path, \"w\") as f:\n        json.dump(asdict(self), f, sort_keys=True)\n</code></pre>"},{"location":"api/processors/#usage-examples","title":"Usage Examples","text":""},{"location":"api/processors/#basic-usage","title":"Basic Usage","text":"<pre><code>from vrAnalysis.processors import SpkmapProcessor, SpkmapParams\nfrom vrAnalysis.sessions import B2Session\n\n# Create a session\nsession = B2Session(\"path/to/session\")\n\n# Create a processor with default parameters\nprocessor = SpkmapProcessor(session)\n\n# Get raw maps (unsmoothed, not normalized)\nraw_maps = processor.get_raw_maps()\n\n# Get processed maps (smoothed and normalized by occupancy)\nprocessed_maps = processor.get_processed_maps()\n\n# Get environment-separated maps\nenv_maps = processor.get_env_maps()\n\n# Calculate reliability\nreliability = processor.get_reliability()\n</code></pre>"},{"location":"api/processors/#custom-parameters","title":"Custom Parameters","text":"<pre><code># Create custom parameters\nparams = SpkmapParams(\n    dist_step=2.0,  # 2 cm bins\n    speed_threshold=2.0,  # 2 cm/s minimum speed\n    smooth_width=5.0,  # 5 cm smoothing\n    standardize_spks=True,\n)\n\n# Use custom parameters\nprocessor = SpkmapProcessor(session, params=params)\n\n# Or update parameters temporarily\nmaps = processor.get_processed_maps(params={\"smooth_width\": 10.0})\n</code></pre>"},{"location":"api/processors/#working-with-maps","title":"Working with Maps","text":"<pre><code># Get processed maps\nmaps = processor.get_processed_maps()\n\n# Filter to specific ROIs\nmaps.filter_rois([0, 1, 2, 3])\n\n# Filter to specific positions\nmaps.filter_positions(np.arange(10, 50))\n\n# Average across trials\nmaps.average_trials()\n\n# Check memory usage\nprint(f\"Maps use {maps.nbytes() / 1e6:.2f} MB\")\n</code></pre>"},{"location":"api/processors/#environment-separated-maps","title":"Environment-Separated Maps","text":"<pre><code># Get maps separated by environment\nenv_maps = processor.get_env_maps()\n\n# Access maps for a specific environment\nenv_idx = 0\noccmap_env0 = env_maps.occmap[env_idx]\nspkmap_env0 = env_maps.spkmap[env_idx]\n\n# Filter to specific environments\nenv_maps.filter_environments([1, 2])  # Keep only environments 1 and 2\n</code></pre>"},{"location":"api/processors/#reliability-analysis","title":"Reliability Analysis","text":"<pre><code># Calculate reliability with default method (leave_one_out)\nreliability = processor.get_reliability()\n\n# Access reliability values\n# Shape: (num_environments, num_rois)\nreliability_values = reliability.values\n\n# Filter to specific ROIs\nreliability_filtered = reliability.filter_rois([0, 1, 2])\n\n# Filter to specific environments\nreliability_env = reliability.filter_by_environment([1, 2])\n</code></pre>"},{"location":"api/processors/#place-field-predictions","title":"Place Field Predictions","text":"<pre><code># Get place field predictions for each frame\nprediction, extras = processor.get_placefield_prediction()\n\n# prediction shape: (num_frames, num_rois)\n# extras contains frame_position_index, frame_environment_index, idx_valid\n\n# Use predictions to analyze neural activity\nvalid_predictions = prediction[extras[\"idx_valid\"]]\n</code></pre>"},{"location":"api/processors/#traversals-analysis","title":"Traversals Analysis","text":"<pre><code># Extract activity around place field peak\nroi_idx = 5  # Neuron index\nenv_idx = 0  # Environment index\n\ntraversals, pred_travs = processor.get_traversals(\n    idx_roi=roi_idx,\n    idx_env=env_idx,\n    width=10,  # 10 frames on each side\n    placefield_threshold=5.0,  # 5 cm threshold\n)\n\n# traversals shape: (num_traversals, 21)  # 2*width + 1\n# pred_travs shape: (num_traversals, 21)\n</code></pre>"},{"location":"api/processors/#caching","title":"Caching","text":"<p>The SpkmapProcessor uses intelligent caching to avoid recomputing maps when parameters haven't changed:</p> <pre><code># Enable autosave to cache results\nparams = SpkmapParams(autosave=True)\nprocessor = SpkmapProcessor(session, params=params)\n\n# First call computes and caches\nmaps1 = processor.get_processed_maps()\n\n# Second call loads from cache (much faster)\nmaps2 = processor.get_processed_maps()\n\n# Force recomputation\nmaps3 = processor.get_processed_maps(force_recompute=True)\n\n# View cache information\nprocessor.show_cache()\nprocessor.show_cache(data_type=\"processed_maps\")\n</code></pre>"},{"location":"api/processors/#protocol-interface","title":"Protocol Interface","text":"<p>The <code>SpkmapProcessor</code> works with any session class that implements the <code>SessionToSpkmapProtocol</code>. This protocol defines the required properties:</p> <ul> <li><code>spks</code>: Spike data array</li> <li><code>spks_type</code>: Type of spike data</li> <li><code>idx_rois</code>: ROI filter mask</li> <li><code>timestamps</code>: Imaging frame timestamps</li> <li><code>env_length</code>: Environment length(s)</li> <li><code>positions</code>: Position data tuple</li> <li><code>trial_environment</code>: Environment for each trial</li> <li><code>num_trials</code>: Number of trials</li> <li><code>zero_baseline_spks</code>: Whether spikes are zero-baselined</li> </ul> <p>See the protocol documentation for details on implementing custom session classes.</p>"},{"location":"api/registration/","title":"Registration API Reference","text":""},{"location":"api/registration/#vrAnalysis.registration","title":"<code>registration</code>","text":""},{"location":"api/registration/#vrAnalysis.registration.B2Registration","title":"<code>B2Registration</code>  <code>dataclass</code>","text":"<p>               Bases: <code>B2Session</code></p> <p>Registration class for processing B2 (vrControl) session data.</p> <p>This class handles the preprocessing and registration of behavioral and imaging data from vrControl experiments. It processes timeline data, behavioral data, imaging data (suite2p outputs), red cell identification, and creates mappings between behavioral and imaging data.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Name of the mouse.</p> required <code>date_string</code> <code>str</code> <p>Date string in format \"YYYY-MM-DD\".</p> required <code>session_id</code> <code>str</code> <p>Session identifier.</p> required <code>opts</code> <code>B2RegistrationOpts or dict</code> <p>Registration options. If a dict, will be converted to B2RegistrationOpts. Default is B2RegistrationOpts().</p> <code>B2RegistrationOpts()</code> <p>Attributes:</p> Name Type Description <code>opts</code> <code>B2RegistrationOpts</code> <p>Registration options.</p> <code>tl_file</code> <code>dict</code> <p>Timeline data loaded from Timeline.mat file.</p> <code>vr_file</code> <code>dict</code> <p>Behavioral data loaded from VRBehavior_trial.mat file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If session directory does not exist.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>@dataclass(init=False)\nclass B2Registration(B2Session):\n    \"\"\"\n    Registration class for processing B2 (vrControl) session data.\n\n    This class handles the preprocessing and registration of behavioral and\n    imaging data from vrControl experiments. It processes timeline data,\n    behavioral data, imaging data (suite2p outputs), red cell identification,\n    and creates mappings between behavioral and imaging data.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Name of the mouse.\n    date_string : str\n        Date string in format \"YYYY-MM-DD\".\n    session_id : str\n        Session identifier.\n    opts : B2RegistrationOpts or dict, optional\n        Registration options. If a dict, will be converted to B2RegistrationOpts.\n        Default is B2RegistrationOpts().\n\n    Attributes\n    ----------\n    opts : B2RegistrationOpts\n        Registration options.\n    tl_file : dict\n        Timeline data loaded from Timeline.mat file.\n    vr_file : dict\n        Behavioral data loaded from VRBehavior_trial.mat file.\n\n    Raises\n    ------\n    ValueError\n        If session directory does not exist.\n    \"\"\"\n\n    _for_registration: bool = True\n\n    def __init__(\n        self,\n        mouse_name: str,\n        date_string: str,\n        session_id: str,\n        opts: Union[B2RegistrationOpts, dict] = B2RegistrationOpts(),\n    ):\n        \"\"\"\n        Initialize B2Registration object.\n\n        Parameters\n        ----------\n        mouse_name : str\n            Name of the mouse.\n        date_string : str\n            Date string in format \"YYYY-MM-DD\".\n        session_id : str\n            Session identifier.\n        opts : B2RegistrationOpts or dict, optional\n            Registration options. If a dict, will be converted to B2RegistrationOpts.\n            Default is B2RegistrationOpts().\n\n        Raises\n        ------\n        ValueError\n            If session directory does not exist.\n        \"\"\"\n        super().__init__(mouse_name, date_string, session_id)\n        if isinstance(opts, B2RegistrationOpts):\n            self.opts = opts\n        elif is_dataclass(opts):\n            self.opts = B2RegistrationOpts(**asdict(opts))\n        elif isinstance(opts, Mapping):\n            self.opts = B2RegistrationOpts(**opts)\n        else:\n            raise TypeError(f\"opts must be B2RegistrationOpts, a dataclass, or a mapping; got {type(opts)}\")\n\n        if not self.data_path.exists():\n            raise ValueError(f\"Session directory does not exist for {self.session_print()}\")\n\n        if not self.one_path.exists():\n            self.one_path.mkdir(parents=True)\n\n    def _additional_loading(self):\n        \"\"\"\n        Override to skip loading registered data.\n\n        Registration objects produce registered data rather than load it,\n        so we skip the parent's _additional_loading() which tries to load\n        from saved JSON files.\n\n        Notes\n        -----\n        This method intentionally does nothing. Registration objects create\n        data rather than loading pre-existing registered data.\n        \"\"\"\n        pass\n\n    def register(self):\n        \"\"\"\n        Register the session by running all preprocessing steps.\n\n        This is the main entry point for registration. It runs all preprocessing\n        steps (timeline, behavior, imaging, red cells, facecam, behavior-to-imaging)\n        and saves session parameters.\n\n        See Also\n        --------\n        do_preprocessing : Run all preprocessing steps.\n        save_session_prms : Save session parameters to oneData.\n        \"\"\"\n        self.do_preprocessing()\n        self.save_session_prms()\n\n    def do_preprocessing(self):\n        \"\"\"\n        Run all preprocessing steps for the session.\n\n        Processes timeline data, behavioral data, imaging data, red cell\n        identification, facecam data (placeholder), and creates mappings\n        between behavioral and imaging data.\n\n        Notes\n        -----\n        Processing steps are run in order:\n        1. Timeline processing\n        2. Behavior processing\n        3. Imaging processing\n        4. Red cell processing (if enabled)\n        5. Facecam processing (not yet implemented)\n        6. Behavior-to-imaging mapping\n        \"\"\"\n        if self.opts.clearOne:\n            self.clear_one_data(certainty=True)\n        self.process_timeline()\n        self.process_behavior()\n        self.process_imaging()\n        self.process_red_cells()\n        self.process_facecam()\n        self.process_behavior_to_imaging()\n\n    # --------------------------------------------------------------- preprocessing methods ------------------------------------------------------------\n    def process_timeline(self):\n        \"\"\"\n        Process timeline data from rigbox.\n\n        Extracts timestamps, rotary encoder position, lick times, reward times,\n        and trial start times from the Timeline.mat file. Processes photodiode\n        signals to align trial starts with imaging frames.\n\n        Notes\n        -----\n        This method:\n        - Loads timeline structure from Timeline.mat\n        - Converts rotary encoder to position\n        - Detects licks and rewards\n        - Processes photodiode signal to find trial start frames\n        - Saves timeline data to oneData\n        \"\"\"\n        # load these files for raw behavioral &amp; timeline data\n        self.load_timeline_structure()\n        self.load_behavior_structure()\n\n        # get time stamps, photodiode, trial start and end times, room position, lick times, trial idx, visual data visible\n        mpepStartTimes = []\n        for mt, me in zip(self.tl_file[\"mpepUDPTimes\"], self.tl_file[\"mpepUDPEvents\"]):\n            if isinstance(me, str):\n                if \"TrialStart\" in me:\n                    mpepStartTimes.append(mt)\n                elif \"StimStart\" in me:\n                    mpepStartTimes.append(mt)\n\n        mpepStartTimes = np.array(mpepStartTimes)\n        timestamps = self.get_timeline_var(\"timestamps\")  # load timestamps\n\n        # Get rotary position -- (load timeline measurement of rotary encoder, which is a circular position counter, use vrExperiment function to convert to a running measurement of position)\n        rotaryEncoder = self.get_timeline_var(\"rotaryEncoder\")\n        rotaryPosition = self.convert_rotary_encoder_to_position(rotaryEncoder, self.vr_file[\"rigInfo\"])\n\n        # Get Licks (uses an edge counter)\n        lickDetector = self.get_timeline_var(\"lickDetector\")  # load lick detector copy\n        lickSamples = np.where(helpers.diffsame(lickDetector) == 1)[0].astype(np.uint64)  # timeline samples of lick times\n\n        # Get Reward Commands (measures voltage of output -- assume it's either low or high)\n        rewardCommand = self.get_timeline_var(\"rewardCommand\")  # load reward command signal\n        rewardCommand = np.round(rewardCommand / np.max(rewardCommand))\n        rewardSamples = np.where(helpers.diffsame(rewardCommand) &gt; 0.5)[0].astype(np.uint64)  # timeline samples when reward was delivered\n\n        # Now process photodiode signal\n        photodiode = self.get_timeline_var(\"photoDiode\")  # load lick detector copy\n\n        # Remove any slow trends\n        pdDetrend = sp.signal.detrend(photodiode)\n        pdDetrend = (pdDetrend - pdDetrend.min()) / pdDetrend.ptp()\n\n        # median filter and take smooth derivative\n        hfpd = 10\n        refreshRate = 30  # hz\n        refreshSamples = int(1.0 / refreshRate / np.mean(np.diff(timestamps)))\n        pdMedFilt = sp.ndimage.median_filter(pdDetrend, size=refreshSamples)\n        pdDerivative, pdIndex = helpers.fivePointDer(pdMedFilt, hfpd, returnIndex=True)\n        pdDerivative = sp.stats.zscore(pdDerivative)\n        pdDerTime = timestamps[pdIndex]\n\n        # find upward and downward peaks, not perfect but in practice close enough\n        locUp = sp.signal.find_peaks(pdDerivative, height=1, distance=refreshSamples / 2)\n        locDn = sp.signal.find_peaks(-pdDerivative, height=1, distance=refreshSamples / 2)\n        flipTimes = np.concatenate((pdDerTime[locUp[0]], pdDerTime[locDn[0]]))\n        flipValue = np.concatenate((np.ones(len(locUp[0])), np.zeros(len(locDn[0]))))\n        flipSortIdx = np.argsort(flipTimes)\n        flipTimes = flipTimes[flipSortIdx]\n        flipValue = flipValue[flipSortIdx]\n\n        # Naive Method (just look for flips before and after trialstart/trialend mpep message:\n        # A sophisticated message uses the time of the photodiode ramps, but those are really just for safety and rare manual curation...\n        firstFlipIndex = np.array([np.where(flipTimes &gt;= mpepStart)[0][0] for mpepStart in mpepStartTimes])\n        startTrialIndex = helpers.nearestpoint(flipTimes[firstFlipIndex], timestamps)[0]  # returns frame index of first photodiode flip in each trial\n\n        # Check that first flip is always down -- all of the vrControl code prepares trials in this way\n        if datetime.strptime(self.date, \"%Y-%m-%d\") &gt;= datetime.strptime(\"2022-08-30\", \"%Y-%m-%d\"):\n            # But it didn't prepare it this way before august 30th :(\n            assert np.all(flipValue[firstFlipIndex] == 0), f\"In session {self.sessionPrint()}, first flips in trial are not all down!!\"\n\n        # Check shapes of timeline arrays\n        assert timestamps.ndim == 1, \"timelineTimestamps is not a 1-d array!\"\n        assert timestamps.shape == rotaryPosition.shape, \"timeline timestamps and rotary position arrays do not have the same shape!\"\n\n        # Save timeline oneData\n        self.saveone(timestamps, \"wheelPosition.times\")\n        self.saveone(rotaryPosition, \"wheelPosition.position\")\n        self.saveone(timestamps[lickSamples], \"licks.times\")\n        self.saveone(timestamps[rewardSamples], \"rewards.times\")\n        self.saveone(timestamps[startTrialIndex], \"trials.startTimes\")\n        self.preprocessing.append(\"timeline\")\n\n    def process_behavior(self):\n        \"\"\"\n        Process behavioral data from vrControl.\n\n        Processes behavioral data using the appropriate behavior processing\n        function based on the vrBehaviorVersion option. Extracts trial-level\n        and sample-level behavioral data and aligns timestamps to the timeline.\n\n        See Also\n        --------\n        register_behavior : Dispatcher function for behavior processing.\n        \"\"\"\n        self = register_behavior(self, self.opts.vrBehaviorVersion)\n\n        # Confirm that vrBehavior has been processed\n        self.preprocessing.append(\"vrBehavior\")\n\n    def process_imaging(self):\n        \"\"\"\n        Process imaging data from suite2p outputs.\n\n        Loads suite2p outputs, identifies available planes and outputs,\n        handles frame count mismatches between suite2p and timeline,\n        optionally recomputes deconvolution using OASIS, and saves imaging\n        data to oneData format.\n\n        Raises\n        ------\n        ValueError\n            If imaging is requested but suite2p directory does not exist, or\n            if required suite2p outputs are missing, or if frame count\n            mismatches cannot be resolved.\n\n        Notes\n        -----\n        This method:\n        - Identifies planes and available suite2p outputs\n        - Checks for required outputs (stat, ops, F, Fneu, iscell, spks)\n        - Handles frame count mismatches between suite2p and timeline\n        - Optionally recomputes deconvolution using OASIS\n        - Saves imaging data to oneData\n        \"\"\"\n        if not self.opts.imaging:\n            print(f\"In session {self.session_print()}, imaging setting set to False in opts['imaging']. Skipping image processing.\")\n            return None\n\n        if not self.s2p_path.exists():\n            raise ValueError(f\"In session {self.session_print()}, suite2p processing was requested but suite2p directory does not exist.\")\n\n        # identifies which planes were processed through suite2p (assume that those are all available planes)\n        # identifies which s2p outputs are available from each plane\n        self.set_value(\"planeNames\", [plane.parts[-1] for plane in self.s2p_path.glob(\"plane*/\")])\n        self.set_value(\"planeIDs\", [int(planeName[5:]) for planeName in self.get_value(\"planeNames\")])\n        npysInPlanes = [[npy.stem for npy in list((self.s2p_path / planeName).glob(\"*.npy\"))] for planeName in self.get_value(\"planeNames\")]\n        commonNPYs = list(set.intersection(*[set(npy) for npy in npysInPlanes]))\n        unionNPYs = list(set.union(*[set(npy) for npy in npysInPlanes]))\n        if set(commonNPYs) &lt; set(unionNPYs):\n            print(\n                f\"The following npy files are present in some but not all plane folders within session {self.session_print()}: {list(set(unionNPYs) - set(commonNPYs))}\"\n            )\n            print(f\"Each plane folder contains the following npy files: {commonNPYs}\")\n        self.set_value(\"available\", commonNPYs)  # a list of npy files available in each plane folder\n\n        # required variables (anything else is either optional or can be computed independently)\n        required = [\"stat\", \"ops\", \"F\", \"Fneu\", \"iscell\"]\n        if not self.opts.oasis:\n            # add deconvolved spikes to required variable if we aren't recomputing it here\n            required.append(\"spks\")\n        for varName in required:\n            assert varName in self.get_value(\"available\"), f\"{self.session_print()} is missing {varName} in at least one suite2p folder!\"\n        # get number of ROIs in each plane\n        self.set_value(\"roiPerPlane\", [iscell.shape[0] for iscell in self.load_s2p(\"iscell\", concatenate=False)])\n        # get number of frames in each plane (might be different!)\n        self.set_value(\"framePerPlane\", [F.shape[1] for F in self.load_s2p(\"F\", concatenate=False)])\n        assert_msg = f\"The frame count in {self.session_print()} varies by more than 1 frame! ({self.get_value('framePerPlane')})\"\n        assert np.max(self.get_value(\"framePerPlane\")) - np.min(self.get_value(\"framePerPlane\")) &lt;= 1, assert_msg\n        self.set_value(\"numROIs\", np.sum(self.get_value(\"roiPerPlane\")))  # number of ROIs in session\n        # number of frames to use when retrieving imaging data (might be overwritten to something smaller if timeline handled improperly)\n        self.set_value(\"numFrames\", np.min(self.get_value(\"framePerPlane\")))\n\n        # Get timeline sample corresponding to each imaging volume\n        timeline_timestamps = self.loadone(\"wheelPosition.times\")\n        changeFrames = (\n            np.append(\n                0,\n                np.diff(np.ceil(self.get_timeline_var(\"neuralFrames\") / len(self.get_value(\"planeIDs\")))),\n            )\n            == 1\n        )\n        frame_samples = np.where(changeFrames)[0]  # TTLs for each volume (increments by 1 for each plane)\n        frame_to_time = timeline_timestamps[frame_samples]  # get timelineTimestamps of each imaging volume\n\n        # Handle mismatch between number of imaging frames saved by scanImage (and propagated through suite2p), and between timeline's measurement of the scanImage frame counter\n        if len(frame_to_time) != self.get_value(\"numFrames\"):\n            if len(frame_to_time) - 1 == self.get_value(\"numFrames\"):\n                # If frame_to_time had one more frame, just trim it and assume everything is fine. This happens when a new volume was started but not finished, so does not required communication to user.\n                frame_samples = frame_samples[:-1]\n                frame_to_time = frame_to_time[:-1]\n            elif len(frame_to_time) - 2 == self.get_value(\"numFrames\"):\n                print(\n                    \"frame_to_time had 2 more than suite2p output. This happens sometimes. I don't like it. I think it's because scanimage sends a TTL before starting the frame\"\n                )\n                frame_samples = frame_samples[:-2]\n                frame_to_time = frame_to_time[:-2]\n            else:\n                # If frameSamples has too few frames, it's possible that the scanImage signal to timeline was broken but scanImage still continued normally.\n                numMissing = self.get_value(\"numFrames\") - len(frame_samples)  # measure number of missing frames\n                if numMissing &lt; 0:\n                    # If frameSamples had many more frames, generate an error -- something went wrong that needs manual inspection\n                    print(\n                        f\"In session {self.session_print()}, frameSamples has {len(frame_samples)} elements, but {self.get_value('numFrames')} frames were reported in suite2p. Cannot resolve.\"\n                    )\n                    raise ValueError(\"Cannot fix mismatches when suite2p data is missing!\")\n                # It's possible that the scanImage signal to timeline was broken but scanImage still continued normally.\n                if numMissing &gt; 1:\n                    print(\n                        f\"In session {self.session_print()}, more than one frameSamples sample was missing. Consider using tiff timelineTimestamps to reproduce accurately.\"\n                    )\n                print(\n                    (\n                        f\"In session {self.session_print()}, frameSamples has {len(frame_samples)} elements, but {self.get_value('numFrames')} frames were saved by suite2p. \"\n                        \"Will extend frameSamples using the typical sampling rate and nearestpoint algorithm.\"\n                    )\n                )\n                # If frame_to_time difference vector is consistent within 1%, then use mean (which is a little more accurate), otherwise use median\n                frame_to_time = timeline_timestamps[frame_samples]\n                medianFramePeriod = np.median(np.diff(frame_to_time))  # measure median sample period\n                consistentFrames = np.all(\n                    np.abs(np.log(np.diff(frame_to_time) / medianFramePeriod)) &lt; np.log(1.01)\n                )  # True if all frames take within 1% of median frame period\n                if consistentFrames:\n                    samplePeriod_f2t = np.mean(np.diff(frame_to_time))\n                else:\n                    samplePeriod_f2t = np.median(np.diff(frame_to_time))\n                appendFrames = frame_to_time[-1] + samplePeriod_f2t * (\n                    np.arange(numMissing) + 1\n                )  # add elements to frame_to_time, assume sampling rate was perfect\n                frame_to_time = np.concatenate((frame_to_time, appendFrames))\n                frame_samples = helpers.nearestpoint(frame_to_time, timeline_timestamps)[0]\n\n        # average percentage difference between all samples differences and median -- just a useful metric to be saved --\n        self.set_value(\n            \"samplingDeviationMedianPercentError\", np.exp(np.mean(np.abs(np.log(np.diff(frame_to_time) / np.median(np.diff(frame_to_time))))))\n        )\n        self.set_value(\n            \"samplingDeviationMaximumPercentError\", np.exp(np.max(np.abs(np.log(np.diff(frame_to_time) / np.median(np.diff(frame_to_time))))))\n        )\n\n        # recompute deconvolution if requested\n        spks = self.load_s2p(\"spks\")\n        if self.opts.oasis:\n            # set parameters for oasis and get corrected fluorescence traces\n            g = np.exp(-1 / self.opts.tau / self.opts.fs)\n            fcorr = self.loadfcorr(try_from_one=False)\n            results = oasis_deconvolution(fcorr, g)\n            ospks = np.stack(results)\n\n            # Check that the shape is correct\n            msg = f\"In session {self.session_print()}, oasis was run and did not produce the same shaped array as suite2p spks...\"\n            assert ospks.shape == spks.shape, msg\n\n        # save onedata (no assertions needed, loadS2P() handles shape checks and this function already handled any mismatch between frameSamples and suite2p output\n        self.saveone(frame_to_time, \"mpci.times\")\n        self.saveone(LoadingRecipe(\"S2P\", \"F\", transforms=[\"transpose\"]), \"mpci.roiActivityF\")\n        self.saveone(LoadingRecipe(\"S2P\", \"Fneu\", transforms=[\"transpose\"]), \"mpci.roiNeuropilActivityF\")\n        self.saveone(LoadingRecipe(\"S2P\", \"spks\", transforms=[\"transpose\"]), \"mpci.roiActivityDeconvolved\")\n        if \"redcell\" in self.get_value(\"available\"):\n            self.saveone(LoadingRecipe(\"S2P\", \"redcell\", transforms=[\"idx_column1\"]), \"mpciROIs.redS2P\")\n        self.saveone(LoadingRecipe(\"S2P\", \"iscell\"), \"mpciROIs.isCell\")\n        self.saveone(self.get_roi_position(), \"mpciROIs.stackPosition\")\n        if self.opts.oasis:\n            self.saveone(ospks.T, \"mpci.roiActivityDeconvolvedOasis\")\n        self.preprocessing.append(\"imaging\")\n\n    def process_facecam(self):\n        \"\"\"\n        Process facecam data.\n\n        Placeholder for facecam preprocessing. Not yet implemented.\n\n        Notes\n        -----\n        This method currently only prints a message indicating that facecam\n        preprocessing has not been implemented yet.\n        \"\"\"\n        print(\"Facecam preprocessing has not been coded yet!\")\n\n    def process_behavior_to_imaging(self):\n        \"\"\"\n        Create mapping from behavioral frames to imaging frames.\n\n        Computes the nearest imaging frame for each behavioral sample and\n        saves the mapping to oneData.\n\n        Notes\n        -----\n        This method is skipped if imaging is disabled in opts. The mapping\n        is saved as \"positionTracking.mpci\" in oneData.\n        \"\"\"\n        if not self.opts.imaging:\n            print(f\"In session {self.session_print()}, imaging setting set to False in opts['imaging']. Skipping behavior2imaging processing.\")\n            return None\n\n        # compute translation mapping from behave frames to imaging frames\n        idx_behave_to_frame = helpers.nearestpoint(self.loadone(\"positionTracking.times\"), self.loadone(\"mpci.times\"))[0]\n        self.saveone(idx_behave_to_frame.astype(int), \"positionTracking.mpci\")\n\n    def process_red_cells(self):\n        \"\"\"\n        Process red cell features for identification.\n\n        Computes red cell features (dot product, Pearson correlation, phase\n        correlation) using RedCellProcessing and saves them to oneData.\n        Initializes red cell index and manual assignment arrays.\n\n        Notes\n        -----\n        This method is skipped if imaging or redCellProcessing is disabled in opts,\n        or if redcell output is not available in suite2p. The computed features\n        are saved to oneData for later use in red cell identification.\n        \"\"\"\n        if not (self.opts.imaging) or not (self.opts.redCellProcessing):\n            return  # if not requested, skip function\n        # if imaging was processed and redCellProcessing was requested, then try to preprocess red cell features\n        if \"redcell\" not in self.get_value(\"available\"):\n            print(f\"In session {self.session_print()}, 'redcell' is not an available suite2p output, although 'redCellProcessing' was requested.\")\n            return\n\n        # create RedCellProcessing object\n        # b2session_of_self = B2Session.create(self.mouse_name, self.date, self.session_id, for_registration=True)\n        red_cell_processing = RedCellProcessing(self)\n\n        # compute red-features\n        dot_parameters = {\"lowcut\": 12, \"highcut\": 250, \"order\": 3, \"fs\": 512}\n        corr_parameters = {\"width\": 20, \"lowcut\": 12, \"highcut\": 250, \"order\": 3, \"fs\": 512}\n        phase_parameters = {\"width\": 40, \"eps\": 1e6, \"winFunc\": \"hamming\"}\n\n        print(f\"Computing red cell features for {self.session_print()}... (usually takes 10-20 seconds)\")\n        dot_product = red_cell_processing.compute_dot(plane_idx=None, **dot_parameters)\n        corr_coeff = red_cell_processing.compute_corr(plane_idx=None, **corr_parameters)\n        phase_corr = red_cell_processing.cropped_phase_correlation(plane_idx=None, **phase_parameters)[3]\n\n        # initialize annotations\n        self.saveone(np.full(self.get_value(\"numROIs\"), False), \"mpciROIs.redCellIdx\")\n        self.saveone(np.full((2, self.get_value(\"numROIs\")), False), \"mpciROIs.redCellManualAssignments\")\n\n        # save oneData\n        self.saveone(dot_product, \"mpciROIs.redDotProduct\")\n        self.saveone(corr_coeff, \"mpciROIs.redPearson\")\n        self.saveone(phase_corr, \"mpciROIs.redPhaseCorrelation\")\n        self.saveone(np.array(dot_parameters), \"parametersRedDotProduct.keyValuePairs\")\n        self.saveone(np.array(corr_parameters), \"parametersRedPearson.keyValuePairs\")\n        self.saveone(np.array(phase_parameters), \"parametersRedPhaseCorrelation.keyValuePairs\")\n\n    # -------------------------------------- methods for handling timeline data produced by rigbox ------------------------------------------------------------\n    def load_timeline_structure(self):\n        \"\"\"\n        Load timeline structure from Timeline.mat file.\n\n        Loads the Timeline.mat file produced by rigbox and stores it in\n        self.tl_file. The file is expected to be named\n        \"{date}_{session_id}_{mouse_name}_Timeline.mat\".\n\n        Notes\n        -----\n        The timeline file contains raw DAQ data, timestamps, and hardware\n        input measurements from the experimental rig.\n        \"\"\"\n        tl_file_name = self.data_path / f\"{self.date}_{self.session_id}_{self.mouse_name}_Timeline.mat\"  # timeline.mat file name\n        self.tl_file = scio.loadmat(tl_file_name, simplify_cells=True)[\"Timeline\"]  # load matlab structure\n\n    def timeline_inputs(self, ignore_timestamps=False):\n        \"\"\"\n        Get list of available timeline input names.\n\n        Parameters\n        ----------\n        ignore_timestamps : bool, optional\n            If True, return only hardware input names. If False, include\n            \"timestamps\" as the first element. Default is False.\n\n        Returns\n        -------\n        list of str\n            List of timeline input names.\n        \"\"\"\n        if not hasattr(self, \"tl_file\"):\n            self.load_timeline_structure()\n        hw_inputs = [hwInput[\"name\"] for hwInput in self.tl_file[\"hw\"][\"inputs\"]]\n        if ignore_timestamps:\n            return hw_inputs\n        return [\"timestamps\", *hw_inputs]\n\n    def get_timeline_var(self, var_name):\n        \"\"\"\n        Get a timeline variable by name.\n\n        Parameters\n        ----------\n        var_name : str\n            Name of the timeline variable to retrieve. Can be \"timestamps\"\n            or any hardware input name.\n\n        Returns\n        -------\n        np.ndarray\n            Timeline variable data as a 1D array.\n\n        Raises\n        ------\n        AssertionError\n            If var_name is not a valid timeline variable name.\n        \"\"\"\n        if not hasattr(self, \"tl_file\"):\n            self.load_timeline_structure()\n        if var_name == \"timestamps\":\n            return self.tl_file[\"rawDAQTimestamps\"]\n        else:\n            inputNames = self.timeline_inputs(ignore_timestamps=True)\n            assert var_name in inputNames, f\"{var_name} is not a tl_file in session {self.session_print()}\"\n            return np.squeeze(self.tl_file[\"rawDAQData\"][:, np.where([inputName == var_name for inputName in inputNames])[0]])\n\n    def convert_rotary_encoder_to_position(self, rotaryEncoder, rigInfo):\n        \"\"\"\n        Convert rotary encoder counts to position in centimeters.\n\n        The rotary encoder is a counter with a large range that sometimes\n        wraps around. This method handles wrap-around, computes cumulative\n        movement, and scales to centimeters.\n\n        Parameters\n        ----------\n        rotaryEncoder : np.ndarray\n            Rotary encoder counts from timeline.\n        rigInfo : DefaultRigInfo or similar\n            Rig information containing rotary encoder parameters.\n\n        Returns\n        -------\n        np.ndarray\n            Position in centimeters, shape (num_samples,).\n        \"\"\"\n        # rotary encoder is a counter with a big range that sometimes flips around it's axis\n        # first get changes in encoder position, fix any big jumps in value, take the cumulative movement and scale to centimeters\n        rotary_movement = helpers.diffsame(rotaryEncoder)\n        idx_high_values = rotary_movement &gt; 2 ** (rigInfo.rotaryRange - 1)\n        idx_low_values = rotary_movement &lt; -(2 ** (rigInfo.rotaryRange - 1))\n        rotary_movement[idx_high_values] -= 2**rigInfo.rotaryRange\n        rotary_movement[idx_low_values] += 2**rigInfo.rotaryRange\n        return rigInfo.rotEncSign * np.cumsum(rotary_movement) * (2 * np.pi * rigInfo.wheelRadius) / rigInfo.wheelToVR\n\n    # -------------------------------------- methods for handling vrBehavior data produced by vrControl ------------------------------------------------------------\n    def load_behavior_structure(self):\n        \"\"\"\n        Load behavioral structure from VRBehavior_trial.mat file.\n\n        Loads the VRBehavior_trial.mat file produced by vrControl and stores\n        it in self.vr_file. If rigInfo is missing, uses DefaultRigInfo as\n        a fallback.\n\n        Notes\n        -----\n        The behavior file contains trial-level and sample-level behavioral\n        data including timestamps, positions, rewards, and licks. The file\n        is expected to be named\n        \"{date}_{session_id}_{mouse_name}_VRBehavior_trial.mat\".\n        \"\"\"\n        vr_file_name = self.data_path / f\"{self.date}_{self.session_id}_{self.mouse_name}_VRBehavior_trial.mat\"  # vrBehavior output file name\n        self.vr_file = scio.loadmat(vr_file_name, struct_as_record=False, squeeze_me=True)\n        if \"rigInfo\" not in self.vr_file.keys():\n            print(f\"Assuming default settings for B2 using `DefaultRigInfo()` in session: {self.session_print()}!!!\")\n            self.vr_file[\"rigInfo\"] = DefaultRigInfo()\n        if not (hasattr(self.vr_file[\"rigInfo\"], \"rotaryRange\")):\n            self.vr_file[\"rigInfo\"].rotaryRange = 32\n\n    def convert_dense(self, data: Union[np.ndarray, sp.sparse.spmatrix]) -&gt; np.ndarray:\n        \"\"\"\n        Convert sparse or dense array to dense numpy array.\n\n        Truncates data to numTrials rows and converts sparse matrices to\n        dense arrays.\n\n        Parameters\n        ----------\n        data : np.ndarray or scipy.sparse.spmatrix\n            Input data, which may be sparse or dense.\n\n        Returns\n        -------\n        np.ndarray\n            Dense numpy array, truncated to numTrials rows and squeezed.\n        \"\"\"\n        data = data[: self.get_value(\"numTrials\")]\n        if sp.sparse.issparse(data):\n            data = data.toarray().squeeze()\n        else:\n            data = np.asarray(data).squeeze()\n        return data\n\n    def create_index(self, time_stamps):\n        \"\"\"\n        Create index arrays for non-zero/non-NaN timestamps per trial.\n\n        Parameters\n        ----------\n        time_stamps : np.ndarray\n            Timestamps as (numTrials x numSamples) dense numpy array.\n\n        Returns\n        -------\n        list of np.ndarray\n            List of index arrays, one per trial, indicating which samples\n            have valid data (non-NaN or non-zero).\n        \"\"\"\n        # requires timestamps as (numTrials x numSamples) dense numpy array\n        if np.any(np.isnan(time_stamps)):\n            return [np.where(~np.isnan(t))[0] for t in time_stamps]  # in case we have dense timestamps with nans where no data\n        else:\n            return [np.nonzero(t)[0] for t in time_stamps]  # in case we have sparse timestamps with 0s where no data\n\n    def get_vr_data(self, data, nzindex):\n        \"\"\"\n        Extract valid data samples using index arrays.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Data array, shape (numTrials, numSamples).\n        nzindex : list of np.ndarray\n            List of index arrays, one per trial, indicating valid samples.\n\n        Returns\n        -------\n        list of np.ndarray\n            List of data arrays, one per trial, containing only valid samples.\n        \"\"\"\n        return [d[nz] for (d, nz) in zip(data, nzindex)]\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.__init__","title":"<code>__init__(mouse_name, date_string, session_id, opts=B2RegistrationOpts())</code>","text":"<p>Initialize B2Registration object.</p> <p>Parameters:</p> Name Type Description Default <code>mouse_name</code> <code>str</code> <p>Name of the mouse.</p> required <code>date_string</code> <code>str</code> <p>Date string in format \"YYYY-MM-DD\".</p> required <code>session_id</code> <code>str</code> <p>Session identifier.</p> required <code>opts</code> <code>B2RegistrationOpts or dict</code> <p>Registration options. If a dict, will be converted to B2RegistrationOpts. Default is B2RegistrationOpts().</p> <code>B2RegistrationOpts()</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If session directory does not exist.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def __init__(\n    self,\n    mouse_name: str,\n    date_string: str,\n    session_id: str,\n    opts: Union[B2RegistrationOpts, dict] = B2RegistrationOpts(),\n):\n    \"\"\"\n    Initialize B2Registration object.\n\n    Parameters\n    ----------\n    mouse_name : str\n        Name of the mouse.\n    date_string : str\n        Date string in format \"YYYY-MM-DD\".\n    session_id : str\n        Session identifier.\n    opts : B2RegistrationOpts or dict, optional\n        Registration options. If a dict, will be converted to B2RegistrationOpts.\n        Default is B2RegistrationOpts().\n\n    Raises\n    ------\n    ValueError\n        If session directory does not exist.\n    \"\"\"\n    super().__init__(mouse_name, date_string, session_id)\n    if isinstance(opts, B2RegistrationOpts):\n        self.opts = opts\n    elif is_dataclass(opts):\n        self.opts = B2RegistrationOpts(**asdict(opts))\n    elif isinstance(opts, Mapping):\n        self.opts = B2RegistrationOpts(**opts)\n    else:\n        raise TypeError(f\"opts must be B2RegistrationOpts, a dataclass, or a mapping; got {type(opts)}\")\n\n    if not self.data_path.exists():\n        raise ValueError(f\"Session directory does not exist for {self.session_print()}\")\n\n    if not self.one_path.exists():\n        self.one_path.mkdir(parents=True)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.convert_dense","title":"<code>convert_dense(data)</code>","text":"<p>Convert sparse or dense array to dense numpy array.</p> <p>Truncates data to numTrials rows and converts sparse matrices to dense arrays.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray or spmatrix</code> <p>Input data, which may be sparse or dense.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Dense numpy array, truncated to numTrials rows and squeezed.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def convert_dense(self, data: Union[np.ndarray, sp.sparse.spmatrix]) -&gt; np.ndarray:\n    \"\"\"\n    Convert sparse or dense array to dense numpy array.\n\n    Truncates data to numTrials rows and converts sparse matrices to\n    dense arrays.\n\n    Parameters\n    ----------\n    data : np.ndarray or scipy.sparse.spmatrix\n        Input data, which may be sparse or dense.\n\n    Returns\n    -------\n    np.ndarray\n        Dense numpy array, truncated to numTrials rows and squeezed.\n    \"\"\"\n    data = data[: self.get_value(\"numTrials\")]\n    if sp.sparse.issparse(data):\n        data = data.toarray().squeeze()\n    else:\n        data = np.asarray(data).squeeze()\n    return data\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.convert_rotary_encoder_to_position","title":"<code>convert_rotary_encoder_to_position(rotaryEncoder, rigInfo)</code>","text":"<p>Convert rotary encoder counts to position in centimeters.</p> <p>The rotary encoder is a counter with a large range that sometimes wraps around. This method handles wrap-around, computes cumulative movement, and scales to centimeters.</p> <p>Parameters:</p> Name Type Description Default <code>rotaryEncoder</code> <code>ndarray</code> <p>Rotary encoder counts from timeline.</p> required <code>rigInfo</code> <code>DefaultRigInfo or similar</code> <p>Rig information containing rotary encoder parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Position in centimeters, shape (num_samples,).</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def convert_rotary_encoder_to_position(self, rotaryEncoder, rigInfo):\n    \"\"\"\n    Convert rotary encoder counts to position in centimeters.\n\n    The rotary encoder is a counter with a large range that sometimes\n    wraps around. This method handles wrap-around, computes cumulative\n    movement, and scales to centimeters.\n\n    Parameters\n    ----------\n    rotaryEncoder : np.ndarray\n        Rotary encoder counts from timeline.\n    rigInfo : DefaultRigInfo or similar\n        Rig information containing rotary encoder parameters.\n\n    Returns\n    -------\n    np.ndarray\n        Position in centimeters, shape (num_samples,).\n    \"\"\"\n    # rotary encoder is a counter with a big range that sometimes flips around it's axis\n    # first get changes in encoder position, fix any big jumps in value, take the cumulative movement and scale to centimeters\n    rotary_movement = helpers.diffsame(rotaryEncoder)\n    idx_high_values = rotary_movement &gt; 2 ** (rigInfo.rotaryRange - 1)\n    idx_low_values = rotary_movement &lt; -(2 ** (rigInfo.rotaryRange - 1))\n    rotary_movement[idx_high_values] -= 2**rigInfo.rotaryRange\n    rotary_movement[idx_low_values] += 2**rigInfo.rotaryRange\n    return rigInfo.rotEncSign * np.cumsum(rotary_movement) * (2 * np.pi * rigInfo.wheelRadius) / rigInfo.wheelToVR\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.create_index","title":"<code>create_index(time_stamps)</code>","text":"<p>Create index arrays for non-zero/non-NaN timestamps per trial.</p> <p>Parameters:</p> Name Type Description Default <code>time_stamps</code> <code>ndarray</code> <p>Timestamps as (numTrials x numSamples) dense numpy array.</p> required <p>Returns:</p> Type Description <code>list of np.ndarray</code> <p>List of index arrays, one per trial, indicating which samples have valid data (non-NaN or non-zero).</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def create_index(self, time_stamps):\n    \"\"\"\n    Create index arrays for non-zero/non-NaN timestamps per trial.\n\n    Parameters\n    ----------\n    time_stamps : np.ndarray\n        Timestamps as (numTrials x numSamples) dense numpy array.\n\n    Returns\n    -------\n    list of np.ndarray\n        List of index arrays, one per trial, indicating which samples\n        have valid data (non-NaN or non-zero).\n    \"\"\"\n    # requires timestamps as (numTrials x numSamples) dense numpy array\n    if np.any(np.isnan(time_stamps)):\n        return [np.where(~np.isnan(t))[0] for t in time_stamps]  # in case we have dense timestamps with nans where no data\n    else:\n        return [np.nonzero(t)[0] for t in time_stamps]  # in case we have sparse timestamps with 0s where no data\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.do_preprocessing","title":"<code>do_preprocessing()</code>","text":"<p>Run all preprocessing steps for the session.</p> <p>Processes timeline data, behavioral data, imaging data, red cell identification, facecam data (placeholder), and creates mappings between behavioral and imaging data.</p> Notes <p>Processing steps are run in order: 1. Timeline processing 2. Behavior processing 3. Imaging processing 4. Red cell processing (if enabled) 5. Facecam processing (not yet implemented) 6. Behavior-to-imaging mapping</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def do_preprocessing(self):\n    \"\"\"\n    Run all preprocessing steps for the session.\n\n    Processes timeline data, behavioral data, imaging data, red cell\n    identification, facecam data (placeholder), and creates mappings\n    between behavioral and imaging data.\n\n    Notes\n    -----\n    Processing steps are run in order:\n    1. Timeline processing\n    2. Behavior processing\n    3. Imaging processing\n    4. Red cell processing (if enabled)\n    5. Facecam processing (not yet implemented)\n    6. Behavior-to-imaging mapping\n    \"\"\"\n    if self.opts.clearOne:\n        self.clear_one_data(certainty=True)\n    self.process_timeline()\n    self.process_behavior()\n    self.process_imaging()\n    self.process_red_cells()\n    self.process_facecam()\n    self.process_behavior_to_imaging()\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.get_timeline_var","title":"<code>get_timeline_var(var_name)</code>","text":"<p>Get a timeline variable by name.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of the timeline variable to retrieve. Can be \"timestamps\" or any hardware input name.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Timeline variable data as a 1D array.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If var_name is not a valid timeline variable name.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def get_timeline_var(self, var_name):\n    \"\"\"\n    Get a timeline variable by name.\n\n    Parameters\n    ----------\n    var_name : str\n        Name of the timeline variable to retrieve. Can be \"timestamps\"\n        or any hardware input name.\n\n    Returns\n    -------\n    np.ndarray\n        Timeline variable data as a 1D array.\n\n    Raises\n    ------\n    AssertionError\n        If var_name is not a valid timeline variable name.\n    \"\"\"\n    if not hasattr(self, \"tl_file\"):\n        self.load_timeline_structure()\n    if var_name == \"timestamps\":\n        return self.tl_file[\"rawDAQTimestamps\"]\n    else:\n        inputNames = self.timeline_inputs(ignore_timestamps=True)\n        assert var_name in inputNames, f\"{var_name} is not a tl_file in session {self.session_print()}\"\n        return np.squeeze(self.tl_file[\"rawDAQData\"][:, np.where([inputName == var_name for inputName in inputNames])[0]])\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.get_vr_data","title":"<code>get_vr_data(data, nzindex)</code>","text":"<p>Extract valid data samples using index arrays.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Data array, shape (numTrials, numSamples).</p> required <code>nzindex</code> <code>list of np.ndarray</code> <p>List of index arrays, one per trial, indicating valid samples.</p> required <p>Returns:</p> Type Description <code>list of np.ndarray</code> <p>List of data arrays, one per trial, containing only valid samples.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def get_vr_data(self, data, nzindex):\n    \"\"\"\n    Extract valid data samples using index arrays.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Data array, shape (numTrials, numSamples).\n    nzindex : list of np.ndarray\n        List of index arrays, one per trial, indicating valid samples.\n\n    Returns\n    -------\n    list of np.ndarray\n        List of data arrays, one per trial, containing only valid samples.\n    \"\"\"\n    return [d[nz] for (d, nz) in zip(data, nzindex)]\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.load_behavior_structure","title":"<code>load_behavior_structure()</code>","text":"<p>Load behavioral structure from VRBehavior_trial.mat file.</p> <p>Loads the VRBehavior_trial.mat file produced by vrControl and stores it in self.vr_file. If rigInfo is missing, uses DefaultRigInfo as a fallback.</p> Notes <p>The behavior file contains trial-level and sample-level behavioral data including timestamps, positions, rewards, and licks. The file is expected to be named \"{date}{session_id}{mouse_name}_VRBehavior_trial.mat\".</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def load_behavior_structure(self):\n    \"\"\"\n    Load behavioral structure from VRBehavior_trial.mat file.\n\n    Loads the VRBehavior_trial.mat file produced by vrControl and stores\n    it in self.vr_file. If rigInfo is missing, uses DefaultRigInfo as\n    a fallback.\n\n    Notes\n    -----\n    The behavior file contains trial-level and sample-level behavioral\n    data including timestamps, positions, rewards, and licks. The file\n    is expected to be named\n    \"{date}_{session_id}_{mouse_name}_VRBehavior_trial.mat\".\n    \"\"\"\n    vr_file_name = self.data_path / f\"{self.date}_{self.session_id}_{self.mouse_name}_VRBehavior_trial.mat\"  # vrBehavior output file name\n    self.vr_file = scio.loadmat(vr_file_name, struct_as_record=False, squeeze_me=True)\n    if \"rigInfo\" not in self.vr_file.keys():\n        print(f\"Assuming default settings for B2 using `DefaultRigInfo()` in session: {self.session_print()}!!!\")\n        self.vr_file[\"rigInfo\"] = DefaultRigInfo()\n    if not (hasattr(self.vr_file[\"rigInfo\"], \"rotaryRange\")):\n        self.vr_file[\"rigInfo\"].rotaryRange = 32\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.load_timeline_structure","title":"<code>load_timeline_structure()</code>","text":"<p>Load timeline structure from Timeline.mat file.</p> <p>Loads the Timeline.mat file produced by rigbox and stores it in self.tl_file. The file is expected to be named \"{date}{session_id}{mouse_name}_Timeline.mat\".</p> Notes <p>The timeline file contains raw DAQ data, timestamps, and hardware input measurements from the experimental rig.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def load_timeline_structure(self):\n    \"\"\"\n    Load timeline structure from Timeline.mat file.\n\n    Loads the Timeline.mat file produced by rigbox and stores it in\n    self.tl_file. The file is expected to be named\n    \"{date}_{session_id}_{mouse_name}_Timeline.mat\".\n\n    Notes\n    -----\n    The timeline file contains raw DAQ data, timestamps, and hardware\n    input measurements from the experimental rig.\n    \"\"\"\n    tl_file_name = self.data_path / f\"{self.date}_{self.session_id}_{self.mouse_name}_Timeline.mat\"  # timeline.mat file name\n    self.tl_file = scio.loadmat(tl_file_name, simplify_cells=True)[\"Timeline\"]  # load matlab structure\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.process_behavior","title":"<code>process_behavior()</code>","text":"<p>Process behavioral data from vrControl.</p> <p>Processes behavioral data using the appropriate behavior processing function based on the vrBehaviorVersion option. Extracts trial-level and sample-level behavioral data and aligns timestamps to the timeline.</p> See Also <p>register_behavior : Dispatcher function for behavior processing.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def process_behavior(self):\n    \"\"\"\n    Process behavioral data from vrControl.\n\n    Processes behavioral data using the appropriate behavior processing\n    function based on the vrBehaviorVersion option. Extracts trial-level\n    and sample-level behavioral data and aligns timestamps to the timeline.\n\n    See Also\n    --------\n    register_behavior : Dispatcher function for behavior processing.\n    \"\"\"\n    self = register_behavior(self, self.opts.vrBehaviorVersion)\n\n    # Confirm that vrBehavior has been processed\n    self.preprocessing.append(\"vrBehavior\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.process_behavior_to_imaging","title":"<code>process_behavior_to_imaging()</code>","text":"<p>Create mapping from behavioral frames to imaging frames.</p> <p>Computes the nearest imaging frame for each behavioral sample and saves the mapping to oneData.</p> Notes <p>This method is skipped if imaging is disabled in opts. The mapping is saved as \"positionTracking.mpci\" in oneData.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def process_behavior_to_imaging(self):\n    \"\"\"\n    Create mapping from behavioral frames to imaging frames.\n\n    Computes the nearest imaging frame for each behavioral sample and\n    saves the mapping to oneData.\n\n    Notes\n    -----\n    This method is skipped if imaging is disabled in opts. The mapping\n    is saved as \"positionTracking.mpci\" in oneData.\n    \"\"\"\n    if not self.opts.imaging:\n        print(f\"In session {self.session_print()}, imaging setting set to False in opts['imaging']. Skipping behavior2imaging processing.\")\n        return None\n\n    # compute translation mapping from behave frames to imaging frames\n    idx_behave_to_frame = helpers.nearestpoint(self.loadone(\"positionTracking.times\"), self.loadone(\"mpci.times\"))[0]\n    self.saveone(idx_behave_to_frame.astype(int), \"positionTracking.mpci\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.process_facecam","title":"<code>process_facecam()</code>","text":"<p>Process facecam data.</p> <p>Placeholder for facecam preprocessing. Not yet implemented.</p> Notes <p>This method currently only prints a message indicating that facecam preprocessing has not been implemented yet.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def process_facecam(self):\n    \"\"\"\n    Process facecam data.\n\n    Placeholder for facecam preprocessing. Not yet implemented.\n\n    Notes\n    -----\n    This method currently only prints a message indicating that facecam\n    preprocessing has not been implemented yet.\n    \"\"\"\n    print(\"Facecam preprocessing has not been coded yet!\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.process_imaging","title":"<code>process_imaging()</code>","text":"<p>Process imaging data from suite2p outputs.</p> <p>Loads suite2p outputs, identifies available planes and outputs, handles frame count mismatches between suite2p and timeline, optionally recomputes deconvolution using OASIS, and saves imaging data to oneData format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If imaging is requested but suite2p directory does not exist, or if required suite2p outputs are missing, or if frame count mismatches cannot be resolved.</p> Notes <p>This method: - Identifies planes and available suite2p outputs - Checks for required outputs (stat, ops, F, Fneu, iscell, spks) - Handles frame count mismatches between suite2p and timeline - Optionally recomputes deconvolution using OASIS - Saves imaging data to oneData</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def process_imaging(self):\n    \"\"\"\n    Process imaging data from suite2p outputs.\n\n    Loads suite2p outputs, identifies available planes and outputs,\n    handles frame count mismatches between suite2p and timeline,\n    optionally recomputes deconvolution using OASIS, and saves imaging\n    data to oneData format.\n\n    Raises\n    ------\n    ValueError\n        If imaging is requested but suite2p directory does not exist, or\n        if required suite2p outputs are missing, or if frame count\n        mismatches cannot be resolved.\n\n    Notes\n    -----\n    This method:\n    - Identifies planes and available suite2p outputs\n    - Checks for required outputs (stat, ops, F, Fneu, iscell, spks)\n    - Handles frame count mismatches between suite2p and timeline\n    - Optionally recomputes deconvolution using OASIS\n    - Saves imaging data to oneData\n    \"\"\"\n    if not self.opts.imaging:\n        print(f\"In session {self.session_print()}, imaging setting set to False in opts['imaging']. Skipping image processing.\")\n        return None\n\n    if not self.s2p_path.exists():\n        raise ValueError(f\"In session {self.session_print()}, suite2p processing was requested but suite2p directory does not exist.\")\n\n    # identifies which planes were processed through suite2p (assume that those are all available planes)\n    # identifies which s2p outputs are available from each plane\n    self.set_value(\"planeNames\", [plane.parts[-1] for plane in self.s2p_path.glob(\"plane*/\")])\n    self.set_value(\"planeIDs\", [int(planeName[5:]) for planeName in self.get_value(\"planeNames\")])\n    npysInPlanes = [[npy.stem for npy in list((self.s2p_path / planeName).glob(\"*.npy\"))] for planeName in self.get_value(\"planeNames\")]\n    commonNPYs = list(set.intersection(*[set(npy) for npy in npysInPlanes]))\n    unionNPYs = list(set.union(*[set(npy) for npy in npysInPlanes]))\n    if set(commonNPYs) &lt; set(unionNPYs):\n        print(\n            f\"The following npy files are present in some but not all plane folders within session {self.session_print()}: {list(set(unionNPYs) - set(commonNPYs))}\"\n        )\n        print(f\"Each plane folder contains the following npy files: {commonNPYs}\")\n    self.set_value(\"available\", commonNPYs)  # a list of npy files available in each plane folder\n\n    # required variables (anything else is either optional or can be computed independently)\n    required = [\"stat\", \"ops\", \"F\", \"Fneu\", \"iscell\"]\n    if not self.opts.oasis:\n        # add deconvolved spikes to required variable if we aren't recomputing it here\n        required.append(\"spks\")\n    for varName in required:\n        assert varName in self.get_value(\"available\"), f\"{self.session_print()} is missing {varName} in at least one suite2p folder!\"\n    # get number of ROIs in each plane\n    self.set_value(\"roiPerPlane\", [iscell.shape[0] for iscell in self.load_s2p(\"iscell\", concatenate=False)])\n    # get number of frames in each plane (might be different!)\n    self.set_value(\"framePerPlane\", [F.shape[1] for F in self.load_s2p(\"F\", concatenate=False)])\n    assert_msg = f\"The frame count in {self.session_print()} varies by more than 1 frame! ({self.get_value('framePerPlane')})\"\n    assert np.max(self.get_value(\"framePerPlane\")) - np.min(self.get_value(\"framePerPlane\")) &lt;= 1, assert_msg\n    self.set_value(\"numROIs\", np.sum(self.get_value(\"roiPerPlane\")))  # number of ROIs in session\n    # number of frames to use when retrieving imaging data (might be overwritten to something smaller if timeline handled improperly)\n    self.set_value(\"numFrames\", np.min(self.get_value(\"framePerPlane\")))\n\n    # Get timeline sample corresponding to each imaging volume\n    timeline_timestamps = self.loadone(\"wheelPosition.times\")\n    changeFrames = (\n        np.append(\n            0,\n            np.diff(np.ceil(self.get_timeline_var(\"neuralFrames\") / len(self.get_value(\"planeIDs\")))),\n        )\n        == 1\n    )\n    frame_samples = np.where(changeFrames)[0]  # TTLs for each volume (increments by 1 for each plane)\n    frame_to_time = timeline_timestamps[frame_samples]  # get timelineTimestamps of each imaging volume\n\n    # Handle mismatch between number of imaging frames saved by scanImage (and propagated through suite2p), and between timeline's measurement of the scanImage frame counter\n    if len(frame_to_time) != self.get_value(\"numFrames\"):\n        if len(frame_to_time) - 1 == self.get_value(\"numFrames\"):\n            # If frame_to_time had one more frame, just trim it and assume everything is fine. This happens when a new volume was started but not finished, so does not required communication to user.\n            frame_samples = frame_samples[:-1]\n            frame_to_time = frame_to_time[:-1]\n        elif len(frame_to_time) - 2 == self.get_value(\"numFrames\"):\n            print(\n                \"frame_to_time had 2 more than suite2p output. This happens sometimes. I don't like it. I think it's because scanimage sends a TTL before starting the frame\"\n            )\n            frame_samples = frame_samples[:-2]\n            frame_to_time = frame_to_time[:-2]\n        else:\n            # If frameSamples has too few frames, it's possible that the scanImage signal to timeline was broken but scanImage still continued normally.\n            numMissing = self.get_value(\"numFrames\") - len(frame_samples)  # measure number of missing frames\n            if numMissing &lt; 0:\n                # If frameSamples had many more frames, generate an error -- something went wrong that needs manual inspection\n                print(\n                    f\"In session {self.session_print()}, frameSamples has {len(frame_samples)} elements, but {self.get_value('numFrames')} frames were reported in suite2p. Cannot resolve.\"\n                )\n                raise ValueError(\"Cannot fix mismatches when suite2p data is missing!\")\n            # It's possible that the scanImage signal to timeline was broken but scanImage still continued normally.\n            if numMissing &gt; 1:\n                print(\n                    f\"In session {self.session_print()}, more than one frameSamples sample was missing. Consider using tiff timelineTimestamps to reproduce accurately.\"\n                )\n            print(\n                (\n                    f\"In session {self.session_print()}, frameSamples has {len(frame_samples)} elements, but {self.get_value('numFrames')} frames were saved by suite2p. \"\n                    \"Will extend frameSamples using the typical sampling rate and nearestpoint algorithm.\"\n                )\n            )\n            # If frame_to_time difference vector is consistent within 1%, then use mean (which is a little more accurate), otherwise use median\n            frame_to_time = timeline_timestamps[frame_samples]\n            medianFramePeriod = np.median(np.diff(frame_to_time))  # measure median sample period\n            consistentFrames = np.all(\n                np.abs(np.log(np.diff(frame_to_time) / medianFramePeriod)) &lt; np.log(1.01)\n            )  # True if all frames take within 1% of median frame period\n            if consistentFrames:\n                samplePeriod_f2t = np.mean(np.diff(frame_to_time))\n            else:\n                samplePeriod_f2t = np.median(np.diff(frame_to_time))\n            appendFrames = frame_to_time[-1] + samplePeriod_f2t * (\n                np.arange(numMissing) + 1\n            )  # add elements to frame_to_time, assume sampling rate was perfect\n            frame_to_time = np.concatenate((frame_to_time, appendFrames))\n            frame_samples = helpers.nearestpoint(frame_to_time, timeline_timestamps)[0]\n\n    # average percentage difference between all samples differences and median -- just a useful metric to be saved --\n    self.set_value(\n        \"samplingDeviationMedianPercentError\", np.exp(np.mean(np.abs(np.log(np.diff(frame_to_time) / np.median(np.diff(frame_to_time))))))\n    )\n    self.set_value(\n        \"samplingDeviationMaximumPercentError\", np.exp(np.max(np.abs(np.log(np.diff(frame_to_time) / np.median(np.diff(frame_to_time))))))\n    )\n\n    # recompute deconvolution if requested\n    spks = self.load_s2p(\"spks\")\n    if self.opts.oasis:\n        # set parameters for oasis and get corrected fluorescence traces\n        g = np.exp(-1 / self.opts.tau / self.opts.fs)\n        fcorr = self.loadfcorr(try_from_one=False)\n        results = oasis_deconvolution(fcorr, g)\n        ospks = np.stack(results)\n\n        # Check that the shape is correct\n        msg = f\"In session {self.session_print()}, oasis was run and did not produce the same shaped array as suite2p spks...\"\n        assert ospks.shape == spks.shape, msg\n\n    # save onedata (no assertions needed, loadS2P() handles shape checks and this function already handled any mismatch between frameSamples and suite2p output\n    self.saveone(frame_to_time, \"mpci.times\")\n    self.saveone(LoadingRecipe(\"S2P\", \"F\", transforms=[\"transpose\"]), \"mpci.roiActivityF\")\n    self.saveone(LoadingRecipe(\"S2P\", \"Fneu\", transforms=[\"transpose\"]), \"mpci.roiNeuropilActivityF\")\n    self.saveone(LoadingRecipe(\"S2P\", \"spks\", transforms=[\"transpose\"]), \"mpci.roiActivityDeconvolved\")\n    if \"redcell\" in self.get_value(\"available\"):\n        self.saveone(LoadingRecipe(\"S2P\", \"redcell\", transforms=[\"idx_column1\"]), \"mpciROIs.redS2P\")\n    self.saveone(LoadingRecipe(\"S2P\", \"iscell\"), \"mpciROIs.isCell\")\n    self.saveone(self.get_roi_position(), \"mpciROIs.stackPosition\")\n    if self.opts.oasis:\n        self.saveone(ospks.T, \"mpci.roiActivityDeconvolvedOasis\")\n    self.preprocessing.append(\"imaging\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.process_red_cells","title":"<code>process_red_cells()</code>","text":"<p>Process red cell features for identification.</p> <p>Computes red cell features (dot product, Pearson correlation, phase correlation) using RedCellProcessing and saves them to oneData. Initializes red cell index and manual assignment arrays.</p> Notes <p>This method is skipped if imaging or redCellProcessing is disabled in opts, or if redcell output is not available in suite2p. The computed features are saved to oneData for later use in red cell identification.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def process_red_cells(self):\n    \"\"\"\n    Process red cell features for identification.\n\n    Computes red cell features (dot product, Pearson correlation, phase\n    correlation) using RedCellProcessing and saves them to oneData.\n    Initializes red cell index and manual assignment arrays.\n\n    Notes\n    -----\n    This method is skipped if imaging or redCellProcessing is disabled in opts,\n    or if redcell output is not available in suite2p. The computed features\n    are saved to oneData for later use in red cell identification.\n    \"\"\"\n    if not (self.opts.imaging) or not (self.opts.redCellProcessing):\n        return  # if not requested, skip function\n    # if imaging was processed and redCellProcessing was requested, then try to preprocess red cell features\n    if \"redcell\" not in self.get_value(\"available\"):\n        print(f\"In session {self.session_print()}, 'redcell' is not an available suite2p output, although 'redCellProcessing' was requested.\")\n        return\n\n    # create RedCellProcessing object\n    # b2session_of_self = B2Session.create(self.mouse_name, self.date, self.session_id, for_registration=True)\n    red_cell_processing = RedCellProcessing(self)\n\n    # compute red-features\n    dot_parameters = {\"lowcut\": 12, \"highcut\": 250, \"order\": 3, \"fs\": 512}\n    corr_parameters = {\"width\": 20, \"lowcut\": 12, \"highcut\": 250, \"order\": 3, \"fs\": 512}\n    phase_parameters = {\"width\": 40, \"eps\": 1e6, \"winFunc\": \"hamming\"}\n\n    print(f\"Computing red cell features for {self.session_print()}... (usually takes 10-20 seconds)\")\n    dot_product = red_cell_processing.compute_dot(plane_idx=None, **dot_parameters)\n    corr_coeff = red_cell_processing.compute_corr(plane_idx=None, **corr_parameters)\n    phase_corr = red_cell_processing.cropped_phase_correlation(plane_idx=None, **phase_parameters)[3]\n\n    # initialize annotations\n    self.saveone(np.full(self.get_value(\"numROIs\"), False), \"mpciROIs.redCellIdx\")\n    self.saveone(np.full((2, self.get_value(\"numROIs\")), False), \"mpciROIs.redCellManualAssignments\")\n\n    # save oneData\n    self.saveone(dot_product, \"mpciROIs.redDotProduct\")\n    self.saveone(corr_coeff, \"mpciROIs.redPearson\")\n    self.saveone(phase_corr, \"mpciROIs.redPhaseCorrelation\")\n    self.saveone(np.array(dot_parameters), \"parametersRedDotProduct.keyValuePairs\")\n    self.saveone(np.array(corr_parameters), \"parametersRedPearson.keyValuePairs\")\n    self.saveone(np.array(phase_parameters), \"parametersRedPhaseCorrelation.keyValuePairs\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.process_timeline","title":"<code>process_timeline()</code>","text":"<p>Process timeline data from rigbox.</p> <p>Extracts timestamps, rotary encoder position, lick times, reward times, and trial start times from the Timeline.mat file. Processes photodiode signals to align trial starts with imaging frames.</p> Notes <p>This method: - Loads timeline structure from Timeline.mat - Converts rotary encoder to position - Detects licks and rewards - Processes photodiode signal to find trial start frames - Saves timeline data to oneData</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def process_timeline(self):\n    \"\"\"\n    Process timeline data from rigbox.\n\n    Extracts timestamps, rotary encoder position, lick times, reward times,\n    and trial start times from the Timeline.mat file. Processes photodiode\n    signals to align trial starts with imaging frames.\n\n    Notes\n    -----\n    This method:\n    - Loads timeline structure from Timeline.mat\n    - Converts rotary encoder to position\n    - Detects licks and rewards\n    - Processes photodiode signal to find trial start frames\n    - Saves timeline data to oneData\n    \"\"\"\n    # load these files for raw behavioral &amp; timeline data\n    self.load_timeline_structure()\n    self.load_behavior_structure()\n\n    # get time stamps, photodiode, trial start and end times, room position, lick times, trial idx, visual data visible\n    mpepStartTimes = []\n    for mt, me in zip(self.tl_file[\"mpepUDPTimes\"], self.tl_file[\"mpepUDPEvents\"]):\n        if isinstance(me, str):\n            if \"TrialStart\" in me:\n                mpepStartTimes.append(mt)\n            elif \"StimStart\" in me:\n                mpepStartTimes.append(mt)\n\n    mpepStartTimes = np.array(mpepStartTimes)\n    timestamps = self.get_timeline_var(\"timestamps\")  # load timestamps\n\n    # Get rotary position -- (load timeline measurement of rotary encoder, which is a circular position counter, use vrExperiment function to convert to a running measurement of position)\n    rotaryEncoder = self.get_timeline_var(\"rotaryEncoder\")\n    rotaryPosition = self.convert_rotary_encoder_to_position(rotaryEncoder, self.vr_file[\"rigInfo\"])\n\n    # Get Licks (uses an edge counter)\n    lickDetector = self.get_timeline_var(\"lickDetector\")  # load lick detector copy\n    lickSamples = np.where(helpers.diffsame(lickDetector) == 1)[0].astype(np.uint64)  # timeline samples of lick times\n\n    # Get Reward Commands (measures voltage of output -- assume it's either low or high)\n    rewardCommand = self.get_timeline_var(\"rewardCommand\")  # load reward command signal\n    rewardCommand = np.round(rewardCommand / np.max(rewardCommand))\n    rewardSamples = np.where(helpers.diffsame(rewardCommand) &gt; 0.5)[0].astype(np.uint64)  # timeline samples when reward was delivered\n\n    # Now process photodiode signal\n    photodiode = self.get_timeline_var(\"photoDiode\")  # load lick detector copy\n\n    # Remove any slow trends\n    pdDetrend = sp.signal.detrend(photodiode)\n    pdDetrend = (pdDetrend - pdDetrend.min()) / pdDetrend.ptp()\n\n    # median filter and take smooth derivative\n    hfpd = 10\n    refreshRate = 30  # hz\n    refreshSamples = int(1.0 / refreshRate / np.mean(np.diff(timestamps)))\n    pdMedFilt = sp.ndimage.median_filter(pdDetrend, size=refreshSamples)\n    pdDerivative, pdIndex = helpers.fivePointDer(pdMedFilt, hfpd, returnIndex=True)\n    pdDerivative = sp.stats.zscore(pdDerivative)\n    pdDerTime = timestamps[pdIndex]\n\n    # find upward and downward peaks, not perfect but in practice close enough\n    locUp = sp.signal.find_peaks(pdDerivative, height=1, distance=refreshSamples / 2)\n    locDn = sp.signal.find_peaks(-pdDerivative, height=1, distance=refreshSamples / 2)\n    flipTimes = np.concatenate((pdDerTime[locUp[0]], pdDerTime[locDn[0]]))\n    flipValue = np.concatenate((np.ones(len(locUp[0])), np.zeros(len(locDn[0]))))\n    flipSortIdx = np.argsort(flipTimes)\n    flipTimes = flipTimes[flipSortIdx]\n    flipValue = flipValue[flipSortIdx]\n\n    # Naive Method (just look for flips before and after trialstart/trialend mpep message:\n    # A sophisticated message uses the time of the photodiode ramps, but those are really just for safety and rare manual curation...\n    firstFlipIndex = np.array([np.where(flipTimes &gt;= mpepStart)[0][0] for mpepStart in mpepStartTimes])\n    startTrialIndex = helpers.nearestpoint(flipTimes[firstFlipIndex], timestamps)[0]  # returns frame index of first photodiode flip in each trial\n\n    # Check that first flip is always down -- all of the vrControl code prepares trials in this way\n    if datetime.strptime(self.date, \"%Y-%m-%d\") &gt;= datetime.strptime(\"2022-08-30\", \"%Y-%m-%d\"):\n        # But it didn't prepare it this way before august 30th :(\n        assert np.all(flipValue[firstFlipIndex] == 0), f\"In session {self.sessionPrint()}, first flips in trial are not all down!!\"\n\n    # Check shapes of timeline arrays\n    assert timestamps.ndim == 1, \"timelineTimestamps is not a 1-d array!\"\n    assert timestamps.shape == rotaryPosition.shape, \"timeline timestamps and rotary position arrays do not have the same shape!\"\n\n    # Save timeline oneData\n    self.saveone(timestamps, \"wheelPosition.times\")\n    self.saveone(rotaryPosition, \"wheelPosition.position\")\n    self.saveone(timestamps[lickSamples], \"licks.times\")\n    self.saveone(timestamps[rewardSamples], \"rewards.times\")\n    self.saveone(timestamps[startTrialIndex], \"trials.startTimes\")\n    self.preprocessing.append(\"timeline\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.register","title":"<code>register()</code>","text":"<p>Register the session by running all preprocessing steps.</p> <p>This is the main entry point for registration. It runs all preprocessing steps (timeline, behavior, imaging, red cells, facecam, behavior-to-imaging) and saves session parameters.</p> See Also <p>do_preprocessing : Run all preprocessing steps. save_session_prms : Save session parameters to oneData.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def register(self):\n    \"\"\"\n    Register the session by running all preprocessing steps.\n\n    This is the main entry point for registration. It runs all preprocessing\n    steps (timeline, behavior, imaging, red cells, facecam, behavior-to-imaging)\n    and saves session parameters.\n\n    See Also\n    --------\n    do_preprocessing : Run all preprocessing steps.\n    save_session_prms : Save session parameters to oneData.\n    \"\"\"\n    self.do_preprocessing()\n    self.save_session_prms()\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.B2Registration.timeline_inputs","title":"<code>timeline_inputs(ignore_timestamps=False)</code>","text":"<p>Get list of available timeline input names.</p> <p>Parameters:</p> Name Type Description Default <code>ignore_timestamps</code> <code>bool</code> <p>If True, return only hardware input names. If False, include \"timestamps\" as the first element. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list of str</code> <p>List of timeline input names.</p> Source code in <code>vrAnalysis/registration/register.py</code> <pre><code>def timeline_inputs(self, ignore_timestamps=False):\n    \"\"\"\n    Get list of available timeline input names.\n\n    Parameters\n    ----------\n    ignore_timestamps : bool, optional\n        If True, return only hardware input names. If False, include\n        \"timestamps\" as the first element. Default is False.\n\n    Returns\n    -------\n    list of str\n        List of timeline input names.\n    \"\"\"\n    if not hasattr(self, \"tl_file\"):\n        self.load_timeline_structure()\n    hw_inputs = [hwInput[\"name\"] for hwInput in self.tl_file[\"hw\"][\"inputs\"]]\n    if ignore_timestamps:\n        return hw_inputs\n    return [\"timestamps\", *hw_inputs]\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing","title":"<code>RedCellProcessing</code>","text":"<p>Handle red cell processing for B2Registration sessions.</p> <p>This class processes red cell data from suite2p outputs, computes features for identifying red cells (S2P, dot product, Pearson correlation, phase correlation), and provides methods for updating red cell indices based on cutoff criteria.</p> <p>Parameters:</p> Name Type Description Default <code>b2session</code> <code>B2Session</code> <p>The B2Session object containing the session data.</p> required <code>um_per_pixel</code> <code>float</code> <p>Micrometers per pixel for spatial measurements. Default is 1.3.</p> <code>1.3</code> <code>autoload</code> <code>bool</code> <p>If True, automatically load reference images and masks on initialization. Default is True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>b2session</code> <code>B2Session</code> <p>The B2Session object containing the session data.</p> <code>feature_names</code> <code>list of str</code> <p>Standard names of features used to determine red cell criterion.</p> <code>num_planes</code> <code>int</code> <p>Number of imaging planes in the session.</p> <code>um_per_pixel</code> <code>float</code> <p>Micrometers per pixel for spatial measurements.</p> <code>data_loaded</code> <code>bool</code> <p>Whether reference images and masks have been loaded.</p> <code>reference</code> <code>list of np.ndarray</code> <p>Reference images for each plane (loaded when data_loaded is True).</p> <code>lx, ly</code> <code>int</code> <p>Dimensions of reference images.</p> <code>lam</code> <code>list of np.ndarray</code> <p>Weights of each pixel in ROI masks.</p> <code>ypix, xpix</code> <code>list of np.ndarray</code> <p>Pixel indices for each ROI mask.</p> <code>roi_plane_idx</code> <code>ndarray</code> <p>Plane index for each ROI.</p> <code>red_s2p</code> <code>ndarray</code> <p>Suite2p red cell values for each ROI.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>class RedCellProcessing:\n    \"\"\"\n    Handle red cell processing for B2Registration sessions.\n\n    This class processes red cell data from suite2p outputs, computes features\n    for identifying red cells (S2P, dot product, Pearson correlation, phase\n    correlation), and provides methods for updating red cell indices based on\n    cutoff criteria.\n\n    Parameters\n    ----------\n    b2session : B2Session\n        The B2Session object containing the session data.\n    um_per_pixel : float, optional\n        Micrometers per pixel for spatial measurements. Default is 1.3.\n    autoload : bool, optional\n        If True, automatically load reference images and masks on initialization.\n        Default is True.\n\n    Attributes\n    ----------\n    b2session : B2Session\n        The B2Session object containing the session data.\n    feature_names : list of str\n        Standard names of features used to determine red cell criterion.\n    num_planes : int\n        Number of imaging planes in the session.\n    um_per_pixel : float\n        Micrometers per pixel for spatial measurements.\n    data_loaded : bool\n        Whether reference images and masks have been loaded.\n    reference : list of np.ndarray\n        Reference images for each plane (loaded when data_loaded is True).\n    lx, ly : int\n        Dimensions of reference images.\n    lam : list of np.ndarray\n        Weights of each pixel in ROI masks.\n    ypix, xpix : list of np.ndarray\n        Pixel indices for each ROI mask.\n    roi_plane_idx : np.ndarray\n        Plane index for each ROI.\n    red_s2p : np.ndarray\n        Suite2p red cell values for each ROI.\n    \"\"\"\n\n    def __init__(\n        self,\n        b2session: \"B2Session\",\n        um_per_pixel: float = 1.3,\n        autoload: bool = True,\n    ):\n        \"\"\"\n        Initialize RedCellProcessing object.\n\n        Parameters\n        ----------\n        b2session : B2Session\n            The B2Session object containing the session data.\n        um_per_pixel : float, optional\n            Micrometers per pixel for spatial measurements. Default is 1.3.\n        autoload : bool, optional\n            If True, automatically load reference images and masks on initialization.\n            Default is True.\n\n        Raises\n        ------\n        AssertionError\n            If redcell is not available in suite2p outputs.\n        \"\"\"\n\n        # Make sure redcell is available...\n        msg = \"redcell is not an available suite2p output, so you can't do redCellProcessing.\"\n        assert \"redcell\" in b2session.get_value(\"available\"), msg\n\n        self.b2session = b2session\n\n        # standard names of the features used to determine red cell criterion\n        self.feature_names = [\"S2P\", \"dotProduct\", \"pearson\", \"phaseCorrelation\"]\n\n        # load some critical values for easy readable access\n        self.num_planes = len(self.b2session.get_value(\"planeNames\"))\n        self.um_per_pixel = um_per_pixel  # store this for generating correct axes and measuring distances\n\n        self.data_loaded = False  # initialize to false in case data isn't loaded\n        if autoload:\n            self.load_reference_and_masks()  # prepare reference images and ROI mask data\n\n    # ------------------------------\n    # -- initialization functions --\n    # ------------------------------\n    def load_reference_and_masks(self):\n        \"\"\"\n        Load reference images and ROI masks from suite2p outputs.\n\n        Loads the mean image for channel 2 (red channel) for each plane, along\n        with ROI mask data (lam, ypix, xpix) and ROI plane indices. Also loads\n        suite2p red cell values and creates supporting variables for spatial\n        measurements.\n\n        Raises\n        ------\n        AssertionError\n            If reference images do not all have the same shape.\n        \"\"\"\n        # load reference images\n        ops = self.b2session.load_s2p(\"ops\")\n        self.reference = [op[\"meanImg_chan2\"] for op in ops]\n        self.lx, self.ly = self.reference[0].shape\n        for ref in self.reference:\n            msg = \"reference images do not all have the same shape\"\n            assert (self.lx, self.ly) == ref.shape, msg\n\n        # load masks (lam=weight of each pixel, xpix &amp; ypix=index of each pixel in ROI mask)\n        stat = self.b2session.load_s2p(\"stat\")\n        self.lam = [s[\"lam\"] for s in stat]\n        self.ypix = [s[\"ypix\"] for s in stat]\n        self.xpix = [s[\"xpix\"] for s in stat]\n        self.roi_plane_idx = self.b2session.loadone(\"mpciROIs.stackPosition\")[:, 2]\n\n        # load S2P red cell value\n        self.red_s2p = self.b2session.loadone(\"mpciROIs.redS2P\")  # (preloaded, will never change in this function)\n\n        # create supporting variables for mapping locations and axes\n        self.y_base_ref = np.arange(self.ly)\n        self.x_base_ref = np.arange(self.lx)\n        self.y_dist_ref = self.create_centered_axis(self.ly, self.um_per_pixel)\n        self.x_dist_ref = self.create_centered_axis(self.lx, self.um_per_pixel)\n\n        # update data_loaded field\n        self.data_loaded = True\n\n    # ---------------------------------\n    # -- updating one data functions --\n    # ---------------------------------\n    def one_name_feature_cutoffs(self, name):\n        \"\"\"\n        Generate oneData name for feature cutoff parameters.\n\n        Parameters\n        ----------\n        name : str\n            Feature name (e.g., \"S2P\", \"dotProduct\", \"pearson\", \"phaseCorrelation\").\n\n        Returns\n        -------\n        str\n            OneData name for the feature cutoff parameter, formatted as\n            \"parametersRed{Name}.minMaxCutoff\" where {Name} is the capitalized\n            feature name.\n        \"\"\"\n        return \"parameters\" + \"Red\" + name[0].upper() + name[1:] + \".minMaxCutoff\"\n\n    def update_red_idx(self, s2p_cutoff=None, dot_product_cutoff=None, corr_coef_cutoff=None, phase_corr_cutoff=None):\n        \"\"\"\n        Update red cell index based on feature cutoff values.\n\n        Updates the red cell index by applying minimum and maximum cutoffs to\n        each feature (S2P, dot product, Pearson correlation, phase correlation).\n        Only features with non-NaN cutoff values are applied. The red cell index\n        is updated to include only ROIs that meet all specified criteria.\n\n        Parameters\n        ----------\n        s2p_cutoff : array-like of float, length 2, optional\n            [min, max] cutoff values for suite2p red cell feature. NaN values\n            indicate the cutoff should not be applied. Default is None.\n        dot_product_cutoff : array-like of float, length 2, optional\n            [min, max] cutoff values for dot product feature. Default is None.\n        corr_coef_cutoff : array-like of float, length 2, optional\n            [min, max] cutoff values for Pearson correlation feature.\n            Default is None.\n        phase_corr_cutoff : array-like of float, length 2, optional\n            [min, max] cutoff values for phase correlation feature.\n            Default is None.\n\n        Raises\n        ------\n        ValueError\n            If any cutoff is not a numpy array or list, or if any cutoff does\n            not have exactly 2 elements.\n\n        Notes\n        -----\n        Cutoff values are saved to oneData for future reference. The red cell\n        index is updated in place and saved to oneData.\n        \"\"\"\n        # create initial all true red cell idx\n        red_cell_idx = np.full(self.b2session.loadone(\"mpciROIs.redCellIdx\").shape, True)\n\n        # load feature values for each ROI\n        red_s2p = self.b2session.loadone(\"mpciROIs.redS2P\")\n        dot_product = self.b2session.loadone(\"mpciROIs.redDotProduct\")\n        corr_coef = self.b2session.loadone(\"mpciROIs.redPearson\")\n        phase_corr = self.b2session.loadone(\"mpciROIs.redPhaseCorrelation\")\n\n        # create lists for zipping through each feature/cutoff combination\n        features = [red_s2p, dot_product, corr_coef, phase_corr]\n        cutoffs = [s2p_cutoff, dot_product_cutoff, corr_coef_cutoff, phase_corr_cutoff]\n        usecutoff = [[False, False] for _ in range(len(cutoffs))]\n\n        # check validity of each cutoff and identify whether it should be used\n        for name, use, cutoff in zip(self.feature_names, usecutoff, cutoffs):\n            if not isinstance(cutoff, np.ndarray) and not isinstance(cutoff, list):\n                raise ValueError(f\"Expecting a numpy array or a list for {name} cutoff, got {type(cutoff)}\")\n            assert len(cutoff) == 2, f\"{name} cutoff does not have 2 elements\"\n            if not (np.isnan(cutoff[0])):\n                use[0] = True\n            if not (np.isnan(cutoff[1])):\n                use[1] = True\n\n        # add feature cutoffs to redCellIdx (sets any to False that don't meet the cutoff)\n        for feature, use, cutoff in zip(features, usecutoff, cutoffs):\n            if use[0]:\n                red_cell_idx &amp;= feature &gt;= cutoff[0]\n            if use[1]:\n                red_cell_idx &amp;= feature &lt;= cutoff[1]\n\n        # save new red cell index to one data\n        self.b2session.saveone(red_cell_idx, \"mpciROIs.redCellIdx\")\n\n        # save feature cutoffs to one data\n        for idx, name in enumerate(self.feature_names):\n            self.b2session.saveone(cutoffs[idx], self.one_name_feature_cutoffs(name))\n        print(f\"Red Cell curation choices are saved for session {self.b2session.session_print()}\")\n\n    def update_from_session(self, red_cell, force_update=False):\n        \"\"\"\n        Update red cell cutoffs from another session.\n\n        Copies red cell cutoff parameters from another RedCellProcessing object\n        and applies them to this session.\n\n        Parameters\n        ----------\n        red_cell : RedCellProcessing\n            Another RedCellProcessing object to copy cutoffs from.\n        force_update : bool, optional\n            If False, only allows copying from sessions with the same mouse name.\n            If True, allows copying from any session. Default is False.\n\n        Raises\n        ------\n        AssertionError\n            If red_cell is not a RedCellProcessing object, or if force_update is\n            False and the mouse names don't match.\n        \"\"\"\n        assert isinstance(red_cell, RedCellProcessing), \"red_cell is not a RedCellProcessing object\"\n        if not (force_update):\n            assert (\n                red_cell.b2session.mouse_name == self.b2session.mouse_name\n            ), \"session to copy from is from a different mouse, this isn't allowed without the force_update=True input\"\n        cutoffs = [red_cell.b2session.loadone(red_cell.one_name_feature_cutoffs(name)) for name in self.feature_names]\n        self.update_red_idx(s2p_cutoff=cutoffs[0], dot_product_cutoff=cutoffs[1], corr_coef_cutoff=cutoffs[2], phase_corr_cutoff=cutoffs[3])\n\n    def cropped_phase_correlation(self, plane_idx=None, width=40, eps=1e6, winFunc=lambda x: np.hamming(x.shape[-1])):\n        \"\"\"\n        Compute phase correlation of cropped masks with cropped reference images.\n\n        Returns the phase correlation of each ROI mask (cropped around the ROI\n        centroid) with the corresponding cropped reference image. This is used\n        as a feature for identifying red cells.\n\n        Parameters\n        ----------\n        plane_idx : int or array-like of int, optional\n            Plane indices to process. If None, processes all planes. Default is None.\n        width : float, optional\n            Width in micrometers of the cropped region around each ROI centroid.\n            Default is 40.\n        eps : float, optional\n            Small value added to avoid division by zero in phase correlation.\n            Default is 1e6.\n        winFunc : callable or str, optional\n            Window function to apply before computing phase correlation. If \"hamming\",\n            uses Hamming window. Otherwise should be a callable that takes an array\n            and returns a windowed array. Default is Hamming window.\n\n        Returns\n        -------\n        refStack : np.ndarray\n            Stack of cropped reference images, shape (num_rois, height, width).\n        maskStack : np.ndarray\n            Stack of cropped ROI masks, shape (num_rois, height, width).\n        pxcStack : np.ndarray\n            Stack of phase correlation maps, shape (num_rois, height, width).\n        phase_corr_values : np.ndarray\n            Phase correlation values at the center of each correlation map,\n            shape (num_rois,). This is the feature value used for red cell identification.\n\n        Notes\n        -----\n        The default parameters (width=40um, eps=1e6, and a Hamming window function)\n        were tested on a few sessions and are subjective. Manual curation and\n        parameter adjustment may be necessary for optimal results.\n        \"\"\"\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        if winFunc == \"hamming\":\n            winFunc = lambda x: np.hamming(x.shape[-1])\n        refStack = self.centered_reference_stack(plane_idx=plane_idx, width=width)  # get stack of reference image centered on each ROI\n        maskStack = self.centered_mask_stack(plane_idx=plane_idx, width=width)  # get stack of mask value centered on each ROI\n        window = winFunc(refStack)  # create a window function\n        pxcStack = np.stack(\n            [helpers.phaseCorrelation(ref, mask, eps=eps, window=window) for (ref, mask) in zip(refStack, maskStack)]\n        )  # measure phase correlation\n        pxcCenterPixel = int((pxcStack.shape[2] - 1) / 2)\n        return refStack, maskStack, pxcStack, pxcStack[:, pxcCenterPixel, pxcCenterPixel]\n\n    def compute_dot(self, plane_idx=None, lowcut=12, highcut=250, order=3, fs=512):\n        \"\"\"\n        Compute normalized dot product between filtered reference and ROI masks.\n\n        Computes the dot product between each ROI mask and a Butterworth-filtered\n        reference image. This is used as a feature for identifying red cells.\n\n        Parameters\n        ----------\n        plane_idx : int or array-like of int, optional\n            Plane indices to process. If None, processes all planes. Default is None.\n        lowcut : float, optional\n            Low cutoff frequency for Butterworth bandpass filter in Hz.\n            Default is 12.\n        highcut : float, optional\n            High cutoff frequency for Butterworth bandpass filter in Hz.\n            Default is 250.\n        order : int, optional\n            Order of the Butterworth filter. Default is 3.\n        fs : float, optional\n            Sampling frequency for the filter in Hz. Default is 512.\n\n        Returns\n        -------\n        np.ndarray\n            Normalized dot product values for each ROI, shape (num_rois,).\n        \"\"\"\n        if plane_idx is None:\n            plane_idx = np.arange(self.num_planes)\n        if isinstance(plane_idx, (int, np.integer)):\n            plane_idx = (plane_idx,)  # make plane_idx iterable\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n\n        dot_prod = []\n        for plane in plane_idx:\n            t = time.time()\n            c_roi_idx = np.where(self.roi_plane_idx == plane)[0]  # index of ROIs in this plane\n            bwReference = helpers.butterworthbpf(self.reference[plane], lowcut, highcut, order=order, fs=fs)  # filtered reference image\n            bwReference /= np.linalg.norm(bwReference)  # adjust to norm for straightforward cosine angle\n            # compute normalized dot product for each ROI\n            dot_prod.append(\n                np.array([bwReference[self.ypix[roi], self.xpix[roi]] @ self.lam[roi] / np.linalg.norm(self.lam[roi]) for roi in c_roi_idx])\n            )\n\n        return np.concatenate(dot_prod)\n\n    def compute_corr(self, plane_idx=None, width=20, lowcut=12, highcut=250, order=3, fs=512):\n        \"\"\"\n        Compute Pearson correlation between filtered reference and ROI masks.\n\n        Computes the Pearson correlation coefficient between each ROI mask and\n        a Butterworth-filtered reference image within a cropped region around\n        each ROI. This is used as a feature for identifying red cells.\n\n        Parameters\n        ----------\n        plane_idx : int or array-like of int, optional\n            Plane indices to process. If None, processes all planes. Default is None.\n        width : float, optional\n            Width in micrometers of the cropped region around each ROI centroid.\n            Default is 20.\n        lowcut : float, optional\n            Low cutoff frequency for Butterworth bandpass filter in Hz.\n            Default is 12.\n        highcut : float, optional\n            High cutoff frequency for Butterworth bandpass filter in Hz.\n            Default is 250.\n        order : int, optional\n            Order of the Butterworth filter. Default is 3.\n        fs : float, optional\n            Sampling frequency for the filter in Hz. Default is 512.\n\n        Returns\n        -------\n        np.ndarray\n            Pearson correlation coefficients for each ROI, shape (num_rois,).\n        \"\"\"\n        if plane_idx is None:\n            plane_idx = np.arange(self.num_planes)\n        if isinstance(plane_idx, (int, np.integer)):\n            plane_idx = (plane_idx,)  # make plane_idx iterable\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n\n        corr_coef = []\n        for plane in plane_idx:\n            num_rois = self.b2session.get_value(\"roiPerPlane\")[plane]\n            c_ref_stack = np.reshape(\n                self.centered_reference_stack(plane_idx=plane, width=width, fill=np.nan, filtPrms=(lowcut, highcut, order, fs)),\n                (num_rois, -1),\n            )\n            c_mask_stack = np.reshape(self.centered_mask_stack(plane_idx=plane, width=width, fill=0), (num_rois, -1))\n            c_mask_stack[np.isnan(c_ref_stack)] = np.nan\n\n            # Measure mean and standard deviation (and number of non-nan datapoints)\n            u_ref = np.nanmean(c_ref_stack, axis=1, keepdims=True)\n            u_mask = np.nanmean(c_mask_stack, axis=1, keepdims=True)\n            s_ref = np.nanstd(c_ref_stack, axis=1)\n            s_mask = np.nanstd(c_mask_stack, axis=1)\n            N = np.sum(~np.isnan(c_ref_stack), axis=1)\n\n            # compute correlation coefficient and add to storage variable\n            corr_coef.append(np.nansum((c_ref_stack - u_ref) * (c_mask_stack - u_mask), axis=1) / N / s_ref / s_mask)\n\n        return np.concatenate(corr_coef)\n\n    # --------------------------\n    # -- supporting functions --\n    # --------------------------\n    def create_centered_axis(self, numElements, scale=1):\n        \"\"\"\n        Create a centered axis array.\n\n        Parameters\n        ----------\n        numElements : int\n            Number of elements in the axis.\n        scale : float, optional\n            Scaling factor for the axis. Default is 1.\n\n        Returns\n        -------\n        np.ndarray\n            Centered axis array, shape (numElements,), with values ranging from\n            -scale*(numElements-1)/2 to scale*(numElements-1)/2.\n        \"\"\"\n        return scale * (np.arange(numElements) - (numElements - 1) / 2)\n\n    def getyref(self, yCenter):\n        \"\"\"\n        Get y-axis reference coordinates relative to a center point.\n\n        Parameters\n        ----------\n        yCenter : float\n            Y-coordinate of the center point in pixels.\n\n        Returns\n        -------\n        np.ndarray\n            Y-axis coordinates in micrometers relative to yCenter, shape (ly,).\n        \"\"\"\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        return self.um_per_pixel * (self.y_base_ref - yCenter)\n\n    def getxref(self, xCenter):\n        \"\"\"\n        Get x-axis reference coordinates relative to a center point.\n\n        Parameters\n        ----------\n        xCenter : float\n            X-coordinate of the center point in pixels.\n\n        Returns\n        -------\n        np.ndarray\n            X-axis coordinates in micrometers relative to xCenter, shape (lx,).\n        \"\"\"\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        return self.um_per_pixel * (self.x_base_ref - xCenter)\n\n    def get_roi_centroid(self, idx, mode=\"weightedmean\"):\n        \"\"\"\n        Get the centroid of an ROI.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the ROI.\n        mode : str, optional\n            Method for computing centroid. \"weightedmean\" uses pixel weights (lam),\n            \"median\" uses median pixel coordinates. Default is \"weightedmean\".\n\n        Returns\n        -------\n        yc : float\n            Y-coordinate of the centroid in pixels.\n        xc : float\n            X-coordinate of the centroid in pixels.\n        \"\"\"\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n\n        if mode == \"weightedmean\":\n            yc = np.sum(self.lam[idx] * self.ypix[idx]) / np.sum(self.lam[idx])\n            xc = np.sum(self.lam[idx] * self.xpix[idx]) / np.sum(self.lam[idx])\n        elif mode == \"median\":\n            yc = int(np.median(self.ypix[idx]))\n            xc = int(np.median(self.xpix[idx]))\n\n        return yc, xc\n\n    def get_roi_range(self, idx):\n        \"\"\"\n        Get the range (peak-to-peak) of x and y pixels for an ROI.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the ROI.\n\n        Returns\n        -------\n        yr : int\n            Range of y-pixels (peak-to-peak).\n        xr : int\n            Range of x-pixels (peak-to-peak).\n        \"\"\"\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        # get range of x and y pixels for a particular ROI\n        yr = np.ptp(self.ypix[idx])\n        xr = np.ptp(self.xpix[idx])\n        return yr, xr\n\n    def get_roi_in_plane_idx(self, idx):\n        \"\"\"\n        Get the index of an ROI within its own plane.\n\n        Parameters\n        ----------\n        idx : int\n            Global ROI index.\n\n        Returns\n        -------\n        int\n            Index of the ROI within its plane (0-indexed within that plane).\n        \"\"\"\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        # return index of ROI within it's own plane\n        plane_idx = self.roi_plane_idx[idx]\n        return idx - np.sum(self.roi_plane_idx &lt; plane_idx)\n\n    def centered_reference_stack(self, plane_idx=None, width=15, fill=0.0, filtPrms=None):\n        \"\"\"\n        Create a stack of reference images centered on each ROI.\n\n        Returns a stack of reference images cropped around each ROI centroid\n        within a specified width. Optionally applies a Butterworth filter to\n        the reference images before cropping.\n\n        Parameters\n        ----------\n        plane_idx : int or array-like of int, optional\n            Plane indices to process. If None, processes all planes. Default is None.\n        width : float, optional\n            Width in micrometers of the cropped region around each ROI centroid.\n            Default is 15.\n        fill : float, optional\n            Value to use for background pixels outside the image bounds.\n            Should be 0.0 or np.nan. Default is 0.0.\n        filtPrms : tuple of 4 floats, optional\n            Parameters for Butterworth filter: (lowcut, highcut, order, fs).\n            If None, no filtering is applied. Default is None.\n\n        Returns\n        -------\n        np.ndarray\n            Stack of centered reference images, shape (num_rois, height, width),\n            where height = width = 2 * round(width / um_per_pixel) + 1.\n        \"\"\"\n        if plane_idx is None:\n            plane_idx = np.arange(self.num_planes)\n        if isinstance(plane_idx, (int, np.integer)):\n            plane_idx = (plane_idx,)  # make plane_idx iterable\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        num_pixels = int(np.round(width / self.um_per_pixel))  # numPixels to each side around the centroid\n        ref_stack = []\n        for plane in plane_idx:\n            c_reference = self.reference[plane]\n            if filtPrms is not None:\n                # filtered reference image\n                c_reference = helpers.butterworthbpf(c_reference, filtPrms[0], filtPrms[1], order=filtPrms[2], fs=filtPrms[3])\n            idx_roi_in_plane = np.where(self.roi_plane_idx == plane)[0]\n            ref_stack.append(np.full((len(idx_roi_in_plane), 2 * num_pixels + 1, 2 * num_pixels + 1), fill))\n            # fill the reference stack with the reference image\n            for idx, idx_roi in enumerate(idx_roi_in_plane):\n                yc, xc = self.get_roi_centroid(idx_roi, mode=\"median\")\n                yUse = (np.maximum(yc - num_pixels, 0), np.minimum(yc + num_pixels + 1, self.ly))\n                xUse = (np.maximum(xc - num_pixels, 0), np.minimum(xc + num_pixels + 1, self.lx))\n                yMissing = (\n                    -np.minimum(yc - num_pixels, 0),\n                    -np.minimum(self.ly - (yc + num_pixels + 1), 0),\n                )\n                xMissing = (\n                    -np.minimum(xc - num_pixels, 0),\n                    -np.minimum(self.lx - (xc + num_pixels + 1), 0),\n                )\n                ref_stack[-1][\n                    idx,\n                    yMissing[0] : 2 * num_pixels + 1 - yMissing[1],\n                    xMissing[0] : 2 * num_pixels + 1 - xMissing[1],\n                ] = c_reference[yUse[0] : yUse[1], xUse[0] : xUse[1]]\n        return np.concatenate(ref_stack, axis=0).astype(np.float32)\n\n    def centered_mask_stack(self, plane_idx=None, width=15, fill=0.0):\n        \"\"\"\n        Create a stack of ROI masks centered on each ROI.\n\n        Returns a stack of ROI masks cropped around each ROI centroid within\n        a specified width. Mask values (lam) are placed at the appropriate\n        positions in the centered stack.\n\n        Parameters\n        ----------\n        plane_idx : int or array-like of int, optional\n            Plane indices to process. If None, processes all planes. Default is None.\n        width : float, optional\n            Width in micrometers of the cropped region around each ROI centroid.\n            Default is 15.\n        fill : float, optional\n            Value to use for background pixels outside the ROI mask.\n            Should be 0.0 or np.nan. Default is 0.0.\n\n        Returns\n        -------\n        np.ndarray\n            Stack of centered ROI masks, shape (num_rois, height, width),\n            where height = width = 2 * round(width / um_per_pixel) + 1.\n        \"\"\"\n        if plane_idx is None:\n            plane_idx = np.arange(self.num_planes)\n        if isinstance(plane_idx, (int, np.integer)):\n            plane_idx = (plane_idx,)  # make plane_idx iterable\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        num_pixels = int(np.round(width / self.um_per_pixel))  # numPixels to each side around the centroid\n        mask_stack = []\n        for plane in plane_idx:\n            idx_roi_in_plane = np.where(self.roi_plane_idx == plane)[0]\n            mask_stack.append(np.full((len(idx_roi_in_plane), 2 * num_pixels + 1, 2 * num_pixels + 1), fill))\n            for idx, idx_roi in enumerate(idx_roi_in_plane):\n                yc, xc = self.get_roi_centroid(idx_roi, mode=\"median\")\n                # centered y&amp;x pixels of ROI\n                cyidx = self.ypix[idx_roi] - yc + num_pixels\n                cxidx = self.xpix[idx_roi] - xc + num_pixels\n                # index of pixels still within width of stack\n                idx_use_points = (cyidx &gt;= 0) &amp; (cyidx &lt; 2 * num_pixels + 1) &amp; (cxidx &gt;= 0) &amp; (cxidx &lt; 2 * num_pixels + 1)\n                mask_stack[-1][idx, cyidx[idx_use_points], cxidx[idx_use_points]] = self.lam[idx_roi][idx_use_points]\n        return np.concatenate(mask_stack, axis=0).astype(np.float32)\n\n    def compute_volume(self, plane_idx=None):\n        \"\"\"\n        Compute full-volume ROI masks for specified planes.\n\n        Creates a 3D array where each ROI mask is placed at its original\n        position in the full image plane. This is useful for visualization\n        or volume-based operations.\n\n        Parameters\n        ----------\n        plane_idx : int or array-like of int, optional\n            Plane indices to process. If None, processes all planes. Default is None.\n\n        Returns\n        -------\n        np.ndarray\n            Volume array of ROI masks, shape (num_rois, ly, lx), where ly and lx\n            are the dimensions of the reference images.\n\n        Raises\n        ------\n        AssertionError\n            If any plane index is out of range.\n        \"\"\"\n        if plane_idx is None:\n            plane_idx = np.arange(self.num_planes)\n        if isinstance(plane_idx, (int, np.integer)):\n            plane_idx = (plane_idx,)  # make plane_idx iterable\n        msg = f\"in session: {self.b2session.session_print()}, there are only {self.num_planes} planes!\"\n        assert all([0 &lt;= plane &lt; self.num_planes for plane in plane_idx]), msg\n        if not (self.data_loaded):\n            self.load_reference_and_masks()\n        roi_mask_volume = []\n        for plane in plane_idx:\n            roi_mask_volume.append(np.zeros((self.b2session.get_value(\"roiPerPlane\")[plane], self.ly, self.lx)))\n            idx_roi_in_plane = np.where(self.roi_plane_idx == plane)[0]\n            for roi in range(self.b2session.get_value(\"roiPerPlane\")[plane]):\n                c_roi_idx = idx_roi_in_plane[roi]\n                roi_mask_volume[-1][roi, self.ypix[c_roi_idx], self.xpix[c_roi_idx]] = self.lam[c_roi_idx]\n        return np.concatenate(roi_mask_volume, axis=0)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.__init__","title":"<code>__init__(b2session, um_per_pixel=1.3, autoload=True)</code>","text":"<p>Initialize RedCellProcessing object.</p> <p>Parameters:</p> Name Type Description Default <code>b2session</code> <code>B2Session</code> <p>The B2Session object containing the session data.</p> required <code>um_per_pixel</code> <code>float</code> <p>Micrometers per pixel for spatial measurements. Default is 1.3.</p> <code>1.3</code> <code>autoload</code> <code>bool</code> <p>If True, automatically load reference images and masks on initialization. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If redcell is not available in suite2p outputs.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def __init__(\n    self,\n    b2session: \"B2Session\",\n    um_per_pixel: float = 1.3,\n    autoload: bool = True,\n):\n    \"\"\"\n    Initialize RedCellProcessing object.\n\n    Parameters\n    ----------\n    b2session : B2Session\n        The B2Session object containing the session data.\n    um_per_pixel : float, optional\n        Micrometers per pixel for spatial measurements. Default is 1.3.\n    autoload : bool, optional\n        If True, automatically load reference images and masks on initialization.\n        Default is True.\n\n    Raises\n    ------\n    AssertionError\n        If redcell is not available in suite2p outputs.\n    \"\"\"\n\n    # Make sure redcell is available...\n    msg = \"redcell is not an available suite2p output, so you can't do redCellProcessing.\"\n    assert \"redcell\" in b2session.get_value(\"available\"), msg\n\n    self.b2session = b2session\n\n    # standard names of the features used to determine red cell criterion\n    self.feature_names = [\"S2P\", \"dotProduct\", \"pearson\", \"phaseCorrelation\"]\n\n    # load some critical values for easy readable access\n    self.num_planes = len(self.b2session.get_value(\"planeNames\"))\n    self.um_per_pixel = um_per_pixel  # store this for generating correct axes and measuring distances\n\n    self.data_loaded = False  # initialize to false in case data isn't loaded\n    if autoload:\n        self.load_reference_and_masks()  # prepare reference images and ROI mask data\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.centered_mask_stack","title":"<code>centered_mask_stack(plane_idx=None, width=15, fill=0.0)</code>","text":"<p>Create a stack of ROI masks centered on each ROI.</p> <p>Returns a stack of ROI masks cropped around each ROI centroid within a specified width. Mask values (lam) are placed at the appropriate positions in the centered stack.</p> <p>Parameters:</p> Name Type Description Default <code>plane_idx</code> <code>int or array-like of int</code> <p>Plane indices to process. If None, processes all planes. Default is None.</p> <code>None</code> <code>width</code> <code>float</code> <p>Width in micrometers of the cropped region around each ROI centroid. Default is 15.</p> <code>15</code> <code>fill</code> <code>float</code> <p>Value to use for background pixels outside the ROI mask. Should be 0.0 or np.nan. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Stack of centered ROI masks, shape (num_rois, height, width), where height = width = 2 * round(width / um_per_pixel) + 1.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def centered_mask_stack(self, plane_idx=None, width=15, fill=0.0):\n    \"\"\"\n    Create a stack of ROI masks centered on each ROI.\n\n    Returns a stack of ROI masks cropped around each ROI centroid within\n    a specified width. Mask values (lam) are placed at the appropriate\n    positions in the centered stack.\n\n    Parameters\n    ----------\n    plane_idx : int or array-like of int, optional\n        Plane indices to process. If None, processes all planes. Default is None.\n    width : float, optional\n        Width in micrometers of the cropped region around each ROI centroid.\n        Default is 15.\n    fill : float, optional\n        Value to use for background pixels outside the ROI mask.\n        Should be 0.0 or np.nan. Default is 0.0.\n\n    Returns\n    -------\n    np.ndarray\n        Stack of centered ROI masks, shape (num_rois, height, width),\n        where height = width = 2 * round(width / um_per_pixel) + 1.\n    \"\"\"\n    if plane_idx is None:\n        plane_idx = np.arange(self.num_planes)\n    if isinstance(plane_idx, (int, np.integer)):\n        plane_idx = (plane_idx,)  # make plane_idx iterable\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    num_pixels = int(np.round(width / self.um_per_pixel))  # numPixels to each side around the centroid\n    mask_stack = []\n    for plane in plane_idx:\n        idx_roi_in_plane = np.where(self.roi_plane_idx == plane)[0]\n        mask_stack.append(np.full((len(idx_roi_in_plane), 2 * num_pixels + 1, 2 * num_pixels + 1), fill))\n        for idx, idx_roi in enumerate(idx_roi_in_plane):\n            yc, xc = self.get_roi_centroid(idx_roi, mode=\"median\")\n            # centered y&amp;x pixels of ROI\n            cyidx = self.ypix[idx_roi] - yc + num_pixels\n            cxidx = self.xpix[idx_roi] - xc + num_pixels\n            # index of pixels still within width of stack\n            idx_use_points = (cyidx &gt;= 0) &amp; (cyidx &lt; 2 * num_pixels + 1) &amp; (cxidx &gt;= 0) &amp; (cxidx &lt; 2 * num_pixels + 1)\n            mask_stack[-1][idx, cyidx[idx_use_points], cxidx[idx_use_points]] = self.lam[idx_roi][idx_use_points]\n    return np.concatenate(mask_stack, axis=0).astype(np.float32)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.centered_reference_stack","title":"<code>centered_reference_stack(plane_idx=None, width=15, fill=0.0, filtPrms=None)</code>","text":"<p>Create a stack of reference images centered on each ROI.</p> <p>Returns a stack of reference images cropped around each ROI centroid within a specified width. Optionally applies a Butterworth filter to the reference images before cropping.</p> <p>Parameters:</p> Name Type Description Default <code>plane_idx</code> <code>int or array-like of int</code> <p>Plane indices to process. If None, processes all planes. Default is None.</p> <code>None</code> <code>width</code> <code>float</code> <p>Width in micrometers of the cropped region around each ROI centroid. Default is 15.</p> <code>15</code> <code>fill</code> <code>float</code> <p>Value to use for background pixels outside the image bounds. Should be 0.0 or np.nan. Default is 0.0.</p> <code>0.0</code> <code>filtPrms</code> <code>tuple of 4 floats</code> <p>Parameters for Butterworth filter: (lowcut, highcut, order, fs). If None, no filtering is applied. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Stack of centered reference images, shape (num_rois, height, width), where height = width = 2 * round(width / um_per_pixel) + 1.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def centered_reference_stack(self, plane_idx=None, width=15, fill=0.0, filtPrms=None):\n    \"\"\"\n    Create a stack of reference images centered on each ROI.\n\n    Returns a stack of reference images cropped around each ROI centroid\n    within a specified width. Optionally applies a Butterworth filter to\n    the reference images before cropping.\n\n    Parameters\n    ----------\n    plane_idx : int or array-like of int, optional\n        Plane indices to process. If None, processes all planes. Default is None.\n    width : float, optional\n        Width in micrometers of the cropped region around each ROI centroid.\n        Default is 15.\n    fill : float, optional\n        Value to use for background pixels outside the image bounds.\n        Should be 0.0 or np.nan. Default is 0.0.\n    filtPrms : tuple of 4 floats, optional\n        Parameters for Butterworth filter: (lowcut, highcut, order, fs).\n        If None, no filtering is applied. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        Stack of centered reference images, shape (num_rois, height, width),\n        where height = width = 2 * round(width / um_per_pixel) + 1.\n    \"\"\"\n    if plane_idx is None:\n        plane_idx = np.arange(self.num_planes)\n    if isinstance(plane_idx, (int, np.integer)):\n        plane_idx = (plane_idx,)  # make plane_idx iterable\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    num_pixels = int(np.round(width / self.um_per_pixel))  # numPixels to each side around the centroid\n    ref_stack = []\n    for plane in plane_idx:\n        c_reference = self.reference[plane]\n        if filtPrms is not None:\n            # filtered reference image\n            c_reference = helpers.butterworthbpf(c_reference, filtPrms[0], filtPrms[1], order=filtPrms[2], fs=filtPrms[3])\n        idx_roi_in_plane = np.where(self.roi_plane_idx == plane)[0]\n        ref_stack.append(np.full((len(idx_roi_in_plane), 2 * num_pixels + 1, 2 * num_pixels + 1), fill))\n        # fill the reference stack with the reference image\n        for idx, idx_roi in enumerate(idx_roi_in_plane):\n            yc, xc = self.get_roi_centroid(idx_roi, mode=\"median\")\n            yUse = (np.maximum(yc - num_pixels, 0), np.minimum(yc + num_pixels + 1, self.ly))\n            xUse = (np.maximum(xc - num_pixels, 0), np.minimum(xc + num_pixels + 1, self.lx))\n            yMissing = (\n                -np.minimum(yc - num_pixels, 0),\n                -np.minimum(self.ly - (yc + num_pixels + 1), 0),\n            )\n            xMissing = (\n                -np.minimum(xc - num_pixels, 0),\n                -np.minimum(self.lx - (xc + num_pixels + 1), 0),\n            )\n            ref_stack[-1][\n                idx,\n                yMissing[0] : 2 * num_pixels + 1 - yMissing[1],\n                xMissing[0] : 2 * num_pixels + 1 - xMissing[1],\n            ] = c_reference[yUse[0] : yUse[1], xUse[0] : xUse[1]]\n    return np.concatenate(ref_stack, axis=0).astype(np.float32)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.compute_corr","title":"<code>compute_corr(plane_idx=None, width=20, lowcut=12, highcut=250, order=3, fs=512)</code>","text":"<p>Compute Pearson correlation between filtered reference and ROI masks.</p> <p>Computes the Pearson correlation coefficient between each ROI mask and a Butterworth-filtered reference image within a cropped region around each ROI. This is used as a feature for identifying red cells.</p> <p>Parameters:</p> Name Type Description Default <code>plane_idx</code> <code>int or array-like of int</code> <p>Plane indices to process. If None, processes all planes. Default is None.</p> <code>None</code> <code>width</code> <code>float</code> <p>Width in micrometers of the cropped region around each ROI centroid. Default is 20.</p> <code>20</code> <code>lowcut</code> <code>float</code> <p>Low cutoff frequency for Butterworth bandpass filter in Hz. Default is 12.</p> <code>12</code> <code>highcut</code> <code>float</code> <p>High cutoff frequency for Butterworth bandpass filter in Hz. Default is 250.</p> <code>250</code> <code>order</code> <code>int</code> <p>Order of the Butterworth filter. Default is 3.</p> <code>3</code> <code>fs</code> <code>float</code> <p>Sampling frequency for the filter in Hz. Default is 512.</p> <code>512</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Pearson correlation coefficients for each ROI, shape (num_rois,).</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def compute_corr(self, plane_idx=None, width=20, lowcut=12, highcut=250, order=3, fs=512):\n    \"\"\"\n    Compute Pearson correlation between filtered reference and ROI masks.\n\n    Computes the Pearson correlation coefficient between each ROI mask and\n    a Butterworth-filtered reference image within a cropped region around\n    each ROI. This is used as a feature for identifying red cells.\n\n    Parameters\n    ----------\n    plane_idx : int or array-like of int, optional\n        Plane indices to process. If None, processes all planes. Default is None.\n    width : float, optional\n        Width in micrometers of the cropped region around each ROI centroid.\n        Default is 20.\n    lowcut : float, optional\n        Low cutoff frequency for Butterworth bandpass filter in Hz.\n        Default is 12.\n    highcut : float, optional\n        High cutoff frequency for Butterworth bandpass filter in Hz.\n        Default is 250.\n    order : int, optional\n        Order of the Butterworth filter. Default is 3.\n    fs : float, optional\n        Sampling frequency for the filter in Hz. Default is 512.\n\n    Returns\n    -------\n    np.ndarray\n        Pearson correlation coefficients for each ROI, shape (num_rois,).\n    \"\"\"\n    if plane_idx is None:\n        plane_idx = np.arange(self.num_planes)\n    if isinstance(plane_idx, (int, np.integer)):\n        plane_idx = (plane_idx,)  # make plane_idx iterable\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n\n    corr_coef = []\n    for plane in plane_idx:\n        num_rois = self.b2session.get_value(\"roiPerPlane\")[plane]\n        c_ref_stack = np.reshape(\n            self.centered_reference_stack(plane_idx=plane, width=width, fill=np.nan, filtPrms=(lowcut, highcut, order, fs)),\n            (num_rois, -1),\n        )\n        c_mask_stack = np.reshape(self.centered_mask_stack(plane_idx=plane, width=width, fill=0), (num_rois, -1))\n        c_mask_stack[np.isnan(c_ref_stack)] = np.nan\n\n        # Measure mean and standard deviation (and number of non-nan datapoints)\n        u_ref = np.nanmean(c_ref_stack, axis=1, keepdims=True)\n        u_mask = np.nanmean(c_mask_stack, axis=1, keepdims=True)\n        s_ref = np.nanstd(c_ref_stack, axis=1)\n        s_mask = np.nanstd(c_mask_stack, axis=1)\n        N = np.sum(~np.isnan(c_ref_stack), axis=1)\n\n        # compute correlation coefficient and add to storage variable\n        corr_coef.append(np.nansum((c_ref_stack - u_ref) * (c_mask_stack - u_mask), axis=1) / N / s_ref / s_mask)\n\n    return np.concatenate(corr_coef)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.compute_dot","title":"<code>compute_dot(plane_idx=None, lowcut=12, highcut=250, order=3, fs=512)</code>","text":"<p>Compute normalized dot product between filtered reference and ROI masks.</p> <p>Computes the dot product between each ROI mask and a Butterworth-filtered reference image. This is used as a feature for identifying red cells.</p> <p>Parameters:</p> Name Type Description Default <code>plane_idx</code> <code>int or array-like of int</code> <p>Plane indices to process. If None, processes all planes. Default is None.</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Low cutoff frequency for Butterworth bandpass filter in Hz. Default is 12.</p> <code>12</code> <code>highcut</code> <code>float</code> <p>High cutoff frequency for Butterworth bandpass filter in Hz. Default is 250.</p> <code>250</code> <code>order</code> <code>int</code> <p>Order of the Butterworth filter. Default is 3.</p> <code>3</code> <code>fs</code> <code>float</code> <p>Sampling frequency for the filter in Hz. Default is 512.</p> <code>512</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Normalized dot product values for each ROI, shape (num_rois,).</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def compute_dot(self, plane_idx=None, lowcut=12, highcut=250, order=3, fs=512):\n    \"\"\"\n    Compute normalized dot product between filtered reference and ROI masks.\n\n    Computes the dot product between each ROI mask and a Butterworth-filtered\n    reference image. This is used as a feature for identifying red cells.\n\n    Parameters\n    ----------\n    plane_idx : int or array-like of int, optional\n        Plane indices to process. If None, processes all planes. Default is None.\n    lowcut : float, optional\n        Low cutoff frequency for Butterworth bandpass filter in Hz.\n        Default is 12.\n    highcut : float, optional\n        High cutoff frequency for Butterworth bandpass filter in Hz.\n        Default is 250.\n    order : int, optional\n        Order of the Butterworth filter. Default is 3.\n    fs : float, optional\n        Sampling frequency for the filter in Hz. Default is 512.\n\n    Returns\n    -------\n    np.ndarray\n        Normalized dot product values for each ROI, shape (num_rois,).\n    \"\"\"\n    if plane_idx is None:\n        plane_idx = np.arange(self.num_planes)\n    if isinstance(plane_idx, (int, np.integer)):\n        plane_idx = (plane_idx,)  # make plane_idx iterable\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n\n    dot_prod = []\n    for plane in plane_idx:\n        t = time.time()\n        c_roi_idx = np.where(self.roi_plane_idx == plane)[0]  # index of ROIs in this plane\n        bwReference = helpers.butterworthbpf(self.reference[plane], lowcut, highcut, order=order, fs=fs)  # filtered reference image\n        bwReference /= np.linalg.norm(bwReference)  # adjust to norm for straightforward cosine angle\n        # compute normalized dot product for each ROI\n        dot_prod.append(\n            np.array([bwReference[self.ypix[roi], self.xpix[roi]] @ self.lam[roi] / np.linalg.norm(self.lam[roi]) for roi in c_roi_idx])\n        )\n\n    return np.concatenate(dot_prod)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.compute_volume","title":"<code>compute_volume(plane_idx=None)</code>","text":"<p>Compute full-volume ROI masks for specified planes.</p> <p>Creates a 3D array where each ROI mask is placed at its original position in the full image plane. This is useful for visualization or volume-based operations.</p> <p>Parameters:</p> Name Type Description Default <code>plane_idx</code> <code>int or array-like of int</code> <p>Plane indices to process. If None, processes all planes. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Volume array of ROI masks, shape (num_rois, ly, lx), where ly and lx are the dimensions of the reference images.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any plane index is out of range.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def compute_volume(self, plane_idx=None):\n    \"\"\"\n    Compute full-volume ROI masks for specified planes.\n\n    Creates a 3D array where each ROI mask is placed at its original\n    position in the full image plane. This is useful for visualization\n    or volume-based operations.\n\n    Parameters\n    ----------\n    plane_idx : int or array-like of int, optional\n        Plane indices to process. If None, processes all planes. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        Volume array of ROI masks, shape (num_rois, ly, lx), where ly and lx\n        are the dimensions of the reference images.\n\n    Raises\n    ------\n    AssertionError\n        If any plane index is out of range.\n    \"\"\"\n    if plane_idx is None:\n        plane_idx = np.arange(self.num_planes)\n    if isinstance(plane_idx, (int, np.integer)):\n        plane_idx = (plane_idx,)  # make plane_idx iterable\n    msg = f\"in session: {self.b2session.session_print()}, there are only {self.num_planes} planes!\"\n    assert all([0 &lt;= plane &lt; self.num_planes for plane in plane_idx]), msg\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    roi_mask_volume = []\n    for plane in plane_idx:\n        roi_mask_volume.append(np.zeros((self.b2session.get_value(\"roiPerPlane\")[plane], self.ly, self.lx)))\n        idx_roi_in_plane = np.where(self.roi_plane_idx == plane)[0]\n        for roi in range(self.b2session.get_value(\"roiPerPlane\")[plane]):\n            c_roi_idx = idx_roi_in_plane[roi]\n            roi_mask_volume[-1][roi, self.ypix[c_roi_idx], self.xpix[c_roi_idx]] = self.lam[c_roi_idx]\n    return np.concatenate(roi_mask_volume, axis=0)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.create_centered_axis","title":"<code>create_centered_axis(numElements, scale=1)</code>","text":"<p>Create a centered axis array.</p> <p>Parameters:</p> Name Type Description Default <code>numElements</code> <code>int</code> <p>Number of elements in the axis.</p> required <code>scale</code> <code>float</code> <p>Scaling factor for the axis. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Centered axis array, shape (numElements,), with values ranging from -scale(numElements-1)/2 to scale(numElements-1)/2.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def create_centered_axis(self, numElements, scale=1):\n    \"\"\"\n    Create a centered axis array.\n\n    Parameters\n    ----------\n    numElements : int\n        Number of elements in the axis.\n    scale : float, optional\n        Scaling factor for the axis. Default is 1.\n\n    Returns\n    -------\n    np.ndarray\n        Centered axis array, shape (numElements,), with values ranging from\n        -scale*(numElements-1)/2 to scale*(numElements-1)/2.\n    \"\"\"\n    return scale * (np.arange(numElements) - (numElements - 1) / 2)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.cropped_phase_correlation","title":"<code>cropped_phase_correlation(plane_idx=None, width=40, eps=1000000.0, winFunc=lambda x: np.hamming(x.shape[-1]))</code>","text":"<p>Compute phase correlation of cropped masks with cropped reference images.</p> <p>Returns the phase correlation of each ROI mask (cropped around the ROI centroid) with the corresponding cropped reference image. This is used as a feature for identifying red cells.</p> <p>Parameters:</p> Name Type Description Default <code>plane_idx</code> <code>int or array-like of int</code> <p>Plane indices to process. If None, processes all planes. Default is None.</p> <code>None</code> <code>width</code> <code>float</code> <p>Width in micrometers of the cropped region around each ROI centroid. Default is 40.</p> <code>40</code> <code>eps</code> <code>float</code> <p>Small value added to avoid division by zero in phase correlation. Default is 1e6.</p> <code>1000000.0</code> <code>winFunc</code> <code>callable or str</code> <p>Window function to apply before computing phase correlation. If \"hamming\", uses Hamming window. Otherwise should be a callable that takes an array and returns a windowed array. Default is Hamming window.</p> <code>lambda x: hamming(shape[-1])</code> <p>Returns:</p> Name Type Description <code>refStack</code> <code>ndarray</code> <p>Stack of cropped reference images, shape (num_rois, height, width).</p> <code>maskStack</code> <code>ndarray</code> <p>Stack of cropped ROI masks, shape (num_rois, height, width).</p> <code>pxcStack</code> <code>ndarray</code> <p>Stack of phase correlation maps, shape (num_rois, height, width).</p> <code>phase_corr_values</code> <code>ndarray</code> <p>Phase correlation values at the center of each correlation map, shape (num_rois,). This is the feature value used for red cell identification.</p> Notes <p>The default parameters (width=40um, eps=1e6, and a Hamming window function) were tested on a few sessions and are subjective. Manual curation and parameter adjustment may be necessary for optimal results.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def cropped_phase_correlation(self, plane_idx=None, width=40, eps=1e6, winFunc=lambda x: np.hamming(x.shape[-1])):\n    \"\"\"\n    Compute phase correlation of cropped masks with cropped reference images.\n\n    Returns the phase correlation of each ROI mask (cropped around the ROI\n    centroid) with the corresponding cropped reference image. This is used\n    as a feature for identifying red cells.\n\n    Parameters\n    ----------\n    plane_idx : int or array-like of int, optional\n        Plane indices to process. If None, processes all planes. Default is None.\n    width : float, optional\n        Width in micrometers of the cropped region around each ROI centroid.\n        Default is 40.\n    eps : float, optional\n        Small value added to avoid division by zero in phase correlation.\n        Default is 1e6.\n    winFunc : callable or str, optional\n        Window function to apply before computing phase correlation. If \"hamming\",\n        uses Hamming window. Otherwise should be a callable that takes an array\n        and returns a windowed array. Default is Hamming window.\n\n    Returns\n    -------\n    refStack : np.ndarray\n        Stack of cropped reference images, shape (num_rois, height, width).\n    maskStack : np.ndarray\n        Stack of cropped ROI masks, shape (num_rois, height, width).\n    pxcStack : np.ndarray\n        Stack of phase correlation maps, shape (num_rois, height, width).\n    phase_corr_values : np.ndarray\n        Phase correlation values at the center of each correlation map,\n        shape (num_rois,). This is the feature value used for red cell identification.\n\n    Notes\n    -----\n    The default parameters (width=40um, eps=1e6, and a Hamming window function)\n    were tested on a few sessions and are subjective. Manual curation and\n    parameter adjustment may be necessary for optimal results.\n    \"\"\"\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    if winFunc == \"hamming\":\n        winFunc = lambda x: np.hamming(x.shape[-1])\n    refStack = self.centered_reference_stack(plane_idx=plane_idx, width=width)  # get stack of reference image centered on each ROI\n    maskStack = self.centered_mask_stack(plane_idx=plane_idx, width=width)  # get stack of mask value centered on each ROI\n    window = winFunc(refStack)  # create a window function\n    pxcStack = np.stack(\n        [helpers.phaseCorrelation(ref, mask, eps=eps, window=window) for (ref, mask) in zip(refStack, maskStack)]\n    )  # measure phase correlation\n    pxcCenterPixel = int((pxcStack.shape[2] - 1) / 2)\n    return refStack, maskStack, pxcStack, pxcStack[:, pxcCenterPixel, pxcCenterPixel]\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.get_roi_centroid","title":"<code>get_roi_centroid(idx, mode='weightedmean')</code>","text":"<p>Get the centroid of an ROI.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the ROI.</p> required <code>mode</code> <code>str</code> <p>Method for computing centroid. \"weightedmean\" uses pixel weights (lam), \"median\" uses median pixel coordinates. Default is \"weightedmean\".</p> <code>'weightedmean'</code> <p>Returns:</p> Name Type Description <code>yc</code> <code>float</code> <p>Y-coordinate of the centroid in pixels.</p> <code>xc</code> <code>float</code> <p>X-coordinate of the centroid in pixels.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def get_roi_centroid(self, idx, mode=\"weightedmean\"):\n    \"\"\"\n    Get the centroid of an ROI.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the ROI.\n    mode : str, optional\n        Method for computing centroid. \"weightedmean\" uses pixel weights (lam),\n        \"median\" uses median pixel coordinates. Default is \"weightedmean\".\n\n    Returns\n    -------\n    yc : float\n        Y-coordinate of the centroid in pixels.\n    xc : float\n        X-coordinate of the centroid in pixels.\n    \"\"\"\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n\n    if mode == \"weightedmean\":\n        yc = np.sum(self.lam[idx] * self.ypix[idx]) / np.sum(self.lam[idx])\n        xc = np.sum(self.lam[idx] * self.xpix[idx]) / np.sum(self.lam[idx])\n    elif mode == \"median\":\n        yc = int(np.median(self.ypix[idx]))\n        xc = int(np.median(self.xpix[idx]))\n\n    return yc, xc\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.get_roi_in_plane_idx","title":"<code>get_roi_in_plane_idx(idx)</code>","text":"<p>Get the index of an ROI within its own plane.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global ROI index.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the ROI within its plane (0-indexed within that plane).</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def get_roi_in_plane_idx(self, idx):\n    \"\"\"\n    Get the index of an ROI within its own plane.\n\n    Parameters\n    ----------\n    idx : int\n        Global ROI index.\n\n    Returns\n    -------\n    int\n        Index of the ROI within its plane (0-indexed within that plane).\n    \"\"\"\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    # return index of ROI within it's own plane\n    plane_idx = self.roi_plane_idx[idx]\n    return idx - np.sum(self.roi_plane_idx &lt; plane_idx)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.get_roi_range","title":"<code>get_roi_range(idx)</code>","text":"<p>Get the range (peak-to-peak) of x and y pixels for an ROI.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the ROI.</p> required <p>Returns:</p> Name Type Description <code>yr</code> <code>int</code> <p>Range of y-pixels (peak-to-peak).</p> <code>xr</code> <code>int</code> <p>Range of x-pixels (peak-to-peak).</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def get_roi_range(self, idx):\n    \"\"\"\n    Get the range (peak-to-peak) of x and y pixels for an ROI.\n\n    Parameters\n    ----------\n    idx : int\n        Index of the ROI.\n\n    Returns\n    -------\n    yr : int\n        Range of y-pixels (peak-to-peak).\n    xr : int\n        Range of x-pixels (peak-to-peak).\n    \"\"\"\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    # get range of x and y pixels for a particular ROI\n    yr = np.ptp(self.ypix[idx])\n    xr = np.ptp(self.xpix[idx])\n    return yr, xr\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.getxref","title":"<code>getxref(xCenter)</code>","text":"<p>Get x-axis reference coordinates relative to a center point.</p> <p>Parameters:</p> Name Type Description Default <code>xCenter</code> <code>float</code> <p>X-coordinate of the center point in pixels.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>X-axis coordinates in micrometers relative to xCenter, shape (lx,).</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def getxref(self, xCenter):\n    \"\"\"\n    Get x-axis reference coordinates relative to a center point.\n\n    Parameters\n    ----------\n    xCenter : float\n        X-coordinate of the center point in pixels.\n\n    Returns\n    -------\n    np.ndarray\n        X-axis coordinates in micrometers relative to xCenter, shape (lx,).\n    \"\"\"\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    return self.um_per_pixel * (self.x_base_ref - xCenter)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.getyref","title":"<code>getyref(yCenter)</code>","text":"<p>Get y-axis reference coordinates relative to a center point.</p> <p>Parameters:</p> Name Type Description Default <code>yCenter</code> <code>float</code> <p>Y-coordinate of the center point in pixels.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Y-axis coordinates in micrometers relative to yCenter, shape (ly,).</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def getyref(self, yCenter):\n    \"\"\"\n    Get y-axis reference coordinates relative to a center point.\n\n    Parameters\n    ----------\n    yCenter : float\n        Y-coordinate of the center point in pixels.\n\n    Returns\n    -------\n    np.ndarray\n        Y-axis coordinates in micrometers relative to yCenter, shape (ly,).\n    \"\"\"\n    if not (self.data_loaded):\n        self.load_reference_and_masks()\n    return self.um_per_pixel * (self.y_base_ref - yCenter)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.load_reference_and_masks","title":"<code>load_reference_and_masks()</code>","text":"<p>Load reference images and ROI masks from suite2p outputs.</p> <p>Loads the mean image for channel 2 (red channel) for each plane, along with ROI mask data (lam, ypix, xpix) and ROI plane indices. Also loads suite2p red cell values and creates supporting variables for spatial measurements.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If reference images do not all have the same shape.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def load_reference_and_masks(self):\n    \"\"\"\n    Load reference images and ROI masks from suite2p outputs.\n\n    Loads the mean image for channel 2 (red channel) for each plane, along\n    with ROI mask data (lam, ypix, xpix) and ROI plane indices. Also loads\n    suite2p red cell values and creates supporting variables for spatial\n    measurements.\n\n    Raises\n    ------\n    AssertionError\n        If reference images do not all have the same shape.\n    \"\"\"\n    # load reference images\n    ops = self.b2session.load_s2p(\"ops\")\n    self.reference = [op[\"meanImg_chan2\"] for op in ops]\n    self.lx, self.ly = self.reference[0].shape\n    for ref in self.reference:\n        msg = \"reference images do not all have the same shape\"\n        assert (self.lx, self.ly) == ref.shape, msg\n\n    # load masks (lam=weight of each pixel, xpix &amp; ypix=index of each pixel in ROI mask)\n    stat = self.b2session.load_s2p(\"stat\")\n    self.lam = [s[\"lam\"] for s in stat]\n    self.ypix = [s[\"ypix\"] for s in stat]\n    self.xpix = [s[\"xpix\"] for s in stat]\n    self.roi_plane_idx = self.b2session.loadone(\"mpciROIs.stackPosition\")[:, 2]\n\n    # load S2P red cell value\n    self.red_s2p = self.b2session.loadone(\"mpciROIs.redS2P\")  # (preloaded, will never change in this function)\n\n    # create supporting variables for mapping locations and axes\n    self.y_base_ref = np.arange(self.ly)\n    self.x_base_ref = np.arange(self.lx)\n    self.y_dist_ref = self.create_centered_axis(self.ly, self.um_per_pixel)\n    self.x_dist_ref = self.create_centered_axis(self.lx, self.um_per_pixel)\n\n    # update data_loaded field\n    self.data_loaded = True\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.one_name_feature_cutoffs","title":"<code>one_name_feature_cutoffs(name)</code>","text":"<p>Generate oneData name for feature cutoff parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Feature name (e.g., \"S2P\", \"dotProduct\", \"pearson\", \"phaseCorrelation\").</p> required <p>Returns:</p> Type Description <code>str</code> <p>OneData name for the feature cutoff parameter, formatted as \"parametersRed{Name}.minMaxCutoff\" where {Name} is the capitalized feature name.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def one_name_feature_cutoffs(self, name):\n    \"\"\"\n    Generate oneData name for feature cutoff parameters.\n\n    Parameters\n    ----------\n    name : str\n        Feature name (e.g., \"S2P\", \"dotProduct\", \"pearson\", \"phaseCorrelation\").\n\n    Returns\n    -------\n    str\n        OneData name for the feature cutoff parameter, formatted as\n        \"parametersRed{Name}.minMaxCutoff\" where {Name} is the capitalized\n        feature name.\n    \"\"\"\n    return \"parameters\" + \"Red\" + name[0].upper() + name[1:] + \".minMaxCutoff\"\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.update_from_session","title":"<code>update_from_session(red_cell, force_update=False)</code>","text":"<p>Update red cell cutoffs from another session.</p> <p>Copies red cell cutoff parameters from another RedCellProcessing object and applies them to this session.</p> <p>Parameters:</p> Name Type Description Default <code>red_cell</code> <code>RedCellProcessing</code> <p>Another RedCellProcessing object to copy cutoffs from.</p> required <code>force_update</code> <code>bool</code> <p>If False, only allows copying from sessions with the same mouse name. If True, allows copying from any session. Default is False.</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If red_cell is not a RedCellProcessing object, or if force_update is False and the mouse names don't match.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def update_from_session(self, red_cell, force_update=False):\n    \"\"\"\n    Update red cell cutoffs from another session.\n\n    Copies red cell cutoff parameters from another RedCellProcessing object\n    and applies them to this session.\n\n    Parameters\n    ----------\n    red_cell : RedCellProcessing\n        Another RedCellProcessing object to copy cutoffs from.\n    force_update : bool, optional\n        If False, only allows copying from sessions with the same mouse name.\n        If True, allows copying from any session. Default is False.\n\n    Raises\n    ------\n    AssertionError\n        If red_cell is not a RedCellProcessing object, or if force_update is\n        False and the mouse names don't match.\n    \"\"\"\n    assert isinstance(red_cell, RedCellProcessing), \"red_cell is not a RedCellProcessing object\"\n    if not (force_update):\n        assert (\n            red_cell.b2session.mouse_name == self.b2session.mouse_name\n        ), \"session to copy from is from a different mouse, this isn't allowed without the force_update=True input\"\n    cutoffs = [red_cell.b2session.loadone(red_cell.one_name_feature_cutoffs(name)) for name in self.feature_names]\n    self.update_red_idx(s2p_cutoff=cutoffs[0], dot_product_cutoff=cutoffs[1], corr_coef_cutoff=cutoffs[2], phase_corr_cutoff=cutoffs[3])\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.RedCellProcessing.update_red_idx","title":"<code>update_red_idx(s2p_cutoff=None, dot_product_cutoff=None, corr_coef_cutoff=None, phase_corr_cutoff=None)</code>","text":"<p>Update red cell index based on feature cutoff values.</p> <p>Updates the red cell index by applying minimum and maximum cutoffs to each feature (S2P, dot product, Pearson correlation, phase correlation). Only features with non-NaN cutoff values are applied. The red cell index is updated to include only ROIs that meet all specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>s2p_cutoff</code> <code>array-like of float, length 2</code> <p>[min, max] cutoff values for suite2p red cell feature. NaN values indicate the cutoff should not be applied. Default is None.</p> <code>None</code> <code>dot_product_cutoff</code> <code>array-like of float, length 2</code> <p>[min, max] cutoff values for dot product feature. Default is None.</p> <code>None</code> <code>corr_coef_cutoff</code> <code>array-like of float, length 2</code> <p>[min, max] cutoff values for Pearson correlation feature. Default is None.</p> <code>None</code> <code>phase_corr_cutoff</code> <code>array-like of float, length 2</code> <p>[min, max] cutoff values for phase correlation feature. Default is None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any cutoff is not a numpy array or list, or if any cutoff does not have exactly 2 elements.</p> Notes <p>Cutoff values are saved to oneData for future reference. The red cell index is updated in place and saved to oneData.</p> Source code in <code>vrAnalysis/registration/redcell.py</code> <pre><code>def update_red_idx(self, s2p_cutoff=None, dot_product_cutoff=None, corr_coef_cutoff=None, phase_corr_cutoff=None):\n    \"\"\"\n    Update red cell index based on feature cutoff values.\n\n    Updates the red cell index by applying minimum and maximum cutoffs to\n    each feature (S2P, dot product, Pearson correlation, phase correlation).\n    Only features with non-NaN cutoff values are applied. The red cell index\n    is updated to include only ROIs that meet all specified criteria.\n\n    Parameters\n    ----------\n    s2p_cutoff : array-like of float, length 2, optional\n        [min, max] cutoff values for suite2p red cell feature. NaN values\n        indicate the cutoff should not be applied. Default is None.\n    dot_product_cutoff : array-like of float, length 2, optional\n        [min, max] cutoff values for dot product feature. Default is None.\n    corr_coef_cutoff : array-like of float, length 2, optional\n        [min, max] cutoff values for Pearson correlation feature.\n        Default is None.\n    phase_corr_cutoff : array-like of float, length 2, optional\n        [min, max] cutoff values for phase correlation feature.\n        Default is None.\n\n    Raises\n    ------\n    ValueError\n        If any cutoff is not a numpy array or list, or if any cutoff does\n        not have exactly 2 elements.\n\n    Notes\n    -----\n    Cutoff values are saved to oneData for future reference. The red cell\n    index is updated in place and saved to oneData.\n    \"\"\"\n    # create initial all true red cell idx\n    red_cell_idx = np.full(self.b2session.loadone(\"mpciROIs.redCellIdx\").shape, True)\n\n    # load feature values for each ROI\n    red_s2p = self.b2session.loadone(\"mpciROIs.redS2P\")\n    dot_product = self.b2session.loadone(\"mpciROIs.redDotProduct\")\n    corr_coef = self.b2session.loadone(\"mpciROIs.redPearson\")\n    phase_corr = self.b2session.loadone(\"mpciROIs.redPhaseCorrelation\")\n\n    # create lists for zipping through each feature/cutoff combination\n    features = [red_s2p, dot_product, corr_coef, phase_corr]\n    cutoffs = [s2p_cutoff, dot_product_cutoff, corr_coef_cutoff, phase_corr_cutoff]\n    usecutoff = [[False, False] for _ in range(len(cutoffs))]\n\n    # check validity of each cutoff and identify whether it should be used\n    for name, use, cutoff in zip(self.feature_names, usecutoff, cutoffs):\n        if not isinstance(cutoff, np.ndarray) and not isinstance(cutoff, list):\n            raise ValueError(f\"Expecting a numpy array or a list for {name} cutoff, got {type(cutoff)}\")\n        assert len(cutoff) == 2, f\"{name} cutoff does not have 2 elements\"\n        if not (np.isnan(cutoff[0])):\n            use[0] = True\n        if not (np.isnan(cutoff[1])):\n            use[1] = True\n\n    # add feature cutoffs to redCellIdx (sets any to False that don't meet the cutoff)\n    for feature, use, cutoff in zip(features, usecutoff, cutoffs):\n        if use[0]:\n            red_cell_idx &amp;= feature &gt;= cutoff[0]\n        if use[1]:\n            red_cell_idx &amp;= feature &lt;= cutoff[1]\n\n    # save new red cell index to one data\n    self.b2session.saveone(red_cell_idx, \"mpciROIs.redCellIdx\")\n\n    # save feature cutoffs to one data\n    for idx, name in enumerate(self.feature_names):\n        self.b2session.saveone(cutoffs[idx], self.one_name_feature_cutoffs(name))\n    print(f\"Red Cell curation choices are saved for session {self.b2session.session_print()}\")\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.behavior","title":"<code>behavior</code>","text":"<p>Behavior processing functions for B2Registration.</p> <p>This module contains functions for processing behavioral data from different versions of the vrControl software. Each function processes behavior data to achieve the same results structure regardless of the data collection method.</p>"},{"location":"api/registration/#vrAnalysis.registration.behavior.BEHAVIOR_PROCESSING","title":"<code>BEHAVIOR_PROCESSING = {1: standard_behavior, 2: cr_hippocannula_behavior}</code>  <code>module-attribute</code>","text":"<p>Dictionary of behavior processing functions.</p> <p>These reflect the different versions of the vrControl software that was used to collect the behavior data. Because the behavioral data was collected in different ways, we need to process it differently to achieve the same results structure.</p> <p>Keys:</p> <ul> <li>1: Standard behavior processing function.</li> <li>2: CR hippocannula behavior processing function.</li> </ul>"},{"location":"api/registration/#vrAnalysis.registration.behavior.cr_hippocannula_behavior","title":"<code>cr_hippocannula_behavior(b2registration)</code>","text":"<p>Process behavior data from CR hippocannula version of vrControl.</p> <p>Extracts behavioral data from TRIAL and EXP structures, processes timestamps, positions, rewards, and licks, and saves them to oneData format. Aligns behavioral timestamps to the timeline using photodiode flips.</p> <p>Parameters:</p> Name Type Description Default <code>b2registration</code> <code>B2Registration</code> <p>The B2Registration object containing the session data to process.</p> required <p>Returns:</p> Type Description <code>B2Registration</code> <p>The B2Registration object with behavior data processed and saved.</p> Notes <p>This function processes behavior data from the CR hippocannula version of vrControl. The data structure differs from the standard version, requiring different field names and processing steps.</p> Source code in <code>vrAnalysis/registration/behavior.py</code> <pre><code>def cr_hippocannula_behavior(b2registration: \"B2Registration\") -&gt; \"B2Registration\":\n    \"\"\"\n    Process behavior data from CR hippocannula version of vrControl.\n\n    Extracts behavioral data from TRIAL and EXP structures, processes\n    timestamps, positions, rewards, and licks, and saves them to oneData format.\n    Aligns behavioral timestamps to the timeline using photodiode flips.\n\n    Parameters\n    ----------\n    b2registration : B2Registration\n        The B2Registration object containing the session data to process.\n\n    Returns\n    -------\n    B2Registration\n        The B2Registration object with behavior data processed and saved.\n\n    Notes\n    -----\n    This function processes behavior data from the CR hippocannula version of\n    vrControl. The data structure differs from the standard version, requiring\n    different field names and processing steps.\n    \"\"\"\n    trialInfo = b2registration.vr_file[\"TRIAL\"]\n    expInfo = b2registration.vr_file[\"EXP\"]\n\n    numTrials = trialInfo.info.no\n    nonNanSamples = np.sum(~np.isnan(trialInfo.time[:, 0]))\n    assert numTrials == nonNanSamples, f\"# trials {trialInfo.info.no} isn't equal to non-nan first time samples {nonNanSamples}\"\n    b2registration.set_value(\"numTrials\", numTrials)\n\n    # trialInfo contains sparse matrices of size (maxTrials, maxSamples), where numTrials&lt;maxTrials and numSamples&lt;maxSamples\n    nzindex = b2registration.create_index(b2registration.convert_dense(trialInfo.time))\n    timeStamps = b2registration.get_vr_data(b2registration.convert_dense(trialInfo.time), nzindex)\n    roomPosition = b2registration.get_vr_data(b2registration.convert_dense(trialInfo.roomPosition), nzindex)\n\n    # oneData with behave prefix is a (numBehavioralSamples, ) shaped array conveying information about the state of VR\n    numTimeStamps = np.array([len(t) for t in timeStamps])  # list of number of behavioral timestamps in each trial\n    behaveTimeStamps = np.concatenate(timeStamps)  # time stamp associated with each behavioral sample\n    behavePosition = np.concatenate(roomPosition)  # virtual position associated with each behavioral sample\n    b2registration.set_value(\"numBehaveTimestamps\", len(behaveTimeStamps))\n\n    # Check shapes and sizes\n    assert behaveTimeStamps.ndim == 1, \"behaveTimeStamps is not a 1-d array!\"\n    assert behaveTimeStamps.shape == behavePosition.shape, \"behave oneData arrays do not have the same shape!\"\n\n    # oneData with trial prefix is a (numTrials,) shaped array conveying information about the state on each trial\n    trialStartFrame = np.array([0, *np.cumsum(numTimeStamps)[:-1]]).astype(np.int64)\n    trialEnvironmentIndex = (\n        b2registration.convert_dense(trialInfo.vrEnvIdx).astype(np.int16)\n        if \"vrEnvIdx\" in trialInfo._fieldnames\n        else -1 * np.ones(b2registration.get_value(\"numTrials\"), dtype=np.int16)\n    )\n    trialRoomLength = np.ones(b2registration.get_value(\"numTrials\")) * expInfo.roomLength\n    trialMovementGain = np.ones(b2registration.get_value(\"numTrials\"))  # mvmt gain always one\n    trialRewardPosition = b2registration.convert_dense(trialInfo.trialRewPos)\n    trialRewardTolerance = b2registration.convert_dense(expInfo.rewPosTolerance * np.ones(b2registration.get_value(\"numTrials\")))\n    trialRewardAvailability = b2registration.convert_dense(trialInfo.trialRewAvailable).astype(np.bool_)\n    rewardDelivery = b2registration.convert_dense(trialInfo.trialRewDelivery)\n    rewardDelivery[np.isnan(rewardDelivery)] = 0  # about to be (-1), indicating no reward delivered\n    rewardDelivery = rewardDelivery.astype(np.int64) - 1  # get reward delivery frame (frame within trial) first (will be -1 if no reward delivered)\n\n    # adjust frame count to behave arrays\n    trialRewardDelivery = np.array(\n        [\n            (rewardDelivery + np.sum(numTimeStamps[:trialIdx]) if rewardDelivery &gt;= 0 else rewardDelivery)\n            for (trialIdx, rewardDelivery) in enumerate(rewardDelivery)\n        ]\n    )\n    trialActiveLicking = b2registration.convert_dense(trialInfo.trialActiveLicking).astype(np.bool_)\n    trialActiveStopping = b2registration.convert_dense(trialInfo.trialActiveStopping).astype(np.bool_)\n\n    # Check shapes and sizes\n    assert trialEnvironmentIndex.ndim == 1 and len(trialEnvironmentIndex) == b2registration.get_value(\n        \"numTrials\"\n    ), \"trialEnvironmentIndex is not a (numTrials,) shaped array!\"\n    assert (\n        trialStartFrame.shape\n        == trialEnvironmentIndex.shape\n        == trialRoomLength.shape\n        == trialMovementGain.shape\n        == trialRewardPosition.shape\n        == trialRewardTolerance.shape\n        == trialRewardAvailability.shape\n        == trialRewardDelivery.shape\n        == trialActiveLicking.shape\n        == trialActiveStopping.shape\n    ), \"trial oneData arrays do not have the same shape!\"\n\n    # oneData with lick prefix is a (numLicks,) shaped array containing information about each lick during VR behavior\n    licks = [vrd.astype(np.int16) for vrd in b2registration.get_vr_data(b2registration.convert_dense(trialInfo.lick), nzindex)]\n    lickFrames = [np.nonzero(licks)[0] for licks in licks]\n    lickCounts = np.concatenate([licks[lickFrames] for (licks, lickFrames) in zip(licks, lickFrames)])\n    lickTrials = np.concatenate([tidx * np.ones_like(lickFrames) for (tidx, lickFrames) in enumerate(lickFrames)])\n    lickFrames = np.concatenate(lickFrames)\n    if np.sum(lickCounts) &gt; 0:\n        lickFramesRepeat = np.concatenate([lf * np.ones(lc, dtype=np.uint8) for (lf, lc) in zip(lickFrames, lickCounts)])\n        lickTrialsRepeat = np.concatenate([lt * np.ones(lc, dtype=np.uint8) for (lt, lc) in zip(lickTrials, lickCounts)])\n        lickCountsRepeat = np.concatenate([lc * np.ones(lc, dtype=np.uint8) for (lc, lc) in zip(lickCounts, lickCounts)])\n        lickBehaveSample = lickFramesRepeat + np.array([np.sum(numTimeStamps[:trialIdx]) for trialIdx in lickTrialsRepeat])\n\n        assert len(lickBehaveSample) == np.sum(\n            lickCounts\n        ), \"the number of licks counted by vrBehavior is not equal to the length of the lickBehaveSample vector!\"\n        assert lickBehaveSample.ndim == 1, \"lickBehaveIndex is not a 1-d array!\"\n        assert (\n            0 &lt;= np.max(lickBehaveSample) &lt;= len(behaveTimeStamps)\n        ), \"lickBehaveSample contains index outside range of possible indices for behaveTimeStamps\"\n    else:\n        # No licks found -- create empty array\n        lickBehaveSample = np.array([], dtype=np.uint8)\n\n    # Align behavioral timestamp data to timeline -- shift each trials timestamps so that they start at the time of the first photodiode flip (which is reliably detected)\n    trialStartOffsets = behaveTimeStamps[trialStartFrame] - b2registration.loadone(\"trials.startTimes\")  # get offset\n    behaveTimeStamps = np.concatenate(\n        [bts - trialStartOffsets[tidx] for (tidx, bts) in enumerate(b2registration.group_behave_by_trial(behaveTimeStamps, trialStartFrame))]\n    )\n\n    # Save behave onedata\n    b2registration.saveone(behaveTimeStamps, \"positionTracking.times\")\n    b2registration.saveone(behavePosition, \"positionTracking.position\")\n\n    # Save trial onedata\n    b2registration.saveone(trialStartFrame, \"trials.positionTracking\")\n    b2registration.saveone(trialEnvironmentIndex, \"trials.environmentIndex\")\n    b2registration.saveone(trialRoomLength, \"trials.roomlength\")\n    b2registration.saveone(trialMovementGain, \"trials.movementGain\")\n    b2registration.saveone(trialRewardPosition, \"trials.rewardPosition\")\n    b2registration.saveone(trialRewardTolerance, \"trials.rewardZoneHalfwidth\")\n    b2registration.saveone(trialRewardAvailability, \"trials.rewardAvailability\")\n    b2registration.saveone(trialRewardDelivery, \"trials.rewardPositionTracking\")\n    b2registration.saveone(trialActiveLicking, \"trials.activeLicking\")\n    b2registration.saveone(trialActiveStopping, \"trials.activeStopping\")\n\n    # Save lick onedata\n    b2registration.saveone(lickBehaveSample, \"licksTracking.positionTracking\")\n\n    return b2registration\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.behavior.register_behavior","title":"<code>register_behavior(b2registration, behavior_type)</code>","text":"<p>Register behavior for a given behavior type.</p> <p>This is a dispatcher function that calls the appropriate behavior processing function based on the behavior type.</p> <p>Parameters:</p> Name Type Description Default <code>b2registration</code> <code>B2Registration</code> <p>The B2Registration object containing the session data to process.</p> required <code>behavior_type</code> <code>int</code> <p>The behavior type to register. Must be a key in BEHAVIOR_PROCESSING.</p> required <p>Returns:</p> Type Description <code>B2Registration</code> <p>The B2Registration object with behavior registered.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If behavior_type is not supported.</p> See Also <p>BEHAVIOR_PROCESSING : Dictionary mapping behavior types to processing functions.</p> Source code in <code>vrAnalysis/registration/behavior.py</code> <pre><code>def register_behavior(b2registration: \"B2Registration\", behavior_type: int) -&gt; \"B2Registration\":\n    \"\"\"\n    Register behavior for a given behavior type.\n\n    This is a dispatcher function that calls the appropriate behavior processing\n    function based on the behavior type.\n\n    Parameters\n    ----------\n    b2registration : B2Registration\n        The B2Registration object containing the session data to process.\n    behavior_type : int\n        The behavior type to register. Must be a key in BEHAVIOR_PROCESSING.\n\n    Returns\n    -------\n    B2Registration\n        The B2Registration object with behavior registered.\n\n    Raises\n    ------\n    ValueError\n        If behavior_type is not supported.\n\n    See Also\n    --------\n    BEHAVIOR_PROCESSING : Dictionary mapping behavior types to processing functions.\n    \"\"\"\n    if behavior_type not in BEHAVIOR_PROCESSING.keys():\n        raise ValueError(f\"Behavior type {behavior_type} not supported. Supported types are: {list(BEHAVIOR_PROCESSING.keys())}.\")\n    return BEHAVIOR_PROCESSING[behavior_type](b2registration)\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.behavior.standard_behavior","title":"<code>standard_behavior(b2registration)</code>","text":"<p>Process standard behavior data from vrControl.</p> <p>Extracts behavioral data from trialInfo and expInfo structures, processes timestamps, positions, rewards, and licks, and saves them to oneData format. Aligns behavioral timestamps to the timeline using photodiode flips.</p> <p>Parameters:</p> Name Type Description Default <code>b2registration</code> <code>B2Registration</code> <p>The B2Registration object containing the session data to process.</p> required <p>Returns:</p> Type Description <code>B2Registration</code> <p>The B2Registration object with behavior data processed and saved.</p> Notes <p>This function processes behavior data from the standard vrControl format. It extracts trial-level and sample-level behavioral data and aligns timestamps to the imaging timeline.</p> Source code in <code>vrAnalysis/registration/behavior.py</code> <pre><code>def standard_behavior(b2registration: \"B2Registration\") -&gt; \"B2Registration\":\n    \"\"\"\n    Process standard behavior data from vrControl.\n\n    Extracts behavioral data from trialInfo and expInfo structures, processes\n    timestamps, positions, rewards, and licks, and saves them to oneData format.\n    Aligns behavioral timestamps to the timeline using photodiode flips.\n\n    Parameters\n    ----------\n    b2registration : B2Registration\n        The B2Registration object containing the session data to process.\n\n    Returns\n    -------\n    B2Registration\n        The B2Registration object with behavior data processed and saved.\n\n    Notes\n    -----\n    This function processes behavior data from the standard vrControl format.\n    It extracts trial-level and sample-level behavioral data and aligns timestamps\n    to the imaging timeline.\n    \"\"\"\n    expInfo = b2registration.vr_file[\"expInfo\"]\n    trialInfo = b2registration.vr_file[\"trialInfo\"]\n    num_values_per_trial = np.diff(trialInfo.time.tocsr().indptr)\n    valid_trials = np.where(num_values_per_trial &gt; 0)[0]\n    numTrials = len(valid_trials)\n    assert np.array_equal(valid_trials, np.arange(numTrials)), \"valid_trials is not a range from 0 to numTrials\"\n    b2registration.set_value(\"numTrials\", numTrials)\n\n    # trialInfo contains sparse matrices of size (maxTrials, maxSamples), where numTrials&lt;maxTrials and numSamples&lt;maxSamples\n    nzindex = b2registration.create_index(b2registration.convert_dense(trialInfo.time))\n    timeStamps = b2registration.get_vr_data(b2registration.convert_dense(trialInfo.time), nzindex)\n    roomPosition = b2registration.get_vr_data(b2registration.convert_dense(trialInfo.roomPosition), nzindex)\n\n    # oneData with behave prefix is a (numBehavioralSamples, ) shaped array conveying information about the state of VR\n    numTimeStamps = np.array([len(t) for t in timeStamps])  # list of number of behavioral timestamps in each trial\n    behaveTimeStamps = np.concatenate(timeStamps)  # time stamp associated with each behavioral sample\n    behavePosition = np.concatenate(roomPosition)  # virtual position associated with each behavioral sample\n    b2registration.set_value(\"numBehaveTimestamps\", len(behaveTimeStamps))\n\n    # Check shapes and sizes\n    assert behaveTimeStamps.ndim == 1, \"behaveTimeStamps is not a 1-d array!\"\n    assert behaveTimeStamps.shape == behavePosition.shape, \"behave oneData arrays do not have the same shape!\"\n\n    # oneData with trial prefix is a (numTrials,) shaped array conveying information about the state on each trial\n    trialStartFrame = np.array([0, *np.cumsum(numTimeStamps)[:-1]]).astype(np.int64)\n    trialEnvironmentIndex = (\n        b2registration.convert_dense(trialInfo.vrEnvIdx).astype(np.int16)\n        if \"vrEnvIdx\" in trialInfo._fieldnames\n        else -1 * np.ones(b2registration.get_value(\"numTrials\"), dtype=np.int16)\n    )\n    trialRoomLength = expInfo.roomLength[: b2registration.get_value(\"numTrials\")]\n    trialMovementGain = expInfo.mvmtGain[: b2registration.get_value(\"numTrials\")]\n    trialRewardPosition = b2registration.convert_dense(trialInfo.rewardPosition)\n    trialRewardTolerance = b2registration.convert_dense(trialInfo.rewardTolerance)\n    trialRewardAvailability = b2registration.convert_dense(trialInfo.rewardAvailable).astype(np.bool_)\n    rewardDelivery = (\n        b2registration.convert_dense(trialInfo.rewardDeliveryFrame).astype(np.int64) - 1\n    )  # get reward delivery frame (frame within trial) first (will be -1 if no reward delivered)\n    # adjust frame count to behave arrays\n    trialRewardDelivery = np.array(\n        [\n            (rewardDelivery + np.sum(numTimeStamps[:trialIdx]) if rewardDelivery &gt;= 0 else rewardDelivery)\n            for (trialIdx, rewardDelivery) in enumerate(rewardDelivery)\n        ]\n    )\n    trialActiveLicking = b2registration.convert_dense(trialInfo.activeLicking).astype(np.bool_)\n    trialActiveStopping = b2registration.convert_dense(trialInfo.activeStopping).astype(np.bool_)\n\n    # Check shapes and sizes\n    assert trialEnvironmentIndex.ndim == 1 and len(trialEnvironmentIndex) == b2registration.get_value(\n        \"numTrials\"\n    ), \"trialEnvironmentIndex is not a (numTrials,) shaped array!\"\n    assert (\n        trialStartFrame.shape\n        == trialEnvironmentIndex.shape\n        == trialRoomLength.shape\n        == trialMovementGain.shape\n        == trialRewardPosition.shape\n        == trialRewardTolerance.shape\n        == trialRewardAvailability.shape\n        == trialRewardDelivery.shape\n        == trialActiveLicking.shape\n        == trialActiveStopping.shape\n    ), \"trial oneData arrays do not have the same shape!\"\n\n    # oneData with lick prefix is a (numLicks,) shaped array containing information about each lick during VR behavior\n    licks = b2registration.get_vr_data(b2registration.convert_dense(trialInfo.lick), nzindex)\n    lickFrames = [np.nonzero(licks)[0] for licks in licks]\n    lickCounts = np.concatenate([licks[lickFrames] for (licks, lickFrames) in zip(licks, lickFrames)])\n    lickTrials = np.concatenate([tidx * np.ones_like(lickFrames) for (tidx, lickFrames) in enumerate(lickFrames)])\n    lickFrames = np.concatenate(lickFrames)\n    if np.sum(lickCounts) &gt; 0:\n        lickFramesRepeat = np.concatenate([lf * np.ones(lc, dtype=np.uint8) for (lf, lc) in zip(lickFrames, lickCounts)])\n        lickTrialsRepeat = np.concatenate([lt * np.ones(lc, dtype=np.uint8) for (lt, lc) in zip(lickTrials, lickCounts)])\n        lickCountsRepeat = np.concatenate([lc * np.ones(lc, dtype=np.uint8) for (lc, lc) in zip(lickCounts, lickCounts)])\n        lickBehaveSample = lickFramesRepeat + np.array([np.sum(numTimeStamps[:trialIdx]) for trialIdx in lickTrialsRepeat])\n\n        assert len(lickBehaveSample) == np.sum(\n            lickCounts\n        ), \"the number of licks counted by vrBehavior is not equal to the length of the lickBehaveSample vector!\"\n        assert lickBehaveSample.ndim == 1, \"lickBehaveIndex is not a 1-d array!\"\n        assert (\n            0 &lt;= np.max(lickBehaveSample) &lt;= len(behaveTimeStamps)\n        ), \"lickBehaveSample contains index outside range of possible indices for behaveTimeStamps\"\n    else:\n        # No licks found -- create empty array\n        lickBehaveSample = np.array([], dtype=np.uint8)\n\n    # Align behavioral timestamp data to timeline -- shift each trials timestamps so that they start at the time of the first photodiode flip (which is reliably detected)\n    trialStartOffsets = behaveTimeStamps[trialStartFrame] - b2registration.loadone(\"trials.startTimes\")  # get offset\n    behaveTimeStamps = np.concatenate(\n        [bts - trialStartOffsets[tidx] for (tidx, bts) in enumerate(b2registration.group_behave_by_trial(behaveTimeStamps, trialStartFrame))]\n    )\n\n    # Save behave onedata\n    b2registration.saveone(behaveTimeStamps, \"positionTracking.times\")\n    b2registration.saveone(behavePosition, \"positionTracking.position\")\n\n    # Save trial onedata\n    b2registration.saveone(trialStartFrame, \"trials.positionTracking\")\n    b2registration.saveone(trialEnvironmentIndex, \"trials.environmentIndex\")\n    b2registration.saveone(trialRoomLength, \"trials.roomlength\")\n    b2registration.saveone(trialMovementGain, \"trials.movementGain\")\n    b2registration.saveone(trialRewardPosition, \"trials.rewardPosition\")\n    b2registration.saveone(trialRewardTolerance, \"trials.rewardZoneHalfwidth\")\n    b2registration.saveone(trialRewardAvailability, \"trials.rewardAvailability\")\n    b2registration.saveone(trialRewardDelivery, \"trials.rewardPositionTracking\")\n    b2registration.saveone(trialActiveLicking, \"trials.activeLicking\")\n    b2registration.saveone(trialActiveStopping, \"trials.activeStopping\")\n\n    # Save lick onedata\n    b2registration.saveone(lickBehaveSample, \"licksTracking.positionTracking\")\n\n    return b2registration\n</code></pre>"},{"location":"api/registration/#vrAnalysis.registration.oasis","title":"<code>oasis</code>","text":""},{"location":"api/registration/#vrAnalysis.registration.oasis.oasis_deconvolution","title":"<code>oasis_deconvolution(fcorr, g, num_processes=cpu_count() - 1)</code>","text":"<p>Perform oasis deconvolution on a batch of fluorescence traces.</p> <p>Processes fluorescence traces in parallel using multiple processes to compute deconvolved spike estimates using the OASIS algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>fcorr</code> <code>ndarray</code> <p>The fluorescence traces to process, shape (num_rois, num_frames).</p> required <code>g</code> <code>float</code> <p>The g parameter for oasis deconvolution (decay constant).</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing. Default is cpu_count() - 1.</p> <code>cpu_count() - 1</code> <p>Returns:</p> Type Description <code>list of np.ndarray</code> <p>List of deconvolved traces, one per ROI. Each trace has negative values clipped to zero.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If fcorr is not a 2D array or if num_processes is less than 1.</p> <code>ImportError</code> <p>If the oasis package cannot be imported.</p> Source code in <code>vrAnalysis/registration/oasis.py</code> <pre><code>def oasis_deconvolution(fcorr: np.ndarray, g: float, num_processes: int = cpu_count() - 1) -&gt; list[np.ndarray]:\n    \"\"\"\n    Perform oasis deconvolution on a batch of fluorescence traces.\n\n    Processes fluorescence traces in parallel using multiple processes to\n    compute deconvolved spike estimates using the OASIS algorithm.\n\n    Parameters\n    ----------\n    fcorr : np.ndarray\n        The fluorescence traces to process, shape (num_rois, num_frames).\n    g : float\n        The g parameter for oasis deconvolution (decay constant).\n    num_processes : int, optional\n        The number of processes to use for parallel processing.\n        Default is cpu_count() - 1.\n\n    Returns\n    -------\n    list of np.ndarray\n        List of deconvolved traces, one per ROI. Each trace has negative\n        values clipped to zero.\n\n    Raises\n    ------\n    ValueError\n        If fcorr is not a 2D array or if num_processes is less than 1.\n    ImportError\n        If the oasis package cannot be imported.\n    \"\"\"\n    if fcorr.ndim != 2:\n        raise ValueError(\"fcorr must be a 2D numpy array.\")\n    if num_processes &lt; 1:\n        raise ValueError(\"num_processes must be at least 1.\")\n\n    # Lazy import of deconvolve method from oasis to not break registration\n    # if oasis_deconvolution isn't used.\n    try:\n        from oasis.functions import deconvolve\n    except ImportError as error:\n        print(\"Failed to import deconvolve from oasis.\")\n        raise error\n\n    # Create partial function with fixed parameters\n    process_func = partial(_process_fc, g=g, deconvolve=deconvolve)\n\n    with Pool(num_processes) as pool:\n        results = tqdm(pool.imap(process_func, fcorr), total=len(fcorr))\n        return list(results)\n</code></pre>"},{"location":"api/sessions/","title":"Sessions API Reference","text":"<p>The <code>vrAnalysis.sessions</code> module provides classes for loading and managing VR session data. It's the base class for all session data objects. It provides critical functionality for managing session data. In addition, it contains the API for handling onedata files, which are the main way that session data is stored and loaded.</p>"},{"location":"api/sessions/#main-classes-and-functions","title":"Main Classes and Functions","text":""},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData","title":"<code>SessionData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for session data.</p> <p>This abstract base class provides the core functionality for managing VR session data, including loading and saving onedata files, managing session identifiers, and providing a flexible values namespace for storing arbitrary data.</p> <p>Attributes:</p> Name Type Description <code>mouse_name</code> <code>str</code> <p>Name of the mouse for this session.</p> <code>date</code> <code>str, datetime, or PrettyDatetime</code> <p>Date of the session. Will be converted to string format.</p> <code>session_id</code> <code>(str, optional)</code> <p>Optional session identifier. Default is None.</p> <code>data_path</code> <code>Path or str</code> <p>Path to the session data directory. Set automatically during initialization.</p> <code>values</code> <code>SimpleNamespace</code> <p>Namespace for storing arbitrary data. Useful for flexible storing of small bytes data.</p> <code>one_cache</code> <code>dict</code> <p>Cache for buffering onedata files to avoid repeated disk reads.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>@dataclass\nclass SessionData(ABC):\n    \"\"\"\n    Base class for session data.\n\n    This abstract base class provides the core functionality for managing VR\n    session data, including loading and saving onedata files, managing session\n    identifiers, and providing a flexible values namespace for storing\n    arbitrary data.\n\n    Attributes\n    ----------\n    mouse_name : str\n        Name of the mouse for this session.\n    date : str, datetime, or PrettyDatetime\n        Date of the session. Will be converted to string format.\n    session_id : str, optional\n        Optional session identifier. Default is None.\n    data_path : Path or str\n        Path to the session data directory. Set automatically during initialization.\n    values : SimpleNamespace\n        Namespace for storing arbitrary data. Useful for flexible storing of\n        small bytes data.\n    one_cache : dict\n        Cache for buffering onedata files to avoid repeated disk reads.\n    \"\"\"\n\n    # session identifiers\n    mouse_name: str\n    date: Union[str, datetime, PrettyDatetime]\n    session_id: Optional[str] = None\n\n    # session data path -- identifies where the session data is stored and loaded from\n    data_path: Union[Path, str] = field(init=False, repr=False)\n\n    # Values: namespace for storing arbitrary data -- useful for flexible storing of (small bytes) data\n    values: SimpleNamespace = field(default_factory=SimpleNamespace, repr=False, init=False)\n\n    # A cache for buffering onedata files\n    one_cache: dict = field(default_factory=dict, repr=False, init=False)\n\n    def __post_init__(self):\n        \"\"\"\n        Post-initialization method to set all properties specific to subclasses.\n\n        This method is called automatically after dataclass initialization.\n        It formats the date, initializes the data path, and calls additional\n        loading steps for subclasses.\n        \"\"\"\n        # Enable additional loading for subclasses if required\n        # usually to put things in the values namespace\n        self.date = str(PrettyDatetime.make_pretty(self.date))\n        self.data_path = self._init_data_path()\n        self._additional_loading()\n\n    def _additional_loading(self) -&gt; None:\n        \"\"\"\n        Additional loading for subclasses.\n\n        Called in the __post_init__ method. Allows subclasses to perform\n        additional loading steps. Default is to do nothing.\n\n        Notes\n        -----\n        Subclasses should override this method to perform any additional\n        initialization steps, such as loading configuration files or setting\n        up specialized data structures.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _init_data_path(self) -&gt; Union[Path, str]:\n        \"\"\"\n        Define the data path for the session data.\n\n        This is required for all subclasses of SessionData to define.\n\n        Returns\n        -------\n        Path or str\n            Path to the session data directory.\n\n        Notes\n        -----\n        This method must be implemented by all subclasses. It should return\n        the path where session data is stored and loaded from.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def spks(self) -&gt; np.ndarray:\n        \"\"\"\n        Neural spks data.\n\n        This property must be implemented by all subclasses. It should return\n        the neural spike data for the session.\n\n        Returns\n        -------\n        np.ndarray\n            Neural spike data array. Shape and format depend on the subclass\n            implementation.\n\n        Notes\n        -----\n        This is always required for all session data objects.\n        \"\"\"\n        pass\n\n    @property\n    def session_name(self) -&gt; tuple[str]:\n        \"\"\"\n        Return the session identifier as a tuple.\n\n        Returns\n        -------\n        tuple[str]\n            Tuple containing (mouse_name, date) or (mouse_name, date, session_id)\n            if session_id is provided.\n        \"\"\"\n        if self.session_id:\n            return self.mouse_name, self.date, self.session_id\n        else:\n            return self.mouse_name, self.date\n\n    def session_print(self, joinby: str = \"/\") -&gt; str:\n        \"\"\"\n        Generate string representation of session name.\n\n        Parameters\n        ----------\n        joinby : str, optional\n            String to join session name components. Default is \"/\".\n\n        Returns\n        -------\n        str\n            String representation of the session name with components joined\n            by the specified separator.\n        \"\"\"\n        return joinby.join(self.session_name)\n\n    def get_value(self, key: str) -&gt; Any:\n        \"\"\"\n        Get value from the session stored in the values namespace.\n\n        Parameters\n        ----------\n        key : str\n            Key name of the value to retrieve.\n\n        Returns\n        -------\n        Any\n            Value stored under the specified key in the values namespace.\n\n        Raises\n        ------\n        AttributeError\n            If the key does not exist in the values namespace.\n        \"\"\"\n        return getattr(self.values, key)\n\n    def set_value(self, key: str, value: Any) -&gt; None:\n        \"\"\"\n        Set value in the session stored in the values namespace.\n\n        Parameters\n        ----------\n        key : str\n            Key name to store the value under.\n        value : Any\n            Value to store in the values namespace.\n        \"\"\"\n        setattr(self.values, key, value)\n\n    @property\n    def recipe_loaders(self) -&gt; dict:\n        \"\"\"\n        Dictionary of loaders for loading data from recipes.\n\n        This property enables specialized loading of onedata stored with recipes.\n        If not specified, attempting to load a recipe will raise an error.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping loader type strings to loader functions.\n\n        Raises\n        ------\n        NotImplementedError\n            If this property is not overridden by a subclass that uses recipes.\n\n        Notes\n        -----\n        Subclasses that use LoadingRecipe objects should override this property\n        to provide a dictionary of loader functions. Each loader function should\n        accept a source_arg and optional kwargs.\n        \"\"\"\n        raise NotImplementedError(f\"Attempting to load a recipe from {self.session_print()} but no recipe_loaders are specified.\")\n\n    @property\n    def recipe_transforms(self) -&gt; dict:\n        \"\"\"\n        Dictionary of transforms for applying to data when loading recipes.\n\n        This property enables specialized transforms of onedata stored with\n        recipes. If not specified, attempting to load a recipe that requires\n        a transform will raise an error.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping transform names to transform functions.\n\n        Raises\n        ------\n        NotImplementedError\n            If this property is not overridden by a subclass that uses recipes\n            with transforms.\n\n        Notes\n        -----\n        Subclasses that use LoadingRecipe objects with transforms should\n        override this property to provide a dictionary of transform functions.\n        Each transform function should accept data and return transformed data.\n        \"\"\"\n        raise NotImplementedError(f\"Attempting to transform a loaded recipe from {self.session_print()} but no recipe_transforms are specified.\")\n\n    @property\n    def one_path(self) -&gt; Path:\n        \"\"\"\n        Path to onedata directory.\n\n        Returns\n        -------\n        Path\n            Path object pointing to the onedata subdirectory within the\n            session data path.\n        \"\"\"\n        return self.data_path / \"onedata\"\n\n    def get_saved_one(self) -&gt; list[Path]:\n        \"\"\"\n        Get all saved onedata files.\n\n        Returns\n        -------\n        list[Path]\n            List of Path objects for all .npy and .npz files in the onedata\n            directory.\n        \"\"\"\n        return list(self.one_path.glob(\"*.npy\")) + list(self.one_path.glob(\"*.npz\"))\n\n    def print_saved_one(self, include_path: bool = False, include_extension: bool = False) -&gt; list[str]:\n        \"\"\"\n        Get formatted list of all saved onedata files.\n\n        Parameters\n        ----------\n        include_path : bool, optional\n            If True, include the full path in the output. Default is False.\n        include_extension : bool, optional\n            If True, include the file extension in the output. Default is False.\n\n        Returns\n        -------\n        list[str]\n            List of formatted file names (and optionally paths) for all saved\n            onedata files.\n        \"\"\"\n\n        def _format_name(name: Path):\n            onename = name.stem\n            if include_extension:\n                onename = onename + name.suffix\n            if include_path:\n                onename = name.parent / onename\n            return onename\n\n        return [_format_name(name) for name in self.get_saved_one()]\n\n    def clear_one_data(self, one_file_names: List[str] = None, certainty: bool = False) -&gt; None:\n        \"\"\"\n        Clear onedata files from the session directory.\n\n        Parameters\n        ----------\n        one_file_names : list[str], optional\n            List of specific onedata file names (without extension) to clear.\n            If None, all onedata files will be cleared. Default is None.\n        certainty : bool, optional\n            Safety flag that must be set to True to actually delete files.\n            Default is False.\n\n        Notes\n        -----\n        This operation is destructive and cannot be undone. The certainty flag\n        must be set to True to prevent accidental deletions.\n        \"\"\"\n        if not certainty:\n            print(f\"You have to be certain to clear onedata! (This means set kwarg certainty=True).\")\n            return None\n        one_files = self.get_saved_one()\n        if one_file_names:\n            one_files = [file for file in one_files if file.stem in one_file_names]\n        for file in one_files:\n            file.unlink()\n        print(f\"Cleared onedata from session: {self.session_print()}\")\n\n    def get_one_filename(self, *names: str) -&gt; str:\n        \"\"\"\n        Create onedata filename from an arbitrary length list of names.\n\n        Parameters\n        ----------\n        *names : str\n            Variable number of strings to join into a filename.\n\n        Returns\n        -------\n        str\n            Filename with components joined by \".\" and \".npy\" extension added.\n            Example: get_one_filename(\"mpci\", \"roiActivityF\") returns\n            \"mpci.roiActivityF.npy\".\n        \"\"\"\n        return \".\".join(names) + \".npy\"\n\n    def saveone(self, data: Union[np.ndarray, LoadingRecipe], *names: str, sparse: bool = False) -&gt; None:\n        \"\"\"\n        Save data directly or as a loading recipe.\n\n        Parameters\n        ----------\n        data : np.ndarray or LoadingRecipe\n            Data to save. Can be a numpy array or a LoadingRecipe object.\n        *names : str\n            Sequence of strings to join into filename (e.g., \"mpci\", \"roiActivityF\"\n            -&gt; \"mpci.roiActivityF.npy\").\n        sparse : bool, optional\n            If True, save as sparse array (.npz format). Data must be a\n            scipy.sparse.csc_array. Default is False.\n\n        Raises\n        ------\n        ValueError\n            If sparse=True but data is not a scipy.sparse.csc_array.\n\n        Notes\n        -----\n        When saving a numpy array (not sparse), the data is also cached in\n        one_cache for efficient subsequent access. LoadingRecipe objects are\n        saved as numpy arrays containing dictionaries with a special marker.\n        \"\"\"\n        file_name = self.get_one_filename(*names)\n        path = self.one_path / file_name\n        if isinstance(data, LoadingRecipe) or (\n            hasattr(data, \"to_dict\") and (hasattr(data, \"RECIPE_MARKER\") and data.RECIPE_MARKER == LoadingRecipe.RECIPE_MARKER)\n        ):\n            # Save recipe as a numpy array containing a dictionary\n            recipe_dict = data.to_dict()\n            np.save(path, np.array(recipe_dict, dtype=object))\n        elif sparse:\n            path = path.with_suffix(\".npz\")\n            if isinstance(data, csc_array):\n                save_npz(path, data)\n            else:\n                raise ValueError(\"Data is not a scipy.sparse.csc_array, not supported for saving with sparse=True\")\n        else:\n            # Save data directly to one file\n            self.one_cache[file_name] = data  # (standard practice is to buffer the data for efficient data handling)\n            np.save(path, data)\n\n    def loadone(self, *names: str, force: bool = False, sparse: bool = False, keep_sparse: bool = False) -&gt; np.ndarray:\n        \"\"\"\n        Load data, either directly or by following a recipe.\n\n        Parameters\n        ----------\n        *names : str\n            Sequence of strings to join into filename (e.g., \"mpci\", \"roiActivityF\"\n            -&gt; \"mpci.roiActivityF.npy\").\n        force : bool, optional\n            If True, reload data even if it is already in the cache. Default\n            is False.\n        sparse : bool, optional\n            If True, load as sparse array (.npz format). Default is False.\n        keep_sparse : bool, optional\n            If True and data is sparse, return a sparse array. Otherwise,\n            convert sparse arrays to dense. Default is False.\n\n        Returns\n        -------\n        np.ndarray\n            Loaded data. May be transformed if loaded from a recipe.\n\n        Raises\n        ------\n        ValueError\n            If the requested onedata file does not exist.\n        NotImplementedError\n            If loading a recipe but recipe_loaders or recipe_transforms are\n            not properly configured.\n\n        Notes\n        -----\n        Data is cached in one_cache after loading to avoid repeated disk reads.\n        If a LoadingRecipe is encountered, it will be executed using the\n        recipe_loaders and recipe_transforms properties.\n        \"\"\"\n        file_name = self.get_one_filename(*names)\n        path = self.one_path / file_name\n        if sparse:\n            path = path.with_suffix(\".npz\")\n            file_name = path.stem + path.suffix\n\n        if not force and file_name in self.one_cache.keys():\n            return self.one_cache[file_name]\n\n        else:\n            if not (path.exists()):\n                print(f\"In session {self.session_print()}, the one file {file_name} doesn't exist. Here is a list of saved oneData files:\")\n                for one_file in self.print_saved_one():\n                    print(one_file)\n                raise ValueError(\"onedata requested is not available\")\n\n            # Load saved numpy array at savepath (or sparse array if sparse=True)\n            if sparse:\n                data = load_npz(path)\n                if not keep_sparse:\n                    data = data.toarray()\n            else:\n                data = np.load(path, allow_pickle=True)\n                if LoadingRecipe.is_recipe(data):\n                    # Get loading recipe\n                    recipe = LoadingRecipe.from_dict(data.item())\n\n                    # Load data using appropriate loader\n                    if recipe.source_arg is not None:\n                        data = self.recipe_loaders[recipe.loader_type](recipe.source_arg, **recipe.kwargs)\n                    else:\n                        data = self.recipe_loaders[recipe.loader_type](**recipe.kwargs)\n\n                    # Apply transforms\n                    for transform in recipe.transforms:\n                        data = self.recipe_transforms[transform](data)\n\n            self.one_cache[file_name] = data\n            return data\n\n    def clear_cache(self, file_names: Optional[List[str]] = None) -&gt; None:\n        \"\"\"\n        Clear cached data to free memory.\n\n        Parameters\n        ----------\n        file_names : list[str], optional\n            List of specific file names (with extension) to remove from cache.\n            If None, the entire cache is cleared. Default is None.\n\n        Notes\n        -----\n        This method helps manage memory usage by removing cached onedata from\n        memory. The files themselves are not deleted, only the in-memory cache.\n        \"\"\"\n        if file_names is None:\n            self.one_cache = {}\n        else:\n            for file_name in file_names:\n                self.one_cache.pop(file_name, None)\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.one_path","title":"<code>one_path</code>  <code>property</code>","text":"<p>Path to onedata directory.</p> <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the onedata subdirectory within the session data path.</p>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.recipe_loaders","title":"<code>recipe_loaders</code>  <code>property</code>","text":"<p>Dictionary of loaders for loading data from recipes.</p> <p>This property enables specialized loading of onedata stored with recipes. If not specified, attempting to load a recipe will raise an error.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping loader type strings to loader functions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this property is not overridden by a subclass that uses recipes.</p> Notes <p>Subclasses that use LoadingRecipe objects should override this property to provide a dictionary of loader functions. Each loader function should accept a source_arg and optional kwargs.</p>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.recipe_transforms","title":"<code>recipe_transforms</code>  <code>property</code>","text":"<p>Dictionary of transforms for applying to data when loading recipes.</p> <p>This property enables specialized transforms of onedata stored with recipes. If not specified, attempting to load a recipe that requires a transform will raise an error.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping transform names to transform functions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If this property is not overridden by a subclass that uses recipes with transforms.</p> Notes <p>Subclasses that use LoadingRecipe objects with transforms should override this property to provide a dictionary of transform functions. Each transform function should accept data and return transformed data.</p>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.session_name","title":"<code>session_name</code>  <code>property</code>","text":"<p>Return the session identifier as a tuple.</p> <p>Returns:</p> Type Description <code>tuple[str]</code> <p>Tuple containing (mouse_name, date) or (mouse_name, date, session_id) if session_id is provided.</p>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.spks","title":"<code>spks</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Neural spks data.</p> <p>This property must be implemented by all subclasses. It should return the neural spike data for the session.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Neural spike data array. Shape and format depend on the subclass implementation.</p> Notes <p>This is always required for all session data objects.</p>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialization method to set all properties specific to subclasses.</p> <p>This method is called automatically after dataclass initialization. It formats the date, initializes the data path, and calls additional loading steps for subclasses.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Post-initialization method to set all properties specific to subclasses.\n\n    This method is called automatically after dataclass initialization.\n    It formats the date, initializes the data path, and calls additional\n    loading steps for subclasses.\n    \"\"\"\n    # Enable additional loading for subclasses if required\n    # usually to put things in the values namespace\n    self.date = str(PrettyDatetime.make_pretty(self.date))\n    self.data_path = self._init_data_path()\n    self._additional_loading()\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.clear_cache","title":"<code>clear_cache(file_names=None)</code>","text":"<p>Clear cached data to free memory.</p> <p>Parameters:</p> Name Type Description Default <code>file_names</code> <code>list[str]</code> <p>List of specific file names (with extension) to remove from cache. If None, the entire cache is cleared. Default is None.</p> <code>None</code> Notes <p>This method helps manage memory usage by removing cached onedata from memory. The files themselves are not deleted, only the in-memory cache.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def clear_cache(self, file_names: Optional[List[str]] = None) -&gt; None:\n    \"\"\"\n    Clear cached data to free memory.\n\n    Parameters\n    ----------\n    file_names : list[str], optional\n        List of specific file names (with extension) to remove from cache.\n        If None, the entire cache is cleared. Default is None.\n\n    Notes\n    -----\n    This method helps manage memory usage by removing cached onedata from\n    memory. The files themselves are not deleted, only the in-memory cache.\n    \"\"\"\n    if file_names is None:\n        self.one_cache = {}\n    else:\n        for file_name in file_names:\n            self.one_cache.pop(file_name, None)\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.clear_one_data","title":"<code>clear_one_data(one_file_names=None, certainty=False)</code>","text":"<p>Clear onedata files from the session directory.</p> <p>Parameters:</p> Name Type Description Default <code>one_file_names</code> <code>list[str]</code> <p>List of specific onedata file names (without extension) to clear. If None, all onedata files will be cleared. Default is None.</p> <code>None</code> <code>certainty</code> <code>bool</code> <p>Safety flag that must be set to True to actually delete files. Default is False.</p> <code>False</code> Notes <p>This operation is destructive and cannot be undone. The certainty flag must be set to True to prevent accidental deletions.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def clear_one_data(self, one_file_names: List[str] = None, certainty: bool = False) -&gt; None:\n    \"\"\"\n    Clear onedata files from the session directory.\n\n    Parameters\n    ----------\n    one_file_names : list[str], optional\n        List of specific onedata file names (without extension) to clear.\n        If None, all onedata files will be cleared. Default is None.\n    certainty : bool, optional\n        Safety flag that must be set to True to actually delete files.\n        Default is False.\n\n    Notes\n    -----\n    This operation is destructive and cannot be undone. The certainty flag\n    must be set to True to prevent accidental deletions.\n    \"\"\"\n    if not certainty:\n        print(f\"You have to be certain to clear onedata! (This means set kwarg certainty=True).\")\n        return None\n    one_files = self.get_saved_one()\n    if one_file_names:\n        one_files = [file for file in one_files if file.stem in one_file_names]\n    for file in one_files:\n        file.unlink()\n    print(f\"Cleared onedata from session: {self.session_print()}\")\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.get_one_filename","title":"<code>get_one_filename(*names)</code>","text":"<p>Create onedata filename from an arbitrary length list of names.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Variable number of strings to join into a filename.</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>Filename with components joined by \".\" and \".npy\" extension added. Example: get_one_filename(\"mpci\", \"roiActivityF\") returns \"mpci.roiActivityF.npy\".</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def get_one_filename(self, *names: str) -&gt; str:\n    \"\"\"\n    Create onedata filename from an arbitrary length list of names.\n\n    Parameters\n    ----------\n    *names : str\n        Variable number of strings to join into a filename.\n\n    Returns\n    -------\n    str\n        Filename with components joined by \".\" and \".npy\" extension added.\n        Example: get_one_filename(\"mpci\", \"roiActivityF\") returns\n        \"mpci.roiActivityF.npy\".\n    \"\"\"\n    return \".\".join(names) + \".npy\"\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.get_saved_one","title":"<code>get_saved_one()</code>","text":"<p>Get all saved onedata files.</p> <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of Path objects for all .npy and .npz files in the onedata directory.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def get_saved_one(self) -&gt; list[Path]:\n    \"\"\"\n    Get all saved onedata files.\n\n    Returns\n    -------\n    list[Path]\n        List of Path objects for all .npy and .npz files in the onedata\n        directory.\n    \"\"\"\n    return list(self.one_path.glob(\"*.npy\")) + list(self.one_path.glob(\"*.npz\"))\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.get_value","title":"<code>get_value(key)</code>","text":"<p>Get value from the session stored in the values namespace.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name of the value to retrieve.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Value stored under the specified key in the values namespace.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the key does not exist in the values namespace.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def get_value(self, key: str) -&gt; Any:\n    \"\"\"\n    Get value from the session stored in the values namespace.\n\n    Parameters\n    ----------\n    key : str\n        Key name of the value to retrieve.\n\n    Returns\n    -------\n    Any\n        Value stored under the specified key in the values namespace.\n\n    Raises\n    ------\n    AttributeError\n        If the key does not exist in the values namespace.\n    \"\"\"\n    return getattr(self.values, key)\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.loadone","title":"<code>loadone(*names, force=False, sparse=False, keep_sparse=False)</code>","text":"<p>Load data, either directly or by following a recipe.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>Sequence of strings to join into filename (e.g., \"mpci\", \"roiActivityF\" -&gt; \"mpci.roiActivityF.npy\").</p> <code>()</code> <code>force</code> <code>bool</code> <p>If True, reload data even if it is already in the cache. Default is False.</p> <code>False</code> <code>sparse</code> <code>bool</code> <p>If True, load as sparse array (.npz format). Default is False.</p> <code>False</code> <code>keep_sparse</code> <code>bool</code> <p>If True and data is sparse, return a sparse array. Otherwise, convert sparse arrays to dense. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Loaded data. May be transformed if loaded from a recipe.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested onedata file does not exist.</p> <code>NotImplementedError</code> <p>If loading a recipe but recipe_loaders or recipe_transforms are not properly configured.</p> Notes <p>Data is cached in one_cache after loading to avoid repeated disk reads. If a LoadingRecipe is encountered, it will be executed using the recipe_loaders and recipe_transforms properties.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def loadone(self, *names: str, force: bool = False, sparse: bool = False, keep_sparse: bool = False) -&gt; np.ndarray:\n    \"\"\"\n    Load data, either directly or by following a recipe.\n\n    Parameters\n    ----------\n    *names : str\n        Sequence of strings to join into filename (e.g., \"mpci\", \"roiActivityF\"\n        -&gt; \"mpci.roiActivityF.npy\").\n    force : bool, optional\n        If True, reload data even if it is already in the cache. Default\n        is False.\n    sparse : bool, optional\n        If True, load as sparse array (.npz format). Default is False.\n    keep_sparse : bool, optional\n        If True and data is sparse, return a sparse array. Otherwise,\n        convert sparse arrays to dense. Default is False.\n\n    Returns\n    -------\n    np.ndarray\n        Loaded data. May be transformed if loaded from a recipe.\n\n    Raises\n    ------\n    ValueError\n        If the requested onedata file does not exist.\n    NotImplementedError\n        If loading a recipe but recipe_loaders or recipe_transforms are\n        not properly configured.\n\n    Notes\n    -----\n    Data is cached in one_cache after loading to avoid repeated disk reads.\n    If a LoadingRecipe is encountered, it will be executed using the\n    recipe_loaders and recipe_transforms properties.\n    \"\"\"\n    file_name = self.get_one_filename(*names)\n    path = self.one_path / file_name\n    if sparse:\n        path = path.with_suffix(\".npz\")\n        file_name = path.stem + path.suffix\n\n    if not force and file_name in self.one_cache.keys():\n        return self.one_cache[file_name]\n\n    else:\n        if not (path.exists()):\n            print(f\"In session {self.session_print()}, the one file {file_name} doesn't exist. Here is a list of saved oneData files:\")\n            for one_file in self.print_saved_one():\n                print(one_file)\n            raise ValueError(\"onedata requested is not available\")\n\n        # Load saved numpy array at savepath (or sparse array if sparse=True)\n        if sparse:\n            data = load_npz(path)\n            if not keep_sparse:\n                data = data.toarray()\n        else:\n            data = np.load(path, allow_pickle=True)\n            if LoadingRecipe.is_recipe(data):\n                # Get loading recipe\n                recipe = LoadingRecipe.from_dict(data.item())\n\n                # Load data using appropriate loader\n                if recipe.source_arg is not None:\n                    data = self.recipe_loaders[recipe.loader_type](recipe.source_arg, **recipe.kwargs)\n                else:\n                    data = self.recipe_loaders[recipe.loader_type](**recipe.kwargs)\n\n                # Apply transforms\n                for transform in recipe.transforms:\n                    data = self.recipe_transforms[transform](data)\n\n        self.one_cache[file_name] = data\n        return data\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.print_saved_one","title":"<code>print_saved_one(include_path=False, include_extension=False)</code>","text":"<p>Get formatted list of all saved onedata files.</p> <p>Parameters:</p> Name Type Description Default <code>include_path</code> <code>bool</code> <p>If True, include the full path in the output. Default is False.</p> <code>False</code> <code>include_extension</code> <code>bool</code> <p>If True, include the file extension in the output. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of formatted file names (and optionally paths) for all saved onedata files.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def print_saved_one(self, include_path: bool = False, include_extension: bool = False) -&gt; list[str]:\n    \"\"\"\n    Get formatted list of all saved onedata files.\n\n    Parameters\n    ----------\n    include_path : bool, optional\n        If True, include the full path in the output. Default is False.\n    include_extension : bool, optional\n        If True, include the file extension in the output. Default is False.\n\n    Returns\n    -------\n    list[str]\n        List of formatted file names (and optionally paths) for all saved\n        onedata files.\n    \"\"\"\n\n    def _format_name(name: Path):\n        onename = name.stem\n        if include_extension:\n            onename = onename + name.suffix\n        if include_path:\n            onename = name.parent / onename\n        return onename\n\n    return [_format_name(name) for name in self.get_saved_one()]\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.saveone","title":"<code>saveone(data, *names, sparse=False)</code>","text":"<p>Save data directly or as a loading recipe.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray or LoadingRecipe</code> <p>Data to save. Can be a numpy array or a LoadingRecipe object.</p> required <code>*names</code> <code>str</code> <p>Sequence of strings to join into filename (e.g., \"mpci\", \"roiActivityF\" -&gt; \"mpci.roiActivityF.npy\").</p> <code>()</code> <code>sparse</code> <code>bool</code> <p>If True, save as sparse array (.npz format). Data must be a scipy.sparse.csc_array. Default is False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sparse=True but data is not a scipy.sparse.csc_array.</p> Notes <p>When saving a numpy array (not sparse), the data is also cached in one_cache for efficient subsequent access. LoadingRecipe objects are saved as numpy arrays containing dictionaries with a special marker.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def saveone(self, data: Union[np.ndarray, LoadingRecipe], *names: str, sparse: bool = False) -&gt; None:\n    \"\"\"\n    Save data directly or as a loading recipe.\n\n    Parameters\n    ----------\n    data : np.ndarray or LoadingRecipe\n        Data to save. Can be a numpy array or a LoadingRecipe object.\n    *names : str\n        Sequence of strings to join into filename (e.g., \"mpci\", \"roiActivityF\"\n        -&gt; \"mpci.roiActivityF.npy\").\n    sparse : bool, optional\n        If True, save as sparse array (.npz format). Data must be a\n        scipy.sparse.csc_array. Default is False.\n\n    Raises\n    ------\n    ValueError\n        If sparse=True but data is not a scipy.sparse.csc_array.\n\n    Notes\n    -----\n    When saving a numpy array (not sparse), the data is also cached in\n    one_cache for efficient subsequent access. LoadingRecipe objects are\n    saved as numpy arrays containing dictionaries with a special marker.\n    \"\"\"\n    file_name = self.get_one_filename(*names)\n    path = self.one_path / file_name\n    if isinstance(data, LoadingRecipe) or (\n        hasattr(data, \"to_dict\") and (hasattr(data, \"RECIPE_MARKER\") and data.RECIPE_MARKER == LoadingRecipe.RECIPE_MARKER)\n    ):\n        # Save recipe as a numpy array containing a dictionary\n        recipe_dict = data.to_dict()\n        np.save(path, np.array(recipe_dict, dtype=object))\n    elif sparse:\n        path = path.with_suffix(\".npz\")\n        if isinstance(data, csc_array):\n            save_npz(path, data)\n        else:\n            raise ValueError(\"Data is not a scipy.sparse.csc_array, not supported for saving with sparse=True\")\n    else:\n        # Save data directly to one file\n        self.one_cache[file_name] = data  # (standard practice is to buffer the data for efficient data handling)\n        np.save(path, data)\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.session_print","title":"<code>session_print(joinby='/')</code>","text":"<p>Generate string representation of session name.</p> <p>Parameters:</p> Name Type Description Default <code>joinby</code> <code>str</code> <p>String to join session name components. Default is \"/\".</p> <code>'/'</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the session name with components joined by the specified separator.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def session_print(self, joinby: str = \"/\") -&gt; str:\n    \"\"\"\n    Generate string representation of session name.\n\n    Parameters\n    ----------\n    joinby : str, optional\n        String to join session name components. Default is \"/\".\n\n    Returns\n    -------\n    str\n        String representation of the session name with components joined\n        by the specified separator.\n    \"\"\"\n    return joinby.join(self.session_name)\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.SessionData.set_value","title":"<code>set_value(key, value)</code>","text":"<p>Set value in the session stored in the values namespace.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key name to store the value under.</p> required <code>value</code> <code>Any</code> <p>Value to store in the values namespace.</p> required Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def set_value(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Set value in the session stored in the values namespace.\n\n    Parameters\n    ----------\n    key : str\n        Key name to store the value under.\n    value : Any\n        Value to store in the values namespace.\n    \"\"\"\n    setattr(self.values, key, value)\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.LoadingRecipe","title":"<code>LoadingRecipe</code>","text":"Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>class LoadingRecipe:\n    RECIPE_MARKER = \"__LOADING_RECIPE__\"\n\n    def __init__(self, loader_type: str, source_arg: str, transforms: list = None, **kwargs):\n        \"\"\"\n        Represents instructions for loading data.\n\n        Use case: Save a recipe to load data from an existing location instead of\n        resaving the data with a new name. Will save memory on the device and\n        generally have a very small overhead.\n\n        Parameters\n        ----------\n        loader_type : str\n            String identifying the loading method (e.g., 'numpy', 's2p').\n        source_arg : str\n            String identifying the source data (e.g. 'F', 'ops').\n        transforms : list, optional\n            List of transformation operations to apply. Default is None.\n        **kwargs\n            Additional arguments for the loader.\n        \"\"\"\n        self.loader_type = loader_type\n        self.source_arg = source_arg\n        self.transforms = transforms or []\n        self.kwargs = kwargs\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"\n        Convert recipe to dictionary for serialization.\n\n        Returns\n        -------\n        dict\n            Dictionary representation of the recipe with marker, loader_type,\n            source_arg, transforms, and kwargs.\n        \"\"\"\n        return {\n            \"marker\": self.RECIPE_MARKER,\n            \"loader_type\": self.loader_type,\n            \"source_arg\": self.source_arg,\n            \"transforms\": self.transforms,\n            \"kwargs\": self.kwargs,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"LoadingRecipe\":\n        \"\"\"\n        Create recipe from dictionary.\n\n        Parameters\n        ----------\n        data : dict\n            Dictionary containing recipe information with keys: loader_type,\n            source_arg, transforms, and kwargs.\n\n        Returns\n        -------\n        LoadingRecipe\n            LoadingRecipe instance created from the dictionary.\n        \"\"\"\n        return cls(data[\"loader_type\"], data[\"source_arg\"], data[\"transforms\"], **data[\"kwargs\"])\n\n    @classmethod\n    def is_recipe(cls, data: np.ndarray) -&gt; bool:\n        \"\"\"\n        Check if the loaded data is actually a recipe.\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Numpy array that may contain a recipe dictionary.\n\n        Returns\n        -------\n        bool\n            True if the data contains a recipe marker, False otherwise.\n        \"\"\"\n        try:\n            dict_data = data.item()\n            return isinstance(dict_data, dict) and dict_data.get(\"marker\", None) == cls.RECIPE_MARKER\n        except (AttributeError, ValueError):\n            return False\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.LoadingRecipe.__init__","title":"<code>__init__(loader_type, source_arg, transforms=None, **kwargs)</code>","text":"<p>Represents instructions for loading data.</p> <p>Use case: Save a recipe to load data from an existing location instead of resaving the data with a new name. Will save memory on the device and generally have a very small overhead.</p> <p>Parameters:</p> Name Type Description Default <code>loader_type</code> <code>str</code> <p>String identifying the loading method (e.g., 'numpy', 's2p').</p> required <code>source_arg</code> <code>str</code> <p>String identifying the source data (e.g. 'F', 'ops').</p> required <code>transforms</code> <code>list</code> <p>List of transformation operations to apply. Default is None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for the loader.</p> <code>{}</code> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def __init__(self, loader_type: str, source_arg: str, transforms: list = None, **kwargs):\n    \"\"\"\n    Represents instructions for loading data.\n\n    Use case: Save a recipe to load data from an existing location instead of\n    resaving the data with a new name. Will save memory on the device and\n    generally have a very small overhead.\n\n    Parameters\n    ----------\n    loader_type : str\n        String identifying the loading method (e.g., 'numpy', 's2p').\n    source_arg : str\n        String identifying the source data (e.g. 'F', 'ops').\n    transforms : list, optional\n        List of transformation operations to apply. Default is None.\n    **kwargs\n        Additional arguments for the loader.\n    \"\"\"\n    self.loader_type = loader_type\n    self.source_arg = source_arg\n    self.transforms = transforms or []\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.LoadingRecipe.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create recipe from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary containing recipe information with keys: loader_type, source_arg, transforms, and kwargs.</p> required <p>Returns:</p> Type Description <code>LoadingRecipe</code> <p>LoadingRecipe instance created from the dictionary.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"LoadingRecipe\":\n    \"\"\"\n    Create recipe from dictionary.\n\n    Parameters\n    ----------\n    data : dict\n        Dictionary containing recipe information with keys: loader_type,\n        source_arg, transforms, and kwargs.\n\n    Returns\n    -------\n    LoadingRecipe\n        LoadingRecipe instance created from the dictionary.\n    \"\"\"\n    return cls(data[\"loader_type\"], data[\"source_arg\"], data[\"transforms\"], **data[\"kwargs\"])\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.LoadingRecipe.is_recipe","title":"<code>is_recipe(data)</code>  <code>classmethod</code>","text":"<p>Check if the loaded data is actually a recipe.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Numpy array that may contain a recipe dictionary.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the data contains a recipe marker, False otherwise.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>@classmethod\ndef is_recipe(cls, data: np.ndarray) -&gt; bool:\n    \"\"\"\n    Check if the loaded data is actually a recipe.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Numpy array that may contain a recipe dictionary.\n\n    Returns\n    -------\n    bool\n        True if the data contains a recipe marker, False otherwise.\n    \"\"\"\n    try:\n        dict_data = data.item()\n        return isinstance(dict_data, dict) and dict_data.get(\"marker\", None) == cls.RECIPE_MARKER\n    except (AttributeError, ValueError):\n        return False\n</code></pre>"},{"location":"api/sessions/#vrAnalysis.sessions.base.LoadingRecipe.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert recipe to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary representation of the recipe with marker, loader_type, source_arg, transforms, and kwargs.</p> Source code in <code>vrAnalysis/sessions/base.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert recipe to dictionary for serialization.\n\n    Returns\n    -------\n    dict\n        Dictionary representation of the recipe with marker, loader_type,\n        source_arg, transforms, and kwargs.\n    \"\"\"\n    return {\n        \"marker\": self.RECIPE_MARKER,\n        \"loader_type\": self.loader_type,\n        \"source_arg\": self.source_arg,\n        \"transforms\": self.transforms,\n        \"kwargs\": self.kwargs,\n    }\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This section contains practical examples demonstrating how to use vrAnalysis for common analysis tasks.</p> <p>Although at the moment there is only one example, sorry!</p> <ul> <li>Session Analysis</li> </ul>"},{"location":"examples/session_analysis/","title":"Session Analysis Example","text":"<p>This example demonstrates how to perform analysis on a single session.</p>"},{"location":"examples/session_analysis/#loading-and-processing-data","title":"Loading and Processing Data","text":"<pre><code>from vrAnalysis.sessions import B2Session\nfrom vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Create session\nsession = B2Session.create(\n    mouse_name=\"mouse001\",\n    date=\"2024-01-15\",\n    session_id=\"001\"\n)\n\n# Load data\nsession.load_data()\n\n# Generate spike maps\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(\n    bin_size=5.0,\n    by_environment=True,\n    min_occupancy=0.1\n)\n\n# Access maps\nfor env_idx, env in enumerate(maps.environments):\n    occ_map = maps.occmap[env_idx]\n    spk_map = maps.spkmap[env_idx]\n\n    # Perform analysis on maps\n    # (analysis code here)\n</code></pre>"},{"location":"examples/session_analysis/#analyzing-place-cells","title":"Analyzing Place Cells","text":"<pre><code>from vrAnalysis.processors.spkmaps import SpkmapProcessor\n\n# Generate spike maps and reliability\nprocessor = SpkmapProcessor(session)\nmaps = processor.process(bin_size=5.0, by_environment=True)\nreliability = processor.get_reliability(envnum=0)  # Get reliability for environment 0\n\n# Access results\nreliability_scores = reliability.reliability  # Array of reliability scores per ROI\n</code></pre>"},{"location":"examples/session_analysis/#visualizing-results","title":"Visualizing Results","text":"<pre><code># Plot spike maps for cells with high reliability\n# (You'll need to implement plotting based on your visualization needs)\nimport matplotlib.pyplot as plt\n\n# Example: plot spike map for a single ROI\nroi_idx = 0\nif maps.by_environment:\n    spk_map = maps.spkmap[0][roi_idx]  # First environment, specific ROI\n    occ_map = maps.occmap[0]\nelse:\n    spk_map = maps.spkmap[roi_idx]\n    occ_map = maps.occmap\n\n# Create rate map (spikes per second)\nrate_map = spk_map / (occ_map + 1e-6)  # Avoid division by zero\n\nplt.imshow(rate_map, aspect='auto', origin='lower')\nplt.colorbar(label='Firing rate (Hz)')\nplt.title(f\"ROI {roi_idx}\")\nplt.show()\n</code></pre>"},{"location":"workflows/","title":"Workflows","text":"<p>This section contains step-by-step workflows for common tasks in vrAnalysis.</p>"},{"location":"workflows/#available-workflows","title":"Available Workflows","text":"<ul> <li>Registration - Guide to registering sessions and aligning data across sessions</li> </ul>"},{"location":"workflows/registration/","title":"Registration","text":"<p>Registration is the process of preprocessing and preparing experimental data from VR sessions for analysis. This includes processing behavioral data from Timeline and vrControl, aligning imaging data from suite2p, running deconvolution algorithms, and creating a unified data structure for analysis.</p>"},{"location":"workflows/registration/#overview","title":"Overview","text":"<p>The registration process transforms raw experimental data into a standardized format that can be used for analysis. It handles both behavioral and imaging data. It handles:</p> <ul> <li>Timeline Data: Raw hardware signals from Rigbox (rotary encoder, photodiode, lick detector, reward commands)</li> <li>Behavioral Data: VR environment data from vrControl (position tracking, trial information, rewards, licks)</li> <li>Imaging Data: Calcium imaging data from suite2p (ROI fluorescence traces, deconvolved spikes)</li> <li>Data Alignment: Synchronizing behavioral and imaging data in time</li> </ul> <p>The registration process produces \"onedata\" files - standardized NumPy arrays stored in the session's <code>onedata/</code> directory that can be easily loaded for analysis.</p>"},{"location":"workflows/registration/#prerequisites","title":"Prerequisites","text":"<p>Before registering a session, several prerequisites must be met:</p>"},{"location":"workflows/registration/#1-database-setup","title":"1. Database Setup","text":"<p>You must have a SQL database (preferably Microsoft Access) set up with a table containing session information. The table should have the following columns (not all are required):</p> <p>Required Columns:</p> <ul> <li><code>uSessionID</code> (int): Unique session identifier</li> <li><code>mouseName</code> (str): Mouse identifier</li> <li><code>sessionDate</code> (datetime): Session date</li> <li><code>sessionID</code> (int): Session ID number</li> <li><code>vrRegistration</code> (bool): Registration status flag</li> <li><code>imaging</code> (bool): Whether session has imaging data</li> <li><code>behavior</code> (bool): Whether session has behavioral data</li> <li><code>faceCamera</code> (bool): Whether session has face camera data</li> <li><code>vrBehaviorVersion</code> (int): Version of vrControl software used (1 or 2)</li> </ul> <p>Optional Columns:</p> <ul> <li><code>sessionQC</code> (bool): Quality control flag</li> <li><code>experimentType</code> (str)</li> <li><code>experimentID</code> (int)</li> <li><code>variableGain</code> (bool)</li> <li><code>vrEnvironments</code> (int)</li> <li><code>headPlateRotation</code> (float)</li> <li><code>numPlanes</code> (int)</li> <li><code>planeSeparation</code> (float)</li> <li><code>pockelsPercentage</code> (float)</li> <li><code>objectiveRotation</code> (float)</li> <li><code>suite2p</code> (bool)</li> <li><code>suite2pQC</code> (bool)</li> <li><code>redCellQC</code> (bool)</li> <li><code>scratchJustification</code> (str)</li> <li><code>logtime</code> (datetime)</li> <li><code>sessionNotes</code> (str)</li> <li><code>suite2pDate</code> (datetime)</li> <li><code>vrRegistrationDate</code> (datetime)</li> <li><code>vrRegistrationError</code> (bool)</li> <li><code>vrRegistrationException</code> (str)</li> <li><code>redCellQCDate</code> (datetime)</li> <li><code>dontTrack</code> (bool)</li> </ul>"},{"location":"workflows/registration/#2-adding-sessions-to-database","title":"2. Adding Sessions to Database","text":"<p>The best way to add a new session entry to the database is using the GUI. The GUI will automatically build a form based on the database schema, so this will actually work for any database object you provide. </p> <pre><code>  from vrAnalysis.database import get_database\n  from vrAnalysis.uilib.add_entry_gui import NewEntryGUI\n\n  # Get database\n  db = get_database(\"vrSessions\")\n\n  # Optionally create a session object to auto-populate fields\n  from vrAnalysis.sessions import B2Session\n  session = B2Session.create(\"mouse001\", \"2024-01-15\", \"001\")\n\n  # any other parameters you want to use to preload the GUI form.\n  other_params = {} \n\n  # Open GUI to add entry\n  gui = NewEntryGUI(db, ses=session, **other_params)\n</code></pre> <p>The GUI will automatically populate fields from the session object if provided, and validate all inputs before submission.</p>"},{"location":"workflows/registration/#3-suite2p-processing-for-imaging-sessions","title":"3. Suite2p Processing (for imaging sessions)","text":"<p>If the session has imaging data, suite2p must be run before registration. The suite2p output directory should be located at:</p> <pre><code>{session_path}/suite2p/\n</code></pre> <p>Where <code>session_path</code> follows the conventional structure: <code>{local_data_path}/{mouse_name}/{date}/{session_id}/</code></p> <p>The suite2p directory should contain subdirectories for each imaging plane (e.g., <code>plane0/</code>, <code>plane1/</code>, etc.), each containing the standard suite2p output files (<code>.npy</code> files like <code>F.npy</code>, <code>Fneu.npy</code>, <code>spks.npy</code>, <code>stat.npy</code>, <code>ops.npy</code>, <code>iscell.npy</code>).</p>"},{"location":"workflows/registration/#4-required-input-files","title":"4. Required Input Files","text":"<p>The session directory must contain the following files. Note that they are determined by other software - including both vrControl and Rigbox.</p> <p>Timeline Data: - <code>{date}_{session_id}_{mouse_name}_Timeline.mat</code>: Contains raw hardware signals from Rigbox</p> <p>Behavioral Data: - <code>{date}_{session_id}_{mouse_name}_VRBehavior_trial.mat</code>: Contains VR environment data from vrControl</p> <p>These files are typically generated during the experiment by Rigbox and vrControl software.</p>"},{"location":"workflows/registration/#data-structure","title":"Data Structure","text":""},{"location":"workflows/registration/#timeline-data","title":"Timeline Data","text":"<p>The Timeline.mat file contains hardware signals recorded by Rigbox:</p> <ul> <li>timestamps: Raw DAQ timestamps for all signals</li> <li>rotaryEncoder: Rotary encoder position counter (circular, needs conversion to linear position)</li> <li>photoDiode: Photodiode signal for detecting visual stimulus flips</li> <li>lickDetector: Edge counter signal for detecting licks</li> <li>rewardCommand: Voltage signal indicating reward delivery</li> <li>neuralFrames: Frame counter from ScanImage indicating imaging volume acquisition</li> <li>mpepUDPTimes/mpepUDPEvents: Trial start/end messages from vrControl</li> </ul>"},{"location":"workflows/registration/#behavioral-data","title":"Behavioral Data","text":"<p>The VRBehavior_trial.mat file contains VR environment data. The structure varies by <code>vrBehaviorVersion</code>:</p> <p>Version 1 (standard_behavior):</p> <ul> <li><code>expInfo</code>: Experiment information (room length, movement gain per trial)</li> <li><code>trialInfo</code>: Sparse matrices containing trial-by-trial data:</li> <li><code>time</code>: Timestamps for each behavioral sample</li> <li><code>roomPosition</code>: Virtual position in VR environment</li> <li><code>rewardPosition</code>: Reward zone position</li> <li><code>rewardTolerance</code>: Reward zone half-width</li> <li><code>rewardAvailable</code>: Whether reward was available</li> <li><code>rewardDeliveryFrame</code>: Frame when reward was delivered</li> <li><code>activeLicking</code>: Whether active licking was required</li> <li><code>activeStopping</code>: Whether active stopping was required</li> <li><code>lick</code>: Lick counts per frame</li> <li><code>vrEnvIdx</code>: Environment index (optional)</li> </ul> <p>Version 2 (cr_hippocannula_behavior):</p> <ul> <li>Similar structure but with different field names (<code>TRIAL</code> instead of <code>trialInfo</code>, <code>EXP</code> instead of <code>expInfo</code>)</li> </ul> <p>Both versions also contain <code>rigInfo</code> with hardware configuration:</p> <ul> <li><code>rotEncPos</code>: Rotary encoder position (\"left\" or \"right\")</li> <li><code>rotEncSign</code>: Sign of rotary encoder (-1 or 1)</li> <li><code>wheelToVR</code>: Conversion factor from wheel counts to VR units</li> <li><code>wheelRadius</code>: Physical wheel radius in cm</li> <li><code>rotaryRange</code>: Bit range of rotary encoder (typically 32)</li> </ul>"},{"location":"workflows/registration/#imaging-data","title":"Imaging Data","text":"<p>Suite2p outputs are organized by plane:</p> <pre><code>suite2p/\n  plane0/\n    F.npy          # Fluorescence traces (nROIs x nFrames)\n    Fneu.npy       # Neuropil fluorescence (nROIs x nFrames)\n    spks.npy       # Deconvolved spikes (nROIs x nFrames)\n    stat.npy        # ROI statistics (list of dicts)\n    ops.npy         # Suite2p options (dict)\n    iscell.npy      # Cell classifier output (nROIs x 2)\n    redcell.npy     # Red cell classifier output (optional, nROIs x 2)\n</code></pre>"},{"location":"workflows/registration/#using-the-database-pipeline","title":"Using the Database Pipeline","text":"<p>The recommended way to register sessions is through the database pipeline, which handles error tracking and status updates automatically. The database object for sessions includes some supporting methods that make it easy to identify sessions that need registration and register them all at once.</p>"},{"location":"workflows/registration/#finding-sessions-needing-registration","title":"Finding Sessions Needing Registration","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Get DataFrame of sessions needing registration\n  needs_reg = db.needs_registration(return_df=True)\n\n  # Filter by mouse\n  needs_reg = db.needs_registration(mouseName=\"mouse001\", return_df=True)\n\n  # Print sessions (alternative to DataFrame)\n  db.needs_registration(return_df=False, mouseName=\"mouse001\")\n</code></pre>"},{"location":"workflows/registration/#registering-a-single-session","title":"Registering a Single Session","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Register by identifiers\n  success = db.register_single_session(\n      mouse_name=\"mouse001\",\n      session_date=\"2024-01-15\",\n      session_id=\"001\",\n  )\n\n  if success:\n      print(\"Registration successful!\")\n  else:\n      print(\"Registration failed - check database for error details\")\n</code></pre>"},{"location":"workflows/registration/#registering-multiple-sessions","title":"Registering Multiple Sessions","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Register all sessions needing registration\n  # Stops when total data size exceeds max_data (default 30 GB)\n  db.register_sessions(\n      max_data=30e9,        # Maximum total data to process (bytes)\n      skip_errors=True,     # Skip sessions with previous errors\n      raise_exception=False, # Don't raise exceptions on failure\n  )\n</code></pre> <p>The <code>register_sessions</code> method provides progress updates including: - Accumulated onedata registered - Average data size per session - Estimated remaining data to process</p>"},{"location":"workflows/registration/#error-handling","title":"Error Handling","text":"<p>When registration fails, the database is automatically updated:</p> <ul> <li><code>vrRegistrationError</code> set to <code>True</code></li> <li><code>vrRegistrationException</code> contains the error message</li> <li>All onedata files are cleared</li> <li>Registration status remains <code>False</code></li> </ul> <p>You can check for errors:</p> <pre><code>  # Print all registration errors\n  db.print_registration_errors()\n\n  # Get sessions with errors\n  errors = db.get_table(vrRegistrationError=True)\n</code></pre> <p>If you try to register sessions without setting the <code>skip_errors</code> parameter to <code>False</code>, the bulk registration pipeline will simply skip sessions that had previous errors. You probably don't want this unless you want to abandon the session! So use the <code>needs_registration</code> method above and the <code>kwarg</code> <code>vrRegistrationError=True</code> to identify sessions that had errors. You'll need to debug them yourself.</p>"},{"location":"workflows/registration/#manual-registration","title":"Manual Registration","text":"<p>You can also register sessions directly without using the database:</p> <pre><code>  from vrAnalysis.registration import B2Registration\n  from vrAnalysis.sessions.b2session import B2RegistrationOpts\n\n  # Create registration options\n  opts = B2RegistrationOpts(\n      vrBehaviorVersion=1,      # Version of vrControl (1 or 2)\n      facecam=False,             # Process face camera data\n      imaging=True,              # Process imaging data\n      oasis=True,                # Run OASIS deconvolution\n      redCellProcessing=True,   # Process red cell features\n      clearOne=True,            # Clear existing onedata before registration\n      neuropilCoefficient=0.7,  # Neuropil subtraction coefficient\n      tau=1.5,                  # OASIS tau parameter (seconds)\n      fs=6                      # Sampling frequency (Hz)\n  )\n\n  # Create registration object\n  registration = B2Registration(\n      mouse_name=\"mouse001\",\n      date_string=\"2024-01-15\",\n      session_id=\"001\",\n      opts=opts\n  )\n\n  # Run registration\n  registration.register()\n\n  # Save session parameters (automatically called by register())\n  # registration.save_session_prms()\n</code></pre>"},{"location":"workflows/registration/#registration-options","title":"Registration Options","text":"<p>The <code>B2RegistrationOpts</code> dataclass controls registration behavior:</p> <p>Behavior Options: - <code>vrBehaviorVersion</code> (int): Version of vrControl software (1=standard, 2=CR hippocampus, software no longer exists!)     - Note: this system will allow you to use this pipeline with new behavior software, just add a new version number and a new behavior processing function in the <code>vrAnalysis.registration.behavior</code> module.</p> <p>Data Processing Flags:</p> <ul> <li><code>imaging</code> (bool): Process imaging data (requires suite2p directory)</li> <li><code>facecam</code> (bool): Process face camera data (not yet implemented)</li> <li><code>redCellProcessing</code> (bool): Compute red cell features</li> </ul> <p>Deconvolution Options:</p> <ul> <li><code>oasis</code> (bool): Run OASIS deconvolution (recomputes spikes from corrected fluorescence)</li> <li><code>tau</code> (float): OASIS decay time constant in seconds (default: 1.5)</li> <li><code>fs</code> (int): Sampling frequency in Hz (default: 6)</li> <li><code>neuropilCoefficient</code> (float): Coefficient for neuropil subtraction (default: 0.7)</li> </ul> <p>Data Management:</p> <ul> <li><code>clearOne</code> (bool): Clear existing onedata before registration (default: True)</li> </ul>"},{"location":"workflows/registration/#output-data-structure","title":"Output Data Structure","text":"<p>After registration, the session's <code>onedata/</code> directory contains standardized NumPy arrays:</p>"},{"location":"workflows/registration/#timeline-data_1","title":"Timeline Data","text":"<ul> <li><code>wheelPosition.times.npy</code>: Timeline timestamps</li> <li><code>wheelPosition.position.npy</code>: Wheel position in cm</li> <li><code>licks.times.npy</code>: Lick event timestamps</li> <li><code>rewards.times.npy</code>: Reward delivery timestamps</li> <li><code>trials.startTimes.npy</code>: Trial start timestamps</li> </ul>"},{"location":"workflows/registration/#behavioral-data_1","title":"Behavioral Data","text":"<ul> <li><code>positionTracking.times.npy</code>: Behavioral timestamps</li> <li><code>positionTracking.position.npy</code>: Virtual position in VR environment</li> <li><code>positionTracking.mpci.npy</code>: Mapping from behavioral samples to imaging frames</li> <li><code>trials.positionTracking.npy</code>: Start frame index for each trial</li> <li><code>trials.environmentIndex.npy</code>: Environment ID for each trial</li> <li><code>trials.roomlength.npy</code>: Room length for each trial</li> <li><code>trials.movementGain.npy</code>: Movement gain for each trial</li> <li><code>trials.rewardPosition.npy</code>: Reward zone position</li> <li><code>trials.rewardZoneHalfwidth.npy</code>: Reward zone half-width</li> <li><code>trials.rewardAvailability.npy</code>: Whether reward was available</li> <li><code>trials.rewardPositionTracking.npy</code>: Reward delivery frame index</li> <li><code>trials.activeLicking.npy</code>: Active licking requirement</li> <li><code>trials.activeStopping.npy</code>: Active stopping requirement</li> <li><code>licksTracking.positionTracking.npy</code>: Lick event indices in behavioral samples</li> </ul>"},{"location":"workflows/registration/#imaging-data_1","title":"Imaging Data","text":"<ul> <li><code>mpci.times.npy</code>: Imaging frame timestamps</li> <li><code>mpci.roiActivityF.npy</code>: Fluorescence traces (or LoadingRecipe reference)</li> <li><code>mpci.roiNeuropilActivityF.npy</code>: Neuropil fluorescence (or LoadingRecipe reference)</li> <li><code>mpci.roiActivityDeconvolved.npy</code>: Suite2p deconvolved spikes (or LoadingRecipe reference)</li> <li><code>mpci.roiActivityDeconvolvedOasis.npy</code>: OASIS deconvolved spikes (if computed)</li> <li><code>mpciROIs.isCell.npy</code>: Cell classifier output</li> <li><code>mpciROIs.stackPosition.npy</code>: ROI positions (nROIs x 3: x, y, plane)</li> <li><code>mpciROIs.redS2P.npy</code>: Red cell classifier output (if available)</li> </ul>"},{"location":"workflows/registration/#red-cell-data-if-processed","title":"Red Cell Data (if processed)","text":"<ul> <li><code>mpciROIs.redDotProduct.npy</code>: Dot product feature</li> <li><code>mpciROIs.redPearson.npy</code>: Pearson correlation feature</li> <li><code>mpciROIs.redPhaseCorrelation.npy</code>: Phase correlation feature</li> <li><code>mpciROIs.redCellIdx.npy</code>: Boolean array for red cell identification</li> <li><code>mpciROIs.redCellManualAssignments.npy</code>: Manual assignment array</li> <li><code>parametersRedDotProduct.keyValuePairs.npy</code>: Dot product parameters</li> <li><code>parametersRedPearson.keyValuePairs.npy</code>: Pearson parameters</li> <li><code>parametersRedPhaseCorrelation.keyValuePairs.npy</code>: Phase correlation parameters</li> </ul>"},{"location":"workflows/registration/#session-metadata","title":"Session Metadata","text":"<ul> <li><code>vrExperimentOptions.json</code>: Registration options used</li> <li><code>vrExperimentPreprocessing.json</code>: List of preprocessing steps completed</li> <li><code>vrExperimentValues.json</code>: Session metadata values (numTrials, numROIs, etc.)</li> </ul>"},{"location":"workflows/registration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"workflows/registration/#common-issues","title":"Common Issues","text":"<p>\"Session directory does not exist\"</p> <ul> <li>Ensure the session directory follows the expected structure: <code>{local_data_path}/{mouse_name}/{date}/{session_id}/</code></li> <li>Check that <code>local_data_path()</code> is configured correctly</li> </ul> <p>\"suite2p directory does not exist\"</p> <ul> <li>Run suite2p processing before registration</li> <li>Ensure suite2p output is in <code>{session_path}/suite2p/</code></li> </ul> <p>\"Missing required suite2p files\"</p> <ul> <li>Verify all planes have the required <code>.npy</code> files (F, Fneu, spks, stat, ops, iscell)</li> <li>Check for typos in file names</li> </ul> <p>\"Frame count mismatch\"</p> <ul> <li>The registration process attempts to handle minor mismatches automatically</li> <li>Large mismatches may indicate a problem with the Timeline neuralFrames signal</li> <li>Check that ScanImage TTL signals were properly recorded</li> </ul> <p>\"Behavior type not supported\"</p> <ul> <li>Ensure <code>vrBehaviorVersion</code> matches the version of vrControl used</li> <li>Currently supported: 1 (standard), 2 (CR hippocampus)</li> </ul> <p>\"First flips in trial are not all down\"</p> <ul> <li>This assertion only applies to sessions after 2022-08-30</li> <li>May indicate a problem with photodiode signal or trial preparation</li> </ul>"},{"location":"workflows/registration/#checking-registration-status","title":"Checking Registration Status","text":"<pre><code>  from vrAnalysis.database import get_database\n\n  db = get_database(\"vrSessions\")\n\n  # Check if session is registered\n  record = db.get_record(\"mouse001\", \"2024-01-15\", \"001\")\n  if record is not None:\n      print(f\"Registration status: {record['vrRegistration']}\")\n      print(f\"Registration date: {record['vrRegistrationDate']}\")\n      if record['vrRegistrationError']:\n          print(f\"Error: {record['vrRegistrationException']}\")\n</code></pre>"}]}