{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib qt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import detrend\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from vrAnalysis.helpers import errorPlot\n",
    "from photometry.loaders import get_doric_files, process_data_parallel, process_single_file\n",
    "from photometry.process import analyze_data, resample_with_antialiasing\n",
    "\n",
    "from syd import make_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import detrend, butter, filtfilt\n",
    "from vrAnalysis.helpers import nearestpoint\n",
    "\n",
    "def resample_with_antialiasing(data, time, new_sampling_rate, filter_order=8):\n",
    "    \"\"\"\n",
    "    Resample data with anti-aliasing filter to prevent aliasing artifacts.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Input signal to be resampled\n",
    "    time : array-like\n",
    "        Original time points\n",
    "    new_sampling_rate : float\n",
    "        Target sampling rate in Hz\n",
    "    filter_order : int\n",
    "        Order of the anti-aliasing filter\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    resampled_data : array\n",
    "        Resampled signal\n",
    "    new_time : array\n",
    "        New time points\n",
    "    \"\"\"\n",
    "    # Calculate original approximate sampling rate\n",
    "    orig_sampling_rate = 1 / np.median(np.diff(time))\n",
    "\n",
    "    # Design anti-aliasing filter\n",
    "    nyquist_freq = new_sampling_rate / 2\n",
    "\n",
    "    # Only apply filter if we're downsampling\n",
    "    if new_sampling_rate < orig_sampling_rate:\n",
    "        # Normalize cutoff frequency to Nyquist frequency\n",
    "        cutoff_freq = nyquist_freq / (orig_sampling_rate / 2)\n",
    "\n",
    "        # Apply Butterworth low-pass filter\n",
    "        b, a = butter(filter_order, cutoff_freq, btype=\"low\")\n",
    "        filtered_data = filtfilt(b, a, data)\n",
    "    else:\n",
    "        filtered_data = data\n",
    "\n",
    "    # Create new time points\n",
    "    min_time = np.min(time) + 1e-3\n",
    "    max_time = np.max(time) - 1e-3\n",
    "    period = 1 / new_sampling_rate\n",
    "    num_periods = int((max_time - min_time) / period)\n",
    "    new_time = np.linspace(min_time, max_time, num_periods)\n",
    "\n",
    "    # Interpolate filtered data\n",
    "    resampled_data = interp1d(time, filtered_data, bounds_error=False, fill_value=\"extrapolate\")(new_time)\n",
    "\n",
    "    return resampled_data, new_time\n",
    "\n",
    "\n",
    "def _filter_cycle_markers(markers, first, last, keep_first=False, keep_last=False):\n",
    "    \"\"\"Filter cycle markers to only include valid cycles.\"\"\"\n",
    "    if keep_first:\n",
    "        markers = markers[markers >= first]\n",
    "    else:\n",
    "        markers = markers[markers > first]\n",
    "    if keep_last:\n",
    "        markers = markers[markers <= last]\n",
    "    else:\n",
    "        markers = markers[markers < last]\n",
    "    return markers\n",
    "\n",
    "\n",
    "def get_cycles(data, cycle_period_tolerance=0.1):\n",
    "    \"\"\"Get output cycles with interleaved data on out1 and out2.\n",
    "\n",
    "    Find start stop indices for each cycle.\n",
    "    Check that the cycles are interleaved correctly.\n",
    "    Return the start and stop indices for each cycle.\n",
    "    Return an index to all samples within the target cycles.\n",
    "\n",
    "    Target cycle definition:\n",
    "    First cycle is always on out1 (will clip if necessary) - last cycle is out2.\n",
    "    \"\"\"\n",
    "    diff1 = np.diff(data[\"out1\"])\n",
    "    diff2 = np.diff(data[\"out2\"])\n",
    "    start1 = np.where(diff1 == 1)[0] + 1\n",
    "    start2 = np.where(diff2 == 1)[0] + 1\n",
    "    stop1 = np.where(diff1 == -1)[0] + 1\n",
    "    stop2 = np.where(diff2 == -1)[0] + 1\n",
    "    first_valid_idx = start1[0]\n",
    "    last_valid_idx = stop2[-1]\n",
    "\n",
    "    start1 = _filter_cycle_markers(start1, first_valid_idx, last_valid_idx, keep_first=True)\n",
    "    start2 = _filter_cycle_markers(start2, first_valid_idx, last_valid_idx)\n",
    "    stop1 = _filter_cycle_markers(stop1, first_valid_idx, last_valid_idx)\n",
    "    stop2 = _filter_cycle_markers(stop2, first_valid_idx, last_valid_idx, keep_last=True)\n",
    "\n",
    "    start1 = _filter_cycle_markers(start1, first_valid_idx, stop1[-1], keep_first=True)\n",
    "    stop2 = _filter_cycle_markers(stop2, start2[0], last_valid_idx, keep_last=True)\n",
    "\n",
    "    if len(start1) != len(start2):\n",
    "        raise ValueError(\"Unequal number of start markers\")\n",
    "    if len(stop1) != len(stop2):\n",
    "        raise ValueError(\"Unequal number of stop markers\")\n",
    "    if len(start1) != len(stop1):\n",
    "        raise ValueError(\"Unequal number of start and stop markers\")\n",
    "    if not np.all(start1 < stop1):\n",
    "        raise ValueError(\"Start marker after stop marker for channel 1\")\n",
    "    if not np.all(start2 < stop2):\n",
    "        raise ValueError(\"Start marker after stop marker for channel 2\")\n",
    "\n",
    "    period1 = stop1 - start1\n",
    "    period2 = stop2 - start2\n",
    "    period1_deviation = period1 / np.mean(period1)\n",
    "    period2_deviation = period2 / np.mean(period2)\n",
    "    bad_period1 = np.abs(period1_deviation - 1) > cycle_period_tolerance\n",
    "    bad_period2 = np.abs(period2_deviation - 1) > cycle_period_tolerance\n",
    "    if np.sum(np.diff(np.where(bad_period1)[0]) < 2) > 2:\n",
    "        raise ValueError(\"Too many consecutive bad periods in channel 1\")\n",
    "    if np.sum(np.diff(np.where(bad_period2)[0]) < 2) > 2:\n",
    "        raise ValueError(\"Too many consecutive bad periods in channel 2\")\n",
    "\n",
    "    # Remove bad periods and filter stop / start signals\n",
    "    valid_period = ~bad_period1 & ~bad_period2\n",
    "    start1 = start1[valid_period]\n",
    "    stop1 = stop1[valid_period]\n",
    "    start2 = start2[valid_period]\n",
    "    stop2 = stop2[valid_period]\n",
    "\n",
    "    if not np.all(data[\"out1\"][start1] == 1) or not np.all(data[\"out2\"][start2] == 1):\n",
    "        raise ValueError(\"Start indices are not positive for out1 / out2!\")\n",
    "    if not np.all(data[\"out1\"][stop1] == 0) or not np.all(data[\"out2\"][stop2] == 0):\n",
    "        raise ValueError(\"Stop indices are not zero for out1 / out2!\")\n",
    "\n",
    "    return start1, stop1, start2, stop2\n",
    "\n",
    "\n",
    "def get_opto_cycles(data, min_period=1, cycle_period_tolerance=0.01):\n",
    "    \"\"\"Get opto cycles (out3) with a minimum period.\n",
    "\n",
    "    Returns the start times for each cycle and an average cycle signal.\n",
    "    \"\"\"\n",
    "    diff3 = np.diff(data[\"out3\"])\n",
    "    start3 = np.where(diff3 == 1)[0] + 1\n",
    "    stop3 = np.where(diff3 == -1)[0] + 1\n",
    "    first_valid_idx = start3[0]\n",
    "    last_valid_idx = stop3[-1]\n",
    "    start3 = _filter_cycle_markers(start3, first_valid_idx, last_valid_idx, keep_first=True)\n",
    "    start_time = data[\"out_time\"][start3]\n",
    "\n",
    "    valid_starts = [start3[0]]\n",
    "    valid_times = [start_time[0]]\n",
    "\n",
    "    for i in range(1, len(start3)):\n",
    "        if start_time[i] > (valid_times[-1] + min_period):\n",
    "            valid_starts.append(start3[i])\n",
    "            valid_times.append(start_time[i])\n",
    "\n",
    "    # Convert valid starts to numpy array (reuse start3 for consistent terminology with get_cycles)\n",
    "    start3 = np.array(valid_starts)\n",
    "\n",
    "    # Measure period between cycles\n",
    "    period3 = start3[1:] - start3[:-1]\n",
    "    period3_deviation = period3 / np.mean(period3)\n",
    "    if not np.all(period3_deviation >= 1 - cycle_period_tolerance) and np.all(period3_deviation <= 1 + cycle_period_tolerance):\n",
    "        min_period = np.min(period3)\n",
    "        max_period = np.max(period3)\n",
    "        raise ValueError(f\"Excess period variation in opto cycles! min={min_period:.2f}, max={max_period:.2f}\")\n",
    "\n",
    "    min_period = np.min(period3)\n",
    "    stop3 = start3 + min_period\n",
    "\n",
    "    if stop3[-1] >= len(data[\"out3\"]):\n",
    "        start3 = start3[:-1]\n",
    "        stop3 = stop3[:-1]\n",
    "\n",
    "    cycles = []\n",
    "    for istart, istop in zip(start3, stop3):\n",
    "        cycles.append(data[\"out3\"][istart:istop])\n",
    "    average_cycle = np.mean(np.stack(cycles), axis=0)\n",
    "\n",
    "    return start3, stop3, average_cycle\n",
    "\n",
    "\n",
    "def get_cycle_data(signal, start, stop, keep_fraction=0.5, signal_cv_tolerance=0.05):\n",
    "    \"\"\"Extract cycle data from a signal.\"\"\"\n",
    "    num_samples = len(start)\n",
    "    assert keep_fraction > 0 and keep_fraction < 1, \"Invalid keep_fraction, must be in between 0 and 1\"\n",
    "    assert num_samples == len(stop), \"Start and stop indices mismatch\"\n",
    "    cycle_data = []\n",
    "    invalid_cycle = []\n",
    "    for i in range(num_samples):\n",
    "        c_stop = stop[i] - 1\n",
    "        c_start = start[i] + int(keep_fraction * (c_stop - start[i]))\n",
    "        cycle_signal = signal[c_start:c_stop]\n",
    "        cycle_cv = np.std(cycle_signal) / np.mean(cycle_signal)\n",
    "        invalid_cycle.append(cycle_cv > signal_cv_tolerance)\n",
    "        cycle_data.append(signal[c_start:c_stop])\n",
    "    cycle_data = np.array([np.mean(cd) for cd in cycle_data])\n",
    "    return cycle_data, np.array(invalid_cycle)\n",
    "\n",
    "\n",
    "def analyze_data(data, preperiod=0.1, postperiod=1.0, cycle_period_tolerance=0.5, keep_fraction=0.5, signal_cv_tolerance=0.05, sampling_rate=1000):\n",
    "    \"\"\"Process a data file, return results and filtered signals.\"\"\"\n",
    "    # First check if the data is valid and meets criteria for processing.\n",
    "    num_samples = len(data[\"in_data\"])\n",
    "    if not num_samples > 0:\n",
    "        raise ValueError(\"No data found! in_data has 0 samples.\")\n",
    "    for key in [\"out1\", \"out2\", \"out3\"]:\n",
    "        assert num_samples == len(data[key]), f\"{key} and in_data length mismatch\"\n",
    "        uvals = np.unique(data[key])\n",
    "        if not np.array_equal(uvals, np.array([0.0, 1.0])):\n",
    "            raise ValueError(f\"Invalid values in {key}: {uvals}\")\n",
    "    for key in [\"in_time\", \"out_time\"]:\n",
    "        assert num_samples == len(data[key]), f\"{key} and in_data length mismatch\"\n",
    "\n",
    "    # Get start and top indices for the interleaved cycles\n",
    "    time = data[\"in_time\"]\n",
    "    start1, stop1, start2, stop2 = get_cycles(data, cycle_period_tolerance=cycle_period_tolerance)\n",
    "    cycle_timestamps = (time[stop2] + time[start1]) / 2  # Midpoint of full cycles\n",
    "    in1, invalid1 = get_cycle_data(data[\"in_data\"], start1, stop1, keep_fraction=keep_fraction, signal_cv_tolerance=signal_cv_tolerance)\n",
    "    in2, invalid2 = get_cycle_data(data[\"in_data\"], start2, stop2, keep_fraction=keep_fraction, signal_cv_tolerance=signal_cv_tolerance)\n",
    "\n",
    "    if np.any(invalid1) or np.any(invalid2):\n",
    "        print(\n",
    "            f\"Warning: excess co. of var. detected for {np.sum(invalid1)/num_samples*100:.2f}% of cycles are invalid for channel 1 and {np.sum(invalid2)/num_samples*100:.2f}% for channel 2.\"\n",
    "        )\n",
    "\n",
    "    data_in1, time_data = resample_with_antialiasing(in1, cycle_timestamps, sampling_rate)\n",
    "    data_in2, in2_time_rs = resample_with_antialiasing(in2, cycle_timestamps, sampling_rate)\n",
    "\n",
    "    data_in1 = detrend(data_in1)\n",
    "    data_in2 = detrend(data_in2)\n",
    "\n",
    "    if not np.allclose(time_data, in2_time_rs):\n",
    "        raise ValueError(\"Inconsistent time stamps for in1 and in2\")\n",
    "\n",
    "    # Get start indices and times for opto cycles\n",
    "    start3, stop3, _ = get_opto_cycles(data, min_period=1.0, cycle_period_tolerance=cycle_period_tolerance)\n",
    "    opto_start_time = data[\"out_time\"][start3]\n",
    "    \n",
    "    # And also get the time of opto start / stops in the new sampling rate\n",
    "    # returns index of y closest to each point in x and distance between points\n",
    "    start3, error_start3 = nearestpoint(opto_start_time, time_data)\n",
    "\n",
    "    # Interpolate opto data to in_data timestamps\n",
    "    opto_data = interp1d(data[\"out_time\"], data[\"out3\"], bounds_error=False, fill_value=\"extrapolate\")(time_data)\n",
    "\n",
    "    # Get cycle data for opto cycles\n",
    "    in1_opto = []\n",
    "    in2_opto = []\n",
    "    out3_opto = []\n",
    "    time_opto = []\n",
    "    samples_pre = int(preperiod * sampling_rate)\n",
    "    samples_post = int(postperiod * sampling_rate)\n",
    "    for istart in start3:\n",
    "        in1_opto.append(data_in1[istart - samples_pre : istart + samples_post])\n",
    "        in2_opto.append(data_in2[istart - samples_pre : istart + samples_post])\n",
    "        out3_opto.append(opto_data[istart - samples_pre : istart + samples_post])\n",
    "        \n",
    "        # Relative time... should always be the same actually\n",
    "        time_opto.append(time_data[istart - samples_pre : istart + samples_post] - time_data[istart])\n",
    "\n",
    "    in1_opto = np.stack(in1_opto)\n",
    "    in2_opto = np.stack(in2_opto)\n",
    "    out3_opto = np.stack(out3_opto)\n",
    "    time_opto = np.mean(np.stack(time_opto), axis=0)  # variance across opto cycles should be within sample error\n",
    "\n",
    "    results = dict(\n",
    "        in1_opto=in1_opto,\n",
    "        in2_opto=in2_opto,\n",
    "        out3_opto=out3_opto,\n",
    "        time_opto=time_opto,\n",
    "        opto_start_time=opto_start_time - time_data[0],\n",
    "        data_in1=data_in1,\n",
    "        data_in2=data_in2,\n",
    "        data_opto=opto_data,\n",
    "        time_data=time_data - time_data[0],\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 files\n"
     ]
    }
   ],
   "source": [
    "mouse_name = \"ATL065\"\n",
    "dirs, findex, all_data = get_doric_files(mouse_name)\n",
    "print(f\"Found {len(all_data)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preperiod = 0.1\n",
    "postperiod = 1.0\n",
    "cycle_period_tolerance = 0.5\n",
    "keep_fraction = 0.5\n",
    "signal_cv_tolerance = 0.05\n",
    "sampling_rate = 1000\n",
    "\n",
    "results = analyze_data(\n",
    "    all_data[3],\n",
    "    preperiod=preperiod,\n",
    "    postperiod=postperiod,\n",
    "    cycle_period_tolerance=cycle_period_tolerance,\n",
    "    keep_fraction=keep_fraction,\n",
    "    signal_cv_tolerance=signal_cv_tolerance,\n",
    "    sampling_rate=sampling_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.ndimage import percentile_filter\n",
    "\n",
    "def percentile_filter(data, percentile=10, size=500):\n",
    "    return percentile_filter(data, percentile, size=size)\n",
    "\n",
    "# 3. Polynomial detrending\n",
    "def polynomial_detrend(data, order=3):\n",
    "    x = np.arange(len(data))\n",
    "    coeffs = np.polyfit(x, data, order)\n",
    "    trend = np.polyval(coeffs, x)\n",
    "    return data - trend\n",
    "\n",
    "def savgol_detrend(data, window_length=301, poly_order=3):\n",
    "    if window_length % 2 == 0:\n",
    "        window_length += 1\n",
    "    trend = signal.savgol_filter(data, window_length, poly_order)\n",
    "    return data - trend\n",
    "\n",
    "def highpass_detrend(data, cutoff_freq=0.01):\n",
    "    nyquist = sampling_rate / 2\n",
    "    b, a = signal.butter(3, cutoff_freq/nyquist, btype='high')\n",
    "    return signal.filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "T = 10000\n",
    "x = np.random.randn(T)\n",
    "K = 10\n",
    "f = signal.filtfilt(np.ones(K)/K, 1.0, x)\n",
    "f = f + 0.001*np.arange(T)\n",
    "pf = percentile_filter(f, size=100, percentile=10)\n",
    "\n",
    "plt.plot(f, color=\"b\", linewidth=0.5)\n",
    "plt.plot(pf, color=\"r\", linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f669fda481e40ec8df0e361b0097198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(VBox(children=(HTML(value='<b>Parameters</b>'), Dropdown(description='method', iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<syd.interactive_viewer.InteractiveViewer at 0x199c36ecc70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "viewer = make_viewer()\n",
    "\n",
    "detrend_options = dict(\n",
    "    percentile = percentile_filter,\n",
    "    polynomial = polynomial_detrend,\n",
    "    savgol = savgol_detrend,\n",
    "    highpass = highpass_detrend,\n",
    ")\n",
    "\n",
    "params = dict(\n",
    "    percentile = [\"size\", \"percentile\"],\n",
    "    polynomial = [\"order\"],\n",
    "    savgol = [\"window_length\", \"poly_order\"],\n",
    "    highpass = [\"cutoff_freq\"],\n",
    ")\n",
    "\n",
    "def make_kwargs(method, state):\n",
    "    kwargs = {}\n",
    "    for param in params[method]:\n",
    "        kwargs[param] = state[param]\n",
    "    return kwargs\n",
    "\n",
    "def plot(viewer, state):\n",
    "    method = state[\"method\"]\n",
    "    kwargs = make_kwargs(method, state)\n",
    "    \n",
    "    detrended1 = detrend_options[method](results[\"data_in1\"], **kwargs)\n",
    "    detrended2 = detrend_options[method](results[\"data_in2\"], **kwargs)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(4, 4), layout=\"constrained\", sharex=True, sharey=False)\n",
    "    ax[0].plot(results[\"time_data\"], results[\"data_in1\"], label=\"in1\", color=\"k\", linewidth=0.5)\n",
    "    ax[0].plot(results[\"time_data\"], results[\"data_in2\"], label=\"in2\", color=\"b\", linewidth=0.5)\n",
    "    ax[1].plot(results[\"time_data\"], detrended1, label=\"in1\", color=\"k\", linewidth=0.5)\n",
    "    ax[1].plot(results[\"time_data\"], detrended2, label=\"in2\", color=\"b\", linewidth=0.5)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "options = list(detrend_options.keys())\n",
    "viewer.set_plot(plot)\n",
    "viewer.add_selection(\"method\", value=options[2], options=options)\n",
    "viewer.add_integer(\"percentile\", value=10, min_value=0, max_value=100)\n",
    "viewer.add_integer(\"size\", value=60, min_value=1, max_value=1000)\n",
    "viewer.add_integer(\"order\", value=3, min_value=1, max_value=10)\n",
    "viewer.add_integer(\"window_length\", value=301, min_value=1, max_value=1000)\n",
    "viewer.add_integer(\"poly_order\", value=3, min_value=1, max_value=10)\n",
    "viewer.add_float(\"cutoff_freq\", value=0.5, min_value=0.01, max_value=10)\n",
    "\n",
    "viewer.deploy()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
